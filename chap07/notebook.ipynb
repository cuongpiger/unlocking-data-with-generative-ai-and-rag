{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New OS parameter to avoid warnings.  \n",
    "# This will not have a material impact on your code, but prevents warnings from appearing related to new LangChain features.\n",
    "import os\n",
    "os.environ['USER_AGENT'] = 'RAGUserAgent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import bs4\n",
    "import chromadb\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain import hub\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from modules import utils\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = utils.load_env_file(\"./../secrets/env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "## Web loading and crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://kbourne.github.io/chapter1.html\",), \n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs: List[Document] = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of docs loaded is 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of docs loaded is {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Introduction to Retrieval Augmented Generation (RAG)\n",
      "    \n",
      "Date: March 10, 2024  |  Estimated Reading Time: 15 min  |  Author: Keith Bourne\n",
      "\n",
      "  In the rapidly evolving field of artificial intelligence, Retrieval-Augmented Generation (RAG) is emerging as a significant addition to the Generative AI toolkit. RAG harnesses the strengths of Large Language Models (LLMs) and integrates them with internal data, offering a method to enhance organizational operations significantly. This book delves into the essential aspects of RAG, examining its role in augmenting the capabilities of LLMs and leveraging internal corporate data for strategic advantage.\n",
      "As it progresses, the book outlines the potential of RAG in business, suggesting how it can make AI applications smarter, more responsive, and aligned with organizational objectives. RAG is positioned as a key facilitator of customized, efficient, and insightful AI solutions, bridging the gap between Generative AI's potential and specific business needs. This exploration of RAG encourages readers to unlock the full potential of their corporate data, paving the way for an era of AI-driven innovation.\n",
      "\n",
      "What You Can Expect to Learn#\n",
      "Expect to launch a comprehensive journey to understand and effectively incorporate Retrieval Augmented Generation (RAG) into AI systems. You'll explore a broad spectrum of essential topics, including vector databases, the vectorization process, vector search techniques, prompt engineering and design, and the use of AI agents for RAG applications, alongside methods for evaluating and visualizing RAG outcomes. Through practical, working code examples utilizing the latest tools and technologies like LangChain and Chroma's vector database, you'll gain hands-on experience in implementing RAG in your projects.\n",
      "At the outset, you'll delve into the core principles of RAG, appreciating its significance in the broader landscape of Generative AI. This foundational knowledge equips you with the perspective needed to discern how RAG applications are designed and why they succeed, paving the way for innovative solution development and problem-solving in AI.\n",
      "You'll discover the symbiosis between Large Language Models (LLMs) and internal data to bolster organizational operations. By learning about the intricacies of this integration, particularly the process of vectorization, including the creation and management of vector databases for efficient information retrieval, you'll gain crucial skills for navigating and harnessing vast data landscapes effectively in today's data-driven environments.\n",
      "Gain expertise in vector search techniques, an essential skill set for identifying pertinent data within extensive datasets. Coupled with this, you'll learn strategies for prompt engineering and design, ensuring that you can craft queries that elicit precise and relevant AI responses.\n",
      "Explore how AI agents play a pivotal role in RAG applications, facilitating sophisticated data interaction and retrieval tasks. You'll also learn methods for evaluating and visualizing RAG implementation outcomes, providing a framework for assessing performance and impact critically.\n",
      "Throughout this journey, you'll engage in practical, hands-on learning, guided through the use of cutting-edge tools like LangChain and Chroma's vector database, supported by real, working code examples. These detailed coding demonstrations, grounded in current frameworks, offer a practical foray into implementing RAG in AI systems, providing a rich learning experience.\n",
      "Case studies and coding exercises strategically interspersed throughout your learning path highlight the application of RAG in various real-world scenarios. These insights into addressing common and complex challenges prepare you to navigate the application of RAG across diverse settings with confidence.  The code will build off the same starting use case provided in the next chapter.  For each topic that relates to code, we will add code that shows how that topic impacts the RAG pipeline, giving you an in-depth understanding about how your coding choices can impact the capabilities of your RAG-based application.\n",
      "You'll also explore optimization strategies for data retrieval and enhancing the interpretability of AI-generated content. These insights are pivotal for improving the usability and effectiveness of AI applications, ensuring they are more aligned with strategic business objectives and user needs.\n",
      "As you progress, you'll gain a deeper understanding of how RAG can revolutionize AI applications, making them more intelligent, responsive, and tailored to specific requirements. The potential of RAG to facilitate personalized, efficient, and insightful AI solutions is thoroughly examined, bridging the theoretical and practical divides.\n",
      "Throughout this learning experience, a spirit of exploration and experimentation is encouraged, aiming to unlock the full potential of data through RAG, fostering innovation, and advancing the domain of AI-driven solutions. By the end, you will have gained comprehensive knowledge and practical skills in RAG, equipping you to contribute to the evolution of AI technologies and applications in your business and beyond.\n",
      "Understanding RAG: Basics and Principles#\n",
      "Modern day large language models (LLM) are impressive, but they have never seen your company’s private data (hopefully!).  This means the ability of an LLM to help your company fully utilize its own data is very limited.  This very large barrier has given rise to the concept of Retrieval Augmented Generation (RAG), where you are using the power and capabilities of the LLM, but combining it with the knowledge and data contained within your company’s internal data repositories.  This is the primary motivation for using RAG, to make new data available to the LLM and significantly increase the value you can extract from that data.  Beyond internal data, it is also useful in cases where the LLM has not been trained on the data, even if it is public, like the most recent research papers or articles about a topic that is strategic to your company.  In both cases, we are talking about data that was not present during the training of the LLM.  You can have the latest LLM trained on the most tokens ever, exceeding 10 trillion, but if that data was not present for the training, then the LLM will be at a disadvantage to help you reach your full productivity.\n",
      "\n",
      "Ultimately, this highlights the fact that for most organizations, connecting to data an LLM is not yet familiar with is a central need for them to fully utilize that LLM.  RAG is the most popular paradigm for doing this.  This book focuses on showing you how to set up a RAG application with your data, as well as how to get the most out of it in various situations.  I intend to give you an in-depth understanding of RAG and its importance in leveraging LLM within the context of a company's private or specific data needs.\n",
      "\n",
      "Advantages of RAG#\n",
      "Potential advantages of using RAG include improved accuracy and relevance, customization, flexibility, and expanding the model’s knowledge beyond the training data.  Here is each advantage more thoroughly explored:\n",
      "\n",
      "Improved Accuracy and Relevance: RAG can significantly enhance the accuracy and relevance of responses that are generated by large language models (LLMs). RAG fetches and incorporates specific information from a database or dataset, typically in real time, and ensures that the output is based on both the model’s pre-existing knowledge and the most current and relevant data that you are providing directly.\n",
      "\n",
      "Customization and Flexibility: RAG can customize its responses based on your domain specific needs. By integrating a company's internal databases into the model's response generation process, RAG allows for outputs that are tailored to the unique context and requirements of the business. This level of customization is invaluable for creating personalized experiences and for applications requiring a high degree of specificity and detail.\n",
      "\n",
      "Expanding Model Knowledge Beyond Training Data: LLMs are limited by the scope of their training data. RAG overcomes this limitation by enabling models to access and utilize information that was not included in their initial training sets. This effectively expands the knowledge base of the model without the need for retraining, making LLMs more versatile and adaptable to new domains or rapidly evolving topics.\n",
      "\n",
      "Limitations of RAG#\n",
      "But there are some limitations to using RAG, which include dependency on the quality of the internal data, computational overhead, more complex integrations, and the potential for information overload.\n",
      "Dependency on Data Quality: \n",
      "When talking about how data can impact an AI model, the saying in data science circles is “garbage in, garbage out.”  Meaning, if you give a model bad data, it will give you bad results.  RAG is no different. The effectiveness of RAG is directly tied to the quality of the data it retrieves. If the underlying database or dataset contains outdated, biased, or inaccurate information, the outputs generated by RAG will likely suffer from the same issues. \n",
      "\n",
      "Need for Data Manipulation and Cleaning:\n",
      "Data in the recesses of the company often has a lot of value to it, but it is not often in good, accessible shape.  For example, data from PDF-based customer statements needs a lot of massaging to get into a format that can be useful to a RAG pipeline.\n",
      "\n",
      "Computational Overhead: \n",
      "A RAG pipeline introduces a host of new computational steps into the response generation process, including data retrieval, processing, and integration.  LLMs are getting faster every day, but even the fastest response can be more than a second, and some can take several seconds.  If you combine that with other data processing steps, and possibly multiple LLM calls, the result can be a very significant increase in the time it takes to receive a response.  This all leads to increased computational overhead, affecting the efficiency and scalability of the entire system. As with any other IT initiative, an organization must balance the benefits of enhanced accuracy and customization against the resource requirements and potential latency introduced by these additional processes. \n",
      "\n",
      "Data Storage Explosion: Complexity in Integration and Maintenance: \n",
      "Traditionally, your data resides in a data source which is queried in various ways to be made available to your internal and external systems.  But with RAG, your data resides in multiple forms and locations, such as vectors in a vector database, that represent the same data, but in a different format. Add in the complexity of connecting these various data sources to LLMs and relevant technical mechanisms like vector searches, and you have a significant increase in complexity.  This increased complexity can be resource-intensive. Maintaining this integration over time, especially as data sources evolve or expand, adds even more complexity and cost. Organizations need to invest in technical expertise and infrastructure to effectively leverage RAG capabilities while accounting for the rapid increase in complexities these systems bring with them.\n",
      "\n",
      "Potential for Information Overload: \n",
      "It is very possible for RAG-based systems to pull in too much information.  It is just as important to implement mechanisms to address this issue as it is to handle times when not enough relevant information is found.  Determining the relevance and importance of retrieved information to be included in the final output requires sophisticated filtering and ranking mechanisms. Without these, the quality of the generated content could be compromised by an excess of unnecessary or marginally relevant details.\n",
      "\n",
      "RAG Vocabulary#\n",
      "Now is as good a time as any to review some vocabulary that should help you get familiar with the various concepts in RAG.  This is not an exhaustive list, but understanding these core concepts should help you understand everything else we teach you about RAG in a more effective way:\n",
      "Large Language Model (LLM)\n",
      "Most of this book will deal with LLMs.  LLMs are generative AI technologies that focus on generating text.  There are also generative models that generate images from text prompts, while others generate video from text prompts.  There are other models that generate text descriptions from images.  We will talk about these other types of models in Chapter 16, Going Beyond the LLM.  But for most of the book, I felt it would keep things simple and let you focus on the core principles of RAG if we focus on the type of model that most RAG pipelines use, the LLM.  But I did want to make sure it was clear, that while the book focuses primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images and videos.\n",
      "\n",
      "Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta LLaMA models, Google’s PaLM and Gemini models, and Anthropic’s Claude models.\n",
      "Foundation model\n",
      "A foundation model is the base model for most LLMs.  In the case of ChatGPT, the foundation model is based on the GPT (Generative Pre-trained Transformer) architecture, and it was fine-tuned for Chat. The specific model used for ChatGPT is not publicly disclosed.  The base GPT model cannot talk with you in chatbot-style like ChatGPT does.  It had to get further trained to gain that skill.  But there are many other skills these foundation models can be fine-tuned for.  LLaMA 2 is a foundation model and because it is open source, there are many spin offs that have been fine-tuned for many applications, such as medical research and conversation.  Most of the models we talk about are very close to foundation models, but will likely be fine-tuned for at least conversational capabilities. \n",
      "\n",
      "Parameters and Biases\n",
      "We are trying to keep our focus on RAG primarily in this book, but it will be helpful for you to understand what parameters and biases are in LLM models. In machine learning models in general, including LLMs, parameters and biases are the learnable variables that the model adjusts during the training process to improve its performance on a given task. Parameters are the weights associated with the connections between neurons in the model's architecture. These weights determine the strength and importance of each connection and are updated during training to minimize the difference between the model's predictions and the expected outputs. Biases, on the other hand, are additional values added to the weighted sum of inputs at each neuron. They help the model learn an offset or shift in the data, allowing for more flexibility in fitting the training data. Together, parameters and biases form the learnable components of the model that are fine-tuned using the training data to improve the model's performance on specific tasks or domains.\n",
      "\n",
      "Understanding parameters and biases is important in the context of RAG because the process of training and fine-tuning LLMs involves adjusting these variables. I mention foundation models, where the initial training involves setting these parameters and biases. However, fine-tuning an LLM by further adjusting its parameters and biases can significantly impact its performance and behavior within a RAG pipeline. By adapting the model to specific domains, writing styles, or tasks, you can improve the accuracy and relevance of the LLM's responses when it is used to generate output based on the retrieved information. This, in turn, can lead to better overall performance of the RAG system in terms of providing more useful and context-appropriate answers to user queries. Therefore, choosing an LLM that is properly fine-tuned for your domain, or applying the fine-tuning yourself, is crucial for optimizing the most important element of your RAG pipeline.\n",
      "Prompting, Prompt Design, Prompt Engineering\n",
      "These terms are sometimes used interchangeably, but technically, while they all have to do with prompting, they do have fairly different meanings. Prompting is the act of sending a query or “prompt” into an LLM.  Prompt design refers to the strategy you take to “design” the prompt you will send to the LLM.  There are many different prompt design strategies that work in different scenarios, and we will review many of these in Chapter 13, Utilizing Prompt Engineering to Improve RAG Efforts.  We will also review prompt engineering in chapter 13.  Prompt engineering focuses more on the technical aspects surrounding the prompt that you use to improve the outputs from the LLM.  For example, you may break up a complex query into two or three different LLM interactions, “engineering” it better to achieve superior results.\n",
      "\n",
      "Inference\n",
      "We will use the term ‘inference’ from time to time.  Generally, this refers to the process of the LLM generating outputs or predictions based on given inputs using a pre-trained language model.  But a key aspect of inference is that with an LLM, particularly with the cloud-based providers, you are charged based on the inference.  \n",
      "\n",
      "Context Window\n",
      "A context window, in the context of LLMs, refers to the maximum number of tokens (words, subwords, or characters) that the model can process as input or generate as output in a single pass. It determines the amount of text the model can \"see\" or \"attend to\" at once when making predictions or generating responses.\n",
      "\n",
      "The context window size is a key parameter of the model architecture and is typically fixed during model training. It directly relates to the input size of the model, as it sets an upper limit on the number of tokens that can be fed into the model at a time.\n",
      "\n",
      "For example, if a model has a context window size of 4,096 tokens, it means that the model can process and generate sequences of up to 4,096 tokens. When processing longer texts, such as documents or conversations, the input needs to be divided into smaller segments that fit within the context window. This is often done using techniques like sliding windows or truncation.\n",
      "The size of the context window has implications for the model's ability to understand and maintain long-range dependencies and context. Models with larger context windows can capture and utilize more contextual information when generating responses, which can lead to more coherent and contextually relevant outputs. However, increasing the context window size also increases the computational resources required to train and run the model.\n",
      "In the context of RAG, the context window size is particularly important because it determines how much information from the retrieved documents can be effectively utilized by the model when generating the final response. Recent advancements in language models have led to the development of models with significantly larger context windows, enabling them to process and retain more information from the retrieved sources.\n",
      "\n",
      "Fine-Tuning in 2 Flavors: Full Model Fine Tuning (FMFT) and Parameter Efficient Fine Tuning (PEFT)\n",
      "Full model fine-tuning (FMFT) is where you take a foundation model and train it further to gain new capabilities.  You could simply give it new knowledge for a specific domain, or you could give it a skill, like being a conversational chat-bot.  FMFT updates all of the parameters and biases in the model. \n",
      "\n",
      "PEFT is a type of fine-tuning, where you focus only on specific parts of the parameters or biases when you fine-tune the model, but with a similar goal as general fine-tuning.  The latest research in this area shows that you can achieve similar results to FMFT with far less cost, time commitment, and data.\n",
      "While this book is not focused on fine-tuning, it is a very valid strategy to try to use a model fine-tuned with your own data to give it more knowledge from your domain or to give it more of “voice” from your domain.  For example, you could train it to talk more like a scientist than a generic foundation model, if using this in a scientific field.  Or if you are developing in a legal field, you may want it to sound more like a lawyer.\n",
      "Vector Store or Vector Database?\n",
      "Both!  All vector databases are vector stores, but not all vector stores are vector databases.  Ok, while you get out your chalkboard to draw the Vinn diagram, I continue to explain this statement.  There are ways to store vectors that are not full databases.  They are simply storage devices for vectors.  So to encompass all possible ways to store vectors, LangChain calls them all vector stores.  Let’s do the same!  Just know that not all of the “vector stores” that LangChain connects with are officially considered vector databases, but in general, most of them are and many people refer to all of them as vector databases.   Phew, glad we cleared that up!\n",
      "\n",
      "Vectors, Vectors, Vectors! \n",
      "A vector is a mathematical representation of your data.  They are often referred to as the embeddings when talking specifically about natural language processing and LLMs. Vectors are one of the most important concepts to understand and there are many different parts of a RAG pipeline that utilize vectors.  I felt it was bigger than just a quick definition, so I go into much more depth in the much larger next section dedicated to vectors.  And beyond that, we literally spend two chapters (6 & 7) going over vectors and how they are used to find similar content. \n",
      "\n",
      "Vectors#\n",
      "It could be argued that understanding vectors and all the ways they are used in RAG is the most important part of this entire book.  As mentioned above, vectors are simply the mathematical representations of your external data, and they are often referred to as embeddings.  These representations capture semantic information in a format that can be processed by algorithms, facilitating tasks such as similarity search, which is a crucial step in the RAG process.\n",
      "\n",
      "Vectors typically have a specific dimension based on how many numbers are represented by them. For example, this is a 4 dimensional vector: [0.123, 0.321, 0.312, 0.231]\n",
      "\n",
      "If you didn’t know we were talking about vectors and you saw this in Python code, you might recognize this as a list of 4 floating points, and you aren’t too far off.  Typically though, when working with vectors in Python, you actually want to recognize them as a Numpy Array.  Numpy Arrays are generally more machine learning friendly because they are optimized to be processed much faster and efficiently than python lists, and they are more broadly recognized as the defacto representation of embeddings across machine learning packages like SciPy, Pandas, Scikit-Learn, TensorFlow, Keras, Pytorch, and many others.  Numpy also enables you to perform vectorized math directly on the Numpy Array, such as performing element-wise operations, without having to code in loops and other approaches you might have to use if using a different type of sequence.\n",
      "\n",
      "When working with vectors for vectorization, they are often hundreds, or thousands of dimensions, which refers to the number of floating points present in the vector.  So a 1024 dimension vector literally has 1024 floating points in a Numpy Array.  Higher dimensionality can capture more detailed semantic information, which is crucial for accurately matching query inputs with relevant documents or data in RAG applications.\n",
      "In chapter 7, we cover the key role vectors and vector databases play In RAG implementation.  And then in chapter 8, we will dive more into the concept of similarity searches, which utilize vectors to conduct the search much faster and efficiently.  These are key concepts that will help you gain a much deeper understanding into how to better implement a RAG pipeline.\n",
      "\n",
      "Implementing RAG in AI Applications#\n",
      "Retrieval Augmented Generation (RAG) is rapidly becoming a cornerstone of GenAI platforms in the corporate world.  RAG combines the power of information retrieval of internal or “new” data with generative language models to enhance the quality and relevance of generated text. This technique can be particularly useful for companies across various industries to improve their products, services, and operational efficiencies. Some examples of how RAG can be used include:\n",
      " Customer Support and Chatbots - These can exist without RAG, but when integrated with RAG, it can connect those chatbots with past customer interactions, FAQs, support documents, and anything else that was specific to that customer.\n",
      "Automated Reporting - RAG can assist in creating initial drafts or summarizing existing articles, research papers, and other types of unstructured data into more digestible formats.\n",
      "Product Descriptions - For e-commerce companies, RAG can be used to help generate or enhance product descriptions by retrieving information from similar products or manufacturer specifications.\n",
      "Searchability and Utility of Internal Knowledge Bases - RAG can improve access to internal knowledge bases. This can be achieved through the generation of summaries of documents or by providing direct answers to queries based on the content of internal documents, emails, and other resources.\n",
      "Searchability and Utility of General Knowledge Bases - In areas like legal and compliance, where companies need to have an understanding of a massive and growing general knowledge base, RAG can be implemented to retrieve and summarize relevant laws, regulations, and compliance documents. Other areas where this is applicable include research and development, medical, academia, patents, and technical documents.\n",
      "Innovation Scouting - Similar to searching general knowledge bases, but with a focus on new innovation, companies can use RAG to scan and summarize information from quality sources to identify trends and potential areas for new innovations that are relevant to that company's specialization.\n",
      "Content Personalization - RAG can be used by media and content platforms to personalize content recommendations or create customized summaries by retrieving information based on a user's past interactions and preferences.\n",
      "Product Recommendations - RAG can be used by e-commerce sites to enhance product recommendation engines, generate personalized descriptions, or highlight features based on the browsing and purchasing history of customers.\n",
      "Training and Education - RAG can be used by education organizations and corporate training programs to generate or customize learning materials based on specific needs and knowledge levels of the learners. With RAG, a much deeper level of internal knowledge from the organization can be incorporated into the educational curriculum in very customized ways to the individual or role.\n",
      "\n",
      "This book will help you understand how you can implement all of these game-changing initiatives in your company.\n",
      "\n",
      "Comparing RAG with Conventional Generative AI#\n",
      "Conventional Generative AI has already shown to be a revolutionary change for companies, helping their employees reach new levels of productivity.   LLMs like ChatGPT are assisting users with a rapidly growing list of applications that include writing business plans, writing and improving code, writing marketing copy, and even providing healthier recipes for a specific type of diet.  Ultimately, much of what users are doing is getting done faster.\n",
      "But conventional Generative AI does not know what it does not know.  And that includes most of the internal data in your company.   Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are?  You do not have to imagine it, that is what RAG does!  \n",
      "Even smaller companies are not able to access much of their internal data resources very effectively.  Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized.  Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company.  With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big.\n",
      "Comparing RAG with Model Fine-Tuning#\n",
      "Established Large Language Models (LLM), what we call the foundation models, can learn in two ways:\n",
      " Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model's intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs.\n",
      "Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon.\n",
      "\n",
      "Why not use fine-tuning in all situations?  Once you have introduced the new knowledge, it will always have it!  It is also how the model was originally created, by training with data, right?  That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall. \n",
      "The reason is complicated, but in general, a model’s knowledge of facts is like a human’s long-term memory.  If you memorize a long passage from a speech or book and then try to recall it a few months later, you will likely still understand the context of the information, but you may forget specific details.  Whereas, adding knowledge through the input of the model is like our short-term memory, where the facts, details, and even the order of wording is all very fresh and available for recall.  It is this latter scenario that lends itself better in a situation where you want successful factual recall.\n",
      "There is a trade-off though.  Inputs are limited by the context window of the model.  This is an area that is actively being addressed though. For example, ChatGPT 3.5 only had a 4,096 token context window, which is the equivalent of about 5 pages of text.  When ChatGPT 4 was released, they expanded the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version that had a context window of 32,768 tokens  (40 pages).  This issue is so important that they included the context window size in the name of the model.  That is a strong indicator of how important the context window is! \n",
      "What about the latest Gemini 1.5 model?  1M token context window, or over 1,000 pages.\n",
      "As the context windows expand though, this has created another issue.  Early models with expanded context windows were shown to lose a lot of the details, especially in the “middle” of the text.  This issue is also being addressed.  The Gemini 1.5 model with the 1 million token context window has performed well in tests called “needle-in-a-haystack” tests for “remembering” all details well throughout the text it can take as input.  Unfortunately, the model did not perform as well in the “multiple needle’s in a haystack” tests. Anthropic’s largest version of their latest model, Claude 3 Opus, performs fairly well with contest windows of 200,000 words.  Expect more effort in this area as these context windows get larger, and keep this in mind if you need to work with large amounts of text at a time.\n",
      "NOTE: It is important to note that token count differs from word count, as tokens include punctuation, symbols, numbers, and other text representations. How a compound word like “ice cream” is treated token-wise depends on the tokenization scheme and it can vary across LLM.  But most well-known LLMs (like ChatGPT and Gemini) would be considered 2 tokens.  Under certain circumstances in NLP you may argue that it should be one based on the concept that a token should represent a useful semantic unit for processing.  \n",
      "Ultimately, when deciding between RAG and fine-tuning, consider your specific use case and requirements. RAG is generally superior for retrieving factual information that is not present in the LLM's training data or is private. It allows for dynamic integration of external knowledge without modifying the model's weights. Fine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. Keep in mind the limitations of context window sizes and the potential for overfitting when fine-tuning on a specific dataset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"llama3.1:8b\"\n",
    "base_url = envs[\"OLLAMA_HOST\"]  # for example, \"http://localhost:11434\"\n",
    "\n",
    "# Split\n",
    "text_splitter: SemanticChunker = SemanticChunker(OllamaEmbeddings(base_url=base_url, model=embedding_model))\n",
    "splits: List[Document] = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of splits is 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of splits is {len(splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Introduction to Retrieval Augmented Generation (RAG)\n",
      "    \n",
      "Date: March 10, 2024  |  Estimated Reading Time: 15 min  |  Author: Keith Bourne\n",
      "\n",
      "  In the rapidly evolving field of artificial intelligence, Retrieval-Augmented Generation (RAG) is emerging as a significant addition to the Generative AI toolkit. RAG harnesses the strengths of Large Language Models (LLMs) and integrates them with internal data, offering a method to enhance organizational operations significantly. This book delves into the essential aspects of RAG, examining its role in augmenting the capabilities of LLMs and leveraging internal corporate data for strategic advantage. As it progresses, the book outlines the potential of RAG in business, suggesting how it can make AI applications smarter, more responsive, and aligned with organizational objectives. RAG is positioned as a key facilitator of customized, efficient, and insightful AI solutions, bridging the gap between Generative AI's potential and specific business needs. This exploration of RAG encourages readers to unlock the full potential of their corporate data, paving the way for an era of AI-driven innovation. What You Can Expect to Learn#\n",
      "Expect to launch a comprehensive journey to understand and effectively incorporate Retrieval Augmented Generation (RAG) into AI systems. You'll explore a broad spectrum of essential topics, including vector databases, the vectorization process, vector search techniques, prompt engineering and design, and the use of AI agents for RAG applications, alongside methods for evaluating and visualizing RAG outcomes. Through practical, working code examples utilizing the latest tools and technologies like LangChain and Chroma's vector database, you'll gain hands-on experience in implementing RAG in your projects. At the outset, you'll delve into the core principles of RAG, appreciating its significance in the broader landscape of Generative AI. This foundational knowledge equips you with the perspective needed to discern how RAG applications are designed and why they succeed, paving the way for innovative solution development and problem-solving in AI. You'll discover the symbiosis between Large Language Models (LLMs) and internal data to bolster organizational operations. By learning about the intricacies of this integration, particularly the process of vectorization, including the creation and management of vector databases for efficient information retrieval, you'll gain crucial skills for navigating and harnessing vast data landscapes effectively in today's data-driven environments. Gain expertise in vector search techniques, an essential skill set for identifying pertinent data within extensive datasets. Coupled with this, you'll learn strategies for prompt engineering and design, ensuring that you can craft queries that elicit precise and relevant AI responses. Explore how AI agents play a pivotal role in RAG applications, facilitating sophisticated data interaction and retrieval tasks. You'll also learn methods for evaluating and visualizing RAG implementation outcomes, providing a framework for assessing performance and impact critically. Throughout this journey, you'll engage in practical, hands-on learning, guided through the use of cutting-edge tools like LangChain and Chroma's vector database, supported by real, working code examples. These detailed coding demonstrations, grounded in current frameworks, offer a practical foray into implementing RAG in AI systems, providing a rich learning experience. Case studies and coding exercises strategically interspersed throughout your learning path highlight the application of RAG in various real-world scenarios. These insights into addressing common and complex challenges prepare you to navigate the application of RAG across diverse settings with confidence. The code will build off the same starting use case provided in the next chapter. For each topic that relates to code, we will add code that shows how that topic impacts the RAG pipeline, giving you an in-depth understanding about how your coding choices can impact the capabilities of your RAG-based application. You'll also explore optimization strategies for data retrieval and enhancing the interpretability of AI-generated content. These insights are pivotal for improving the usability and effectiveness of AI applications, ensuring they are more aligned with strategic business objectives and user needs. As you progress, you'll gain a deeper understanding of how RAG can revolutionize AI applications, making them more intelligent, responsive, and tailored to specific requirements. The potential of RAG to facilitate personalized, efficient, and insightful AI solutions is thoroughly examined, bridging the theoretical and practical divides. Throughout this learning experience, a spirit of exploration and experimentation is encouraged, aiming to unlock the full potential of data through RAG, fostering innovation, and advancing the domain of AI-driven solutions. By the end, you will have gained comprehensive knowledge and practical skills in RAG, equipping you to contribute to the evolution of AI technologies and applications in your business and beyond. Understanding RAG: Basics and Principles#\n",
      "Modern day large language models (LLM) are impressive, but they have never seen your company’s private data (hopefully!). This means the ability of an LLM to help your company fully utilize its own data is very limited. This very large barrier has given rise to the concept of Retrieval Augmented Generation (RAG), where you are using the power and capabilities of the LLM, but combining it with the knowledge and data contained within your company’s internal data repositories. This is the primary motivation for using RAG, to make new data available to the LLM and significantly increase the value you can extract from that data. Beyond internal data, it is also useful in cases where the LLM has not been trained on the data, even if it is public, like the most recent research papers or articles about a topic that is strategic to your company. In both cases, we are talking about data that was not present during the training of the LLM. You can have the latest LLM trained on the most tokens ever, exceeding 10 trillion, but if that data was not present for the training, then the LLM will be at a disadvantage to help you reach your full productivity. Ultimately, this highlights the fact that for most organizations, connecting to data an LLM is not yet familiar with is a central need for them to fully utilize that LLM. RAG is the most popular paradigm for doing this. This book focuses on showing you how to set up a RAG application with your data, as well as how to get the most out of it in various situations. I intend to give you an in-depth understanding of RAG and its importance in leveraging LLM within the context of a company's private or specific data needs. Advantages of RAG#\n",
      "Potential advantages of using RAG include improved accuracy and relevance, customization, flexibility, and expanding the model’s knowledge beyond the training data. Here is each advantage more thoroughly explored:\n",
      "\n",
      "Improved Accuracy and Relevance: RAG can significantly enhance the accuracy and relevance of responses that are generated by large language models (LLMs). RAG fetches and incorporates specific information from a database or dataset, typically in real time, and ensures that the output is based on both the model’s pre-existing knowledge and the most current and relevant data that you are providing directly. Customization and Flexibility: RAG can customize its responses based on your domain specific needs. By integrating a company's internal databases into the model's response generation process, RAG allows for outputs that are tailored to the unique context and requirements of the business. This level of customization is invaluable for creating personalized experiences and for applications requiring a high degree of specificity and detail. Expanding Model Knowledge Beyond Training Data: LLMs are limited by the scope of their training data. RAG overcomes this limitation by enabling models to access and utilize information that was not included in their initial training sets. This effectively expands the knowledge base of the model without the need for retraining, making LLMs more versatile and adaptable to new domains or rapidly evolving topics. Limitations of RAG#\n",
      "But there are some limitations to using RAG, which include dependency on the quality of the internal data, computational overhead, more complex integrations, and the potential for information overload. Dependency on Data Quality: \n",
      "When talking about how data can impact an AI model, the saying in data science circles is “garbage in, garbage out.”  Meaning, if you give a model bad data, it will give you bad results. RAG is no different. The effectiveness of RAG is directly tied to the quality of the data it retrieves. If the underlying database or dataset contains outdated, biased, or inaccurate information, the outputs generated by RAG will likely suffer from the same issues. Need for Data Manipulation and Cleaning:\n",
      "Data in the recesses of the company often has a lot of value to it, but it is not often in good, accessible shape. For example, data from PDF-based customer statements needs a lot of massaging to get into a format that can be useful to a RAG pipeline. Computational Overhead: \n",
      "A RAG pipeline introduces a host of new computational steps into the response generation process, including data retrieval, processing, and integration. LLMs are getting faster every day, but even the fastest response can be more than a second, and some can take several seconds. If you combine that with other data processing steps, and possibly multiple LLM calls, the result can be a very significant increase in the time it takes to receive a response. This all leads to increased computational overhead, affecting the efficiency and scalability of the entire system. As with any other IT initiative, an organization must balance the benefits of enhanced accuracy and customization against the resource requirements and potential latency introduced by these additional processes. Data Storage Explosion: Complexity in Integration and Maintenance: \n",
      "Traditionally, your data resides in a data source which is queried in various ways to be made available to your internal and external systems. But with RAG, your data resides in multiple forms and locations, such as vectors in a vector database, that represent the same data, but in a different format. Add in the complexity of connecting these various data sources to LLMs and relevant technical mechanisms like vector searches, and you have a significant increase in complexity. This increased complexity can be resource-intensive. Maintaining this integration over time, especially as data sources evolve or expand, adds even more complexity and cost. Organizations need to invest in technical expertise and infrastructure to effectively leverage RAG capabilities while accounting for the rapid increase in complexities these systems bring with them. Potential for Information Overload: \n",
      "It is very possible for RAG-based systems to pull in too much information. It is just as important to implement mechanisms to address this issue as it is to handle times when not enough relevant information is found. Determining the relevance and importance of retrieved information to be included in the final output requires sophisticated filtering and ranking mechanisms. Without these, the quality of the generated content could be compromised by an excess of unnecessary or marginally relevant details. RAG Vocabulary#\n",
      "Now is as good a time as any to review some vocabulary that should help you get familiar with the various concepts in RAG. This is not an exhaustive list, but understanding these core concepts should help you understand everything else we teach you about RAG in a more effective way:\n",
      "Large Language Model (LLM)\n",
      "Most of this book will deal with LLMs. LLMs are generative AI technologies that focus on generating text. There are also generative models that generate images from text prompts, while others generate video from text prompts. There are other models that generate text descriptions from images. We will talk about these other types of models in Chapter 16, Going Beyond the LLM. But for most of the book, I felt it would keep things simple and let you focus on the core principles of RAG if we focus on the type of model that most RAG pipelines use, the LLM. But I did want to make sure it was clear, that while the book focuses primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images and videos. Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta LLaMA models, Google’s PaLM and Gemini models, and Anthropic’s Claude models. Foundation model\n",
      "A foundation model is the base model for most LLMs. In the case of ChatGPT, the foundation model is based on the GPT (Generative Pre-trained Transformer) architecture, and it was fine-tuned for Chat. The specific model used for ChatGPT is not publicly disclosed. The base GPT model cannot talk with you in chatbot-style like ChatGPT does. It had to get further trained to gain that skill. But there are many other skills these foundation models can be fine-tuned for. LLaMA 2 is a foundation model and because it is open source, there are many spin offs that have been fine-tuned for many applications, such as medical research and conversation. Most of the models we talk about are very close to foundation models, but will likely be fine-tuned for at least conversational capabilities. Parameters and Biases\n",
      "We are trying to keep our focus on RAG primarily in this book, but it will be helpful for you to understand what parameters and biases are in LLM models. In machine learning models in general, including LLMs, parameters and biases are the learnable variables that the model adjusts during the training process to improve its performance on a given task. Parameters are the weights associated with the connections between neurons in the model's architecture. These weights determine the strength and importance of each connection and are updated during training to minimize the difference between the model's predictions and the expected outputs. Biases, on the other hand, are additional values added to the weighted sum of inputs at each neuron. They help the model learn an offset or shift in the data, allowing for more flexibility in fitting the training data. Together, parameters and biases form the learnable components of the model that are fine-tuned using the training data to improve the model's performance on specific tasks or domains. Understanding parameters and biases is important in the context of RAG because the process of training and fine-tuning LLMs involves adjusting these variables. I mention foundation models, where the initial training involves setting these parameters and biases. However, fine-tuning an LLM by further adjusting its parameters and biases can significantly impact its performance and behavior within a RAG pipeline. By adapting the model to specific domains, writing styles, or tasks, you can improve the accuracy and relevance of the LLM's responses when it is used to generate output based on the retrieved information. This, in turn, can lead to better overall performance of the RAG system in terms of providing more useful and context-appropriate answers to user queries. Therefore, choosing an LLM that is properly fine-tuned for your domain, or applying the fine-tuning yourself, is crucial for optimizing the most important element of your RAG pipeline. Prompting, Prompt Design, Prompt Engineering\n",
      "These terms are sometimes used interchangeably, but technically, while they all have to do with prompting, they do have fairly different meanings. Prompting is the act of sending a query or “prompt” into an LLM. Prompt design refers to the strategy you take to “design” the prompt you will send to the LLM. There are many different prompt design strategies that work in different scenarios, and we will review many of these in Chapter 13, Utilizing Prompt Engineering to Improve RAG Efforts. We will also review prompt engineering in chapter 13. Prompt engineering focuses more on the technical aspects surrounding the prompt that you use to improve the outputs from the LLM. For example, you may break up a complex query into two or three different LLM interactions, “engineering” it better to achieve superior results. Inference\n",
      "We will use the term ‘inference’ from time to time. Generally, this refers to the process of the LLM generating outputs or predictions based on given inputs using a pre-trained language model. But a key aspect of inference is that with an LLM, particularly with the cloud-based providers, you are charged based on the inference. Context Window\n",
      "A context window, in the context of LLMs, refers to the maximum number of tokens (words, subwords, or characters) that the model can process as input or generate as output in a single pass. It determines the amount of text the model can \"see\" or \"attend to\" at once when making predictions or generating responses. The context window size is a key parameter of the model architecture and is typically fixed during model training. It directly relates to the input size of the model, as it sets an upper limit on the number of tokens that can be fed into the model at a time. For example, if a model has a context window size of 4,096 tokens, it means that the model can process and generate sequences of up to 4,096 tokens. When processing longer texts, such as documents or conversations, the input needs to be divided into smaller segments that fit within the context window. This is often done using techniques like sliding windows or truncation. The size of the context window has implications for the model's ability to understand and maintain long-range dependencies and context. Models with larger context windows can capture and utilize more contextual information when generating responses, which can lead to more coherent and contextually relevant outputs. However, increasing the context window size also increases the computational resources required to train and run the model. In the context of RAG, the context window size is particularly important because it determines how much information from the retrieved documents can be effectively utilized by the model when generating the final response. Recent advancements in language models have led to the development of models with significantly larger context windows, enabling them to process and retain more information from the retrieved sources. Fine-Tuning in 2 Flavors: Full Model Fine Tuning (FMFT) and Parameter Efficient Fine Tuning (PEFT)\n",
      "Full model fine-tuning (FMFT) is where you take a foundation model and train it further to gain new capabilities. You could simply give it new knowledge for a specific domain, or you could give it a skill, like being a conversational chat-bot. FMFT updates all of the parameters and biases in the model. PEFT is a type of fine-tuning, where you focus only on specific parts of the parameters or biases when you fine-tune the model, but with a similar goal as general fine-tuning. The latest research in this area shows that you can achieve similar results to FMFT with far less cost, time commitment, and data. While this book is not focused on fine-tuning, it is a very valid strategy to try to use a model fine-tuned with your own data to give it more knowledge from your domain or to give it more of “voice” from your domain. For example, you could train it to talk more like a scientist than a generic foundation model, if using this in a scientific field. Or if you are developing in a legal field, you may want it to sound more like a lawyer. Vector Store or Vector Database? Both! All vector databases are vector stores, but not all vector stores are vector databases. Ok, while you get out your chalkboard to draw the Vinn diagram, I continue to explain this statement. There are ways to store vectors that are not full databases. They are simply storage devices for vectors. So to encompass all possible ways to store vectors, LangChain calls them all vector stores. Let’s do the same! Just know that not all of the “vector stores” that LangChain connects with are officially considered vector databases, but in general, most of them are and many people refer to all of them as vector databases. Phew, glad we cleared that up! Vectors, Vectors, Vectors! A vector is a mathematical representation of your data. They are often referred to as the embeddings when talking specifically about natural language processing and LLMs. Vectors are one of the most important concepts to understand and there are many different parts of a RAG pipeline that utilize vectors. I felt it was bigger than just a quick definition, so I go into much more depth in the much larger next section dedicated to vectors. And beyond that, we literally spend two chapters (6 & 7) going over vectors and how they are used to find similar content. Vectors#\n",
      "It could be argued that understanding vectors and all the ways they are used in RAG is the most important part of this entire book. As mentioned above, vectors are simply the mathematical representations of your external data, and they are often referred to as embeddings. These representations capture semantic information in a format that can be processed by algorithms, facilitating tasks such as similarity search, which is a crucial step in the RAG process. Vectors typically have a specific dimension based on how many numbers are represented by them. For example, this is a 4 dimensional vector: [0.123, 0.321, 0.312, 0.231]\n",
      "\n",
      "If you didn’t know we were talking about vectors and you saw this in Python code, you might recognize this as a list of 4 floating points, and you aren’t too far off. Typically though, when working with vectors in Python, you actually want to recognize them as a Numpy Array. Numpy Arrays are generally more machine learning friendly because they are optimized to be processed much faster and efficiently than python lists, and they are more broadly recognized as the defacto representation of embeddings across machine learning packages like SciPy, Pandas, Scikit-Learn, TensorFlow, Keras, Pytorch, and many others. Numpy also enables you to perform vectorized math directly on the Numpy Array, such as performing element-wise operations, without having to code in loops and other approaches you might have to use if using a different type of sequence. When working with vectors for vectorization, they are often hundreds, or thousands of dimensions, which refers to the number of floating points present in the vector. So a 1024 dimension vector literally has 1024 floating points in a Numpy Array. Higher dimensionality can capture more detailed semantic information, which is crucial for accurately matching query inputs with relevant documents or data in RAG applications. In chapter 7, we cover the key role vectors and vector databases play In RAG implementation. And then in chapter 8, we will dive more into the concept of similarity searches, which utilize vectors to conduct the search much faster and efficiently. These are key concepts that will help you gain a much deeper understanding into how to better implement a RAG pipeline. Implementing RAG in AI Applications#\n",
      "Retrieval Augmented Generation (RAG) is rapidly becoming a cornerstone of GenAI platforms in the corporate world. RAG combines the power of information retrieval of internal or “new” data with generative language models to enhance the quality and relevance of generated text. This technique can be particularly useful for companies across various industries to improve their products, services, and operational efficiencies. Some examples of how RAG can be used include:\n",
      " Customer Support and Chatbots - These can exist without RAG, but when integrated with RAG, it can connect those chatbots with past customer interactions, FAQs, support documents, and anything else that was specific to that customer. Automated Reporting - RAG can assist in creating initial drafts or summarizing existing articles, research papers, and other types of unstructured data into more digestible formats. Product Descriptions - For e-commerce companies, RAG can be used to help generate or enhance product descriptions by retrieving information from similar products or manufacturer specifications. Searchability and Utility of Internal Knowledge Bases - RAG can improve access to internal knowledge bases. This can be achieved through the generation of summaries of documents or by providing direct answers to queries based on the content of internal documents, emails, and other resources. Searchability and Utility of General Knowledge Bases - In areas like legal and compliance, where companies need to have an understanding of a massive and growing general knowledge base, RAG can be implemented to retrieve and summarize relevant laws, regulations, and compliance documents. Other areas where this is applicable include research and development, medical, academia, patents, and technical documents. Innovation Scouting - Similar to searching general knowledge bases, but with a focus on new innovation, companies can use RAG to scan and summarize information from quality sources to identify trends and potential areas for new innovations that are relevant to that company's specialization. Content Personalization - RAG can be used by media and content platforms to personalize content recommendations or create customized summaries by retrieving information based on a user's past interactions and preferences. Product Recommendations - RAG can be used by e-commerce sites to enhance product recommendation engines, generate personalized descriptions, or highlight features based on the browsing and purchasing history of customers. Training and Education - RAG can be used by education organizations and corporate training programs to generate or customize learning materials based on specific needs and knowledge levels of the learners. With RAG, a much deeper level of internal knowledge from the organization can be incorporated into the educational curriculum in very customized ways to the individual or role. This book will help you understand how you can implement all of these game-changing initiatives in your company. Comparing RAG with Conventional Generative AI#\n",
      "Conventional Generative AI has already shown to be a revolutionary change for companies, helping their employees reach new levels of productivity. LLMs like ChatGPT are assisting users with a rapidly growing list of applications that include writing business plans, writing and improving code, writing marketing copy, and even providing healthier recipes for a specific type of diet. Ultimately, much of what users are doing is getting done faster. But conventional Generative AI does not know what it does not know. And that includes most of the internal data in your company. Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\n",
      "Established Large Language Models (LLM), what we call the foundation models, can learn in two ways:\n",
      " Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model's intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs.\n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "\n",
      "Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations? Once you have introduced the new knowledge, it will always have it! It is also how the model was originally created, by training with data, right? That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall. The reason is complicated, but in general, a model’s knowledge of facts is like a human’s long-term memory. If you memorize a long passage from a speech or book and then try to recall it a few months later, you will likely still understand the context of the information, but you may forget specific details. Whereas, adding knowledge through the input of the model is like our short-term memory, where the facts, details, and even the order of wording is all very fresh and available for recall. It is this latter scenario that lends itself better in a situation where you want successful factual recall. There is a trade-off though. Inputs are limited by the context window of the model. This is an area that is actively being addressed though. For example, ChatGPT 3.5 only had a 4,096 token context window, which is the equivalent of about 5 pages of text. When ChatGPT 4 was released, they expanded the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version that had a context window of 32,768 tokens  (40 pages). This issue is so important that they included the context window size in the name of the model. That is a strong indicator of how important the context window is! What about the latest Gemini 1.5 model? 1M token context window, or over 1,000 pages. As the context windows expand though, this has created another issue. Early models with expanded context windows were shown to lose a lot of the details, especially in the “middle” of the text. This issue is also being addressed. The Gemini 1.5 model with the 1 million token context window has performed well in tests called “needle-in-a-haystack” tests for “remembering” all details well throughout the text it can take as input. Unfortunately, the model did not perform as well in the “multiple needle’s in a haystack” tests. Anthropic’s largest version of their latest model, Claude 3 Opus, performs fairly well with contest windows of 200,000 words. Expect more effort in this area as these context windows get larger, and keep this in mind if you need to work with large amounts of text at a time. NOTE: It is important to note that token count differs from word count, as tokens include punctuation, symbols, numbers, and other text representations. How a compound word like “ice cream” is treated token-wise depends on the tokenization scheme and it can vary across LLM. But most well-known LLMs (like ChatGPT and Gemini) would be considered 2 tokens. Under certain circumstances in NLP you may argue that it should be one based on the concept that a token should represent a useful semantic unit for processing. Ultimately, when deciding between RAG and fine-tuning, consider your specific use case and requirements. RAG is generally superior for retrieving factual information that is not present in the LLM's training data or is private. It allows for dynamic integration of external knowledge without modifying the model's weights. Fine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. Keep in mind the limitations of context window sizes and the potential for overfitting when fine-tuning on a specific dataset. \n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    print(split.page_content, end=f\"\\n\\n\\n{'='*300}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed\n",
    "vectorstore: Chroma = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OllamaEmbeddings(base_url=base_url, model=embedding_model))\n",
    "\n",
    "retriever: VectorStoreRetriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of retriever is 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of retriever is {len(vectorstore.get())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorstore.get()[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "query = \"How does RAG compare with fine-tuning?\"\n",
    "relevant_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations? Once you have introduced the new knowledge, it will always have it! It is also how the model was originally created, by training with data, right? That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall. The reason is complicated, but in general, a model’s knowledge of facts is like a human’s long-term memory. If you memorize a long passage from a speech or book and then try to recall it a few months later, you will likely still understand the context of the information, but you may forget specific details. Whereas, adding knowledge through the input of the model is like our short-term memory, where the facts, details, and even the order of wording is all very fresh and available for recall. It is this latter scenario that lends itself better in a situation where you want successful factual recall. There is a trade-off though. Inputs are limited by the context window of the model. This is an area that is actively being addressed though. For example, ChatGPT 3.5 only had a 4,096 token context window, which is the equivalent of about 5 pages of text. When ChatGPT 4 was released, they expanded the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version that had a context window of 32,768 tokens  (40 pages). This issue is so important that they included the context window size in the name of the model. That is a strong indicator of how important the context window is! What about the latest Gemini 1.5 model? 1M token context window, or over 1,000 pages. As the context windows expand though, this has created another issue. Early models with expanded context windows were shown to lose a lot of the details, especially in the “middle” of the text. This issue is also being addressed. The Gemini 1.5 model with the 1 million token context window has performed well in tests called “needle-in-a-haystack” tests for “remembering” all details well throughout the text it can take as input. Unfortunately, the model did not perform as well in the “multiple needle’s in a haystack” tests. Anthropic’s largest version of their latest model, Claude 3 Opus, performs fairly well with contest windows of 200,000 words. Expect more effort in this area as these context windows get larger, and keep this in mind if you need to work with large amounts of text at a time. NOTE: It is important to note that token count differs from word count, as tokens include punctuation, symbols, numbers, and other text representations. How a compound word like “ice cream” is treated token-wise depends on the tokenization scheme and it can vary across LLM. But most well-known LLMs (like ChatGPT and Gemini) would be considered 2 tokens. Under certain circumstances in NLP you may argue that it should be one based on the concept that a token should represent a useful semantic unit for processing. Ultimately, when deciding between RAG and fine-tuning, consider your specific use case and requirements. RAG is generally superior for retrieving factual information that is not present in the LLM's training data or is private. It allows for dynamic integration of external knowledge without modifying the model's weights. Fine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. Keep in mind the limitations of context window sizes and the potential for overfitting when fine-tuning on a specific dataset. \n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Introduction to Retrieval Augmented Generation (RAG)\n",
      "    \n",
      "Date: March 10, 2024  |  Estimated Reading Time: 15 min  |  Author: Keith Bourne\n",
      "\n",
      "  In the rapidly evolving field of artificial intelligence, Retrieval-Augmented Generation (RAG) is emerging as a significant addition to the Generative AI toolkit. RAG harnesses the strengths of Large Language Models (LLMs) and integrates them with internal data, offering a method to enhance organizational operations significantly. This book delves into the essential aspects of RAG, examining its role in augmenting the capabilities of LLMs and leveraging internal corporate data for strategic advantage. As it progresses, the book outlines the potential of RAG in business, suggesting how it can make AI applications smarter, more responsive, and aligned with organizational objectives. RAG is positioned as a key facilitator of customized, efficient, and insightful AI solutions, bridging the gap between Generative AI's potential and specific business needs. This exploration of RAG encourages readers to unlock the full potential of their corporate data, paving the way for an era of AI-driven innovation. What You Can Expect to Learn#\n",
      "Expect to launch a comprehensive journey to understand and effectively incorporate Retrieval Augmented Generation (RAG) into AI systems. You'll explore a broad spectrum of essential topics, including vector databases, the vectorization process, vector search techniques, prompt engineering and design, and the use of AI agents for RAG applications, alongside methods for evaluating and visualizing RAG outcomes. Through practical, working code examples utilizing the latest tools and technologies like LangChain and Chroma's vector database, you'll gain hands-on experience in implementing RAG in your projects. At the outset, you'll delve into the core principles of RAG, appreciating its significance in the broader landscape of Generative AI. This foundational knowledge equips you with the perspective needed to discern how RAG applications are designed and why they succeed, paving the way for innovative solution development and problem-solving in AI. You'll discover the symbiosis between Large Language Models (LLMs) and internal data to bolster organizational operations. By learning about the intricacies of this integration, particularly the process of vectorization, including the creation and management of vector databases for efficient information retrieval, you'll gain crucial skills for navigating and harnessing vast data landscapes effectively in today's data-driven environments. Gain expertise in vector search techniques, an essential skill set for identifying pertinent data within extensive datasets. Coupled with this, you'll learn strategies for prompt engineering and design, ensuring that you can craft queries that elicit precise and relevant AI responses. Explore how AI agents play a pivotal role in RAG applications, facilitating sophisticated data interaction and retrieval tasks. You'll also learn methods for evaluating and visualizing RAG implementation outcomes, providing a framework for assessing performance and impact critically. Throughout this journey, you'll engage in practical, hands-on learning, guided through the use of cutting-edge tools like LangChain and Chroma's vector database, supported by real, working code examples. These detailed coding demonstrations, grounded in current frameworks, offer a practical foray into implementing RAG in AI systems, providing a rich learning experience. Case studies and coding exercises strategically interspersed throughout your learning path highlight the application of RAG in various real-world scenarios. These insights into addressing common and complex challenges prepare you to navigate the application of RAG across diverse settings with confidence. The code will build off the same starting use case provided in the next chapter. For each topic that relates to code, we will add code that shows how that topic impacts the RAG pipeline, giving you an in-depth understanding about how your coding choices can impact the capabilities of your RAG-based application. You'll also explore optimization strategies for data retrieval and enhancing the interpretability of AI-generated content. These insights are pivotal for improving the usability and effectiveness of AI applications, ensuring they are more aligned with strategic business objectives and user needs. As you progress, you'll gain a deeper understanding of how RAG can revolutionize AI applications, making them more intelligent, responsive, and tailored to specific requirements. The potential of RAG to facilitate personalized, efficient, and insightful AI solutions is thoroughly examined, bridging the theoretical and practical divides. Throughout this learning experience, a spirit of exploration and experimentation is encouraged, aiming to unlock the full potential of data through RAG, fostering innovation, and advancing the domain of AI-driven solutions. By the end, you will have gained comprehensive knowledge and practical skills in RAG, equipping you to contribute to the evolution of AI technologies and applications in your business and beyond. Understanding RAG: Basics and Principles#\n",
      "Modern day large language models (LLM) are impressive, but they have never seen your company’s private data (hopefully!). This means the ability of an LLM to help your company fully utilize its own data is very limited. This very large barrier has given rise to the concept of Retrieval Augmented Generation (RAG), where you are using the power and capabilities of the LLM, but combining it with the knowledge and data contained within your company’s internal data repositories. This is the primary motivation for using RAG, to make new data available to the LLM and significantly increase the value you can extract from that data. Beyond internal data, it is also useful in cases where the LLM has not been trained on the data, even if it is public, like the most recent research papers or articles about a topic that is strategic to your company. In both cases, we are talking about data that was not present during the training of the LLM. You can have the latest LLM trained on the most tokens ever, exceeding 10 trillion, but if that data was not present for the training, then the LLM will be at a disadvantage to help you reach your full productivity. Ultimately, this highlights the fact that for most organizations, connecting to data an LLM is not yet familiar with is a central need for them to fully utilize that LLM. RAG is the most popular paradigm for doing this. This book focuses on showing you how to set up a RAG application with your data, as well as how to get the most out of it in various situations. I intend to give you an in-depth understanding of RAG and its importance in leveraging LLM within the context of a company's private or specific data needs. Advantages of RAG#\n",
      "Potential advantages of using RAG include improved accuracy and relevance, customization, flexibility, and expanding the model’s knowledge beyond the training data. Here is each advantage more thoroughly explored:\n",
      "\n",
      "Improved Accuracy and Relevance: RAG can significantly enhance the accuracy and relevance of responses that are generated by large language models (LLMs). RAG fetches and incorporates specific information from a database or dataset, typically in real time, and ensures that the output is based on both the model’s pre-existing knowledge and the most current and relevant data that you are providing directly. Customization and Flexibility: RAG can customize its responses based on your domain specific needs. By integrating a company's internal databases into the model's response generation process, RAG allows for outputs that are tailored to the unique context and requirements of the business. This level of customization is invaluable for creating personalized experiences and for applications requiring a high degree of specificity and detail. Expanding Model Knowledge Beyond Training Data: LLMs are limited by the scope of their training data. RAG overcomes this limitation by enabling models to access and utilize information that was not included in their initial training sets. This effectively expands the knowledge base of the model without the need for retraining, making LLMs more versatile and adaptable to new domains or rapidly evolving topics. Limitations of RAG#\n",
      "But there are some limitations to using RAG, which include dependency on the quality of the internal data, computational overhead, more complex integrations, and the potential for information overload. Dependency on Data Quality: \n",
      "When talking about how data can impact an AI model, the saying in data science circles is “garbage in, garbage out.”  Meaning, if you give a model bad data, it will give you bad results. RAG is no different. The effectiveness of RAG is directly tied to the quality of the data it retrieves. If the underlying database or dataset contains outdated, biased, or inaccurate information, the outputs generated by RAG will likely suffer from the same issues. Need for Data Manipulation and Cleaning:\n",
      "Data in the recesses of the company often has a lot of value to it, but it is not often in good, accessible shape. For example, data from PDF-based customer statements needs a lot of massaging to get into a format that can be useful to a RAG pipeline. Computational Overhead: \n",
      "A RAG pipeline introduces a host of new computational steps into the response generation process, including data retrieval, processing, and integration. LLMs are getting faster every day, but even the fastest response can be more than a second, and some can take several seconds. If you combine that with other data processing steps, and possibly multiple LLM calls, the result can be a very significant increase in the time it takes to receive a response. This all leads to increased computational overhead, affecting the efficiency and scalability of the entire system. As with any other IT initiative, an organization must balance the benefits of enhanced accuracy and customization against the resource requirements and potential latency introduced by these additional processes. Data Storage Explosion: Complexity in Integration and Maintenance: \n",
      "Traditionally, your data resides in a data source which is queried in various ways to be made available to your internal and external systems. But with RAG, your data resides in multiple forms and locations, such as vectors in a vector database, that represent the same data, but in a different format. Add in the complexity of connecting these various data sources to LLMs and relevant technical mechanisms like vector searches, and you have a significant increase in complexity. This increased complexity can be resource-intensive. Maintaining this integration over time, especially as data sources evolve or expand, adds even more complexity and cost. Organizations need to invest in technical expertise and infrastructure to effectively leverage RAG capabilities while accounting for the rapid increase in complexities these systems bring with them. Potential for Information Overload: \n",
      "It is very possible for RAG-based systems to pull in too much information. It is just as important to implement mechanisms to address this issue as it is to handle times when not enough relevant information is found. Determining the relevance and importance of retrieved information to be included in the final output requires sophisticated filtering and ranking mechanisms. Without these, the quality of the generated content could be compromised by an excess of unnecessary or marginally relevant details. RAG Vocabulary#\n",
      "Now is as good a time as any to review some vocabulary that should help you get familiar with the various concepts in RAG. This is not an exhaustive list, but understanding these core concepts should help you understand everything else we teach you about RAG in a more effective way:\n",
      "Large Language Model (LLM)\n",
      "Most of this book will deal with LLMs. LLMs are generative AI technologies that focus on generating text. There are also generative models that generate images from text prompts, while others generate video from text prompts. There are other models that generate text descriptions from images. We will talk about these other types of models in Chapter 16, Going Beyond the LLM. But for most of the book, I felt it would keep things simple and let you focus on the core principles of RAG if we focus on the type of model that most RAG pipelines use, the LLM. But I did want to make sure it was clear, that while the book focuses primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images and videos. Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta LLaMA models, Google’s PaLM and Gemini models, and Anthropic’s Claude models. Foundation model\n",
      "A foundation model is the base model for most LLMs. In the case of ChatGPT, the foundation model is based on the GPT (Generative Pre-trained Transformer) architecture, and it was fine-tuned for Chat. The specific model used for ChatGPT is not publicly disclosed. The base GPT model cannot talk with you in chatbot-style like ChatGPT does. It had to get further trained to gain that skill. But there are many other skills these foundation models can be fine-tuned for. LLaMA 2 is a foundation model and because it is open source, there are many spin offs that have been fine-tuned for many applications, such as medical research and conversation. Most of the models we talk about are very close to foundation models, but will likely be fine-tuned for at least conversational capabilities. Parameters and Biases\n",
      "We are trying to keep our focus on RAG primarily in this book, but it will be helpful for you to understand what parameters and biases are in LLM models. In machine learning models in general, including LLMs, parameters and biases are the learnable variables that the model adjusts during the training process to improve its performance on a given task. Parameters are the weights associated with the connections between neurons in the model's architecture. These weights determine the strength and importance of each connection and are updated during training to minimize the difference between the model's predictions and the expected outputs. Biases, on the other hand, are additional values added to the weighted sum of inputs at each neuron. They help the model learn an offset or shift in the data, allowing for more flexibility in fitting the training data. Together, parameters and biases form the learnable components of the model that are fine-tuned using the training data to improve the model's performance on specific tasks or domains. Understanding parameters and biases is important in the context of RAG because the process of training and fine-tuning LLMs involves adjusting these variables. I mention foundation models, where the initial training involves setting these parameters and biases. However, fine-tuning an LLM by further adjusting its parameters and biases can significantly impact its performance and behavior within a RAG pipeline. By adapting the model to specific domains, writing styles, or tasks, you can improve the accuracy and relevance of the LLM's responses when it is used to generate output based on the retrieved information. This, in turn, can lead to better overall performance of the RAG system in terms of providing more useful and context-appropriate answers to user queries. Therefore, choosing an LLM that is properly fine-tuned for your domain, or applying the fine-tuning yourself, is crucial for optimizing the most important element of your RAG pipeline. Prompting, Prompt Design, Prompt Engineering\n",
      "These terms are sometimes used interchangeably, but technically, while they all have to do with prompting, they do have fairly different meanings. Prompting is the act of sending a query or “prompt” into an LLM. Prompt design refers to the strategy you take to “design” the prompt you will send to the LLM. There are many different prompt design strategies that work in different scenarios, and we will review many of these in Chapter 13, Utilizing Prompt Engineering to Improve RAG Efforts. We will also review prompt engineering in chapter 13. Prompt engineering focuses more on the technical aspects surrounding the prompt that you use to improve the outputs from the LLM. For example, you may break up a complex query into two or three different LLM interactions, “engineering” it better to achieve superior results. Inference\n",
      "We will use the term ‘inference’ from time to time. Generally, this refers to the process of the LLM generating outputs or predictions based on given inputs using a pre-trained language model. But a key aspect of inference is that with an LLM, particularly with the cloud-based providers, you are charged based on the inference. Context Window\n",
      "A context window, in the context of LLMs, refers to the maximum number of tokens (words, subwords, or characters) that the model can process as input or generate as output in a single pass. It determines the amount of text the model can \"see\" or \"attend to\" at once when making predictions or generating responses. The context window size is a key parameter of the model architecture and is typically fixed during model training. It directly relates to the input size of the model, as it sets an upper limit on the number of tokens that can be fed into the model at a time. For example, if a model has a context window size of 4,096 tokens, it means that the model can process and generate sequences of up to 4,096 tokens. When processing longer texts, such as documents or conversations, the input needs to be divided into smaller segments that fit within the context window. This is often done using techniques like sliding windows or truncation. The size of the context window has implications for the model's ability to understand and maintain long-range dependencies and context. Models with larger context windows can capture and utilize more contextual information when generating responses, which can lead to more coherent and contextually relevant outputs. However, increasing the context window size also increases the computational resources required to train and run the model. In the context of RAG, the context window size is particularly important because it determines how much information from the retrieved documents can be effectively utilized by the model when generating the final response. Recent advancements in language models have led to the development of models with significantly larger context windows, enabling them to process and retain more information from the retrieved sources. Fine-Tuning in 2 Flavors: Full Model Fine Tuning (FMFT) and Parameter Efficient Fine Tuning (PEFT)\n",
      "Full model fine-tuning (FMFT) is where you take a foundation model and train it further to gain new capabilities. You could simply give it new knowledge for a specific domain, or you could give it a skill, like being a conversational chat-bot. FMFT updates all of the parameters and biases in the model. PEFT is a type of fine-tuning, where you focus only on specific parts of the parameters or biases when you fine-tune the model, but with a similar goal as general fine-tuning. The latest research in this area shows that you can achieve similar results to FMFT with far less cost, time commitment, and data. While this book is not focused on fine-tuning, it is a very valid strategy to try to use a model fine-tuned with your own data to give it more knowledge from your domain or to give it more of “voice” from your domain. For example, you could train it to talk more like a scientist than a generic foundation model, if using this in a scientific field. Or if you are developing in a legal field, you may want it to sound more like a lawyer. Vector Store or Vector Database? Both! All vector databases are vector stores, but not all vector stores are vector databases. Ok, while you get out your chalkboard to draw the Vinn diagram, I continue to explain this statement. There are ways to store vectors that are not full databases. They are simply storage devices for vectors. So to encompass all possible ways to store vectors, LangChain calls them all vector stores. Let’s do the same! Just know that not all of the “vector stores” that LangChain connects with are officially considered vector databases, but in general, most of them are and many people refer to all of them as vector databases. Phew, glad we cleared that up! Vectors, Vectors, Vectors! A vector is a mathematical representation of your data. They are often referred to as the embeddings when talking specifically about natural language processing and LLMs. Vectors are one of the most important concepts to understand and there are many different parts of a RAG pipeline that utilize vectors. I felt it was bigger than just a quick definition, so I go into much more depth in the much larger next section dedicated to vectors. And beyond that, we literally spend two chapters (6 & 7) going over vectors and how they are used to find similar content. Vectors#\n",
      "It could be argued that understanding vectors and all the ways they are used in RAG is the most important part of this entire book. As mentioned above, vectors are simply the mathematical representations of your external data, and they are often referred to as embeddings. These representations capture semantic information in a format that can be processed by algorithms, facilitating tasks such as similarity search, which is a crucial step in the RAG process. Vectors typically have a specific dimension based on how many numbers are represented by them. For example, this is a 4 dimensional vector: [0.123, 0.321, 0.312, 0.231]\n",
      "\n",
      "If you didn’t know we were talking about vectors and you saw this in Python code, you might recognize this as a list of 4 floating points, and you aren’t too far off. Typically though, when working with vectors in Python, you actually want to recognize them as a Numpy Array. Numpy Arrays are generally more machine learning friendly because they are optimized to be processed much faster and efficiently than python lists, and they are more broadly recognized as the defacto representation of embeddings across machine learning packages like SciPy, Pandas, Scikit-Learn, TensorFlow, Keras, Pytorch, and many others. Numpy also enables you to perform vectorized math directly on the Numpy Array, such as performing element-wise operations, without having to code in loops and other approaches you might have to use if using a different type of sequence. When working with vectors for vectorization, they are often hundreds, or thousands of dimensions, which refers to the number of floating points present in the vector. So a 1024 dimension vector literally has 1024 floating points in a Numpy Array. Higher dimensionality can capture more detailed semantic information, which is crucial for accurately matching query inputs with relevant documents or data in RAG applications. In chapter 7, we cover the key role vectors and vector databases play In RAG implementation. And then in chapter 8, we will dive more into the concept of similarity searches, which utilize vectors to conduct the search much faster and efficiently. These are key concepts that will help you gain a much deeper understanding into how to better implement a RAG pipeline. Implementing RAG in AI Applications#\n",
      "Retrieval Augmented Generation (RAG) is rapidly becoming a cornerstone of GenAI platforms in the corporate world. RAG combines the power of information retrieval of internal or “new” data with generative language models to enhance the quality and relevance of generated text. This technique can be particularly useful for companies across various industries to improve their products, services, and operational efficiencies. Some examples of how RAG can be used include:\n",
      " Customer Support and Chatbots - These can exist without RAG, but when integrated with RAG, it can connect those chatbots with past customer interactions, FAQs, support documents, and anything else that was specific to that customer. Automated Reporting - RAG can assist in creating initial drafts or summarizing existing articles, research papers, and other types of unstructured data into more digestible formats. Product Descriptions - For e-commerce companies, RAG can be used to help generate or enhance product descriptions by retrieving information from similar products or manufacturer specifications. Searchability and Utility of Internal Knowledge Bases - RAG can improve access to internal knowledge bases. This can be achieved through the generation of summaries of documents or by providing direct answers to queries based on the content of internal documents, emails, and other resources. Searchability and Utility of General Knowledge Bases - In areas like legal and compliance, where companies need to have an understanding of a massive and growing general knowledge base, RAG can be implemented to retrieve and summarize relevant laws, regulations, and compliance documents. Other areas where this is applicable include research and development, medical, academia, patents, and technical documents. Innovation Scouting - Similar to searching general knowledge bases, but with a focus on new innovation, companies can use RAG to scan and summarize information from quality sources to identify trends and potential areas for new innovations that are relevant to that company's specialization. Content Personalization - RAG can be used by media and content platforms to personalize content recommendations or create customized summaries by retrieving information based on a user's past interactions and preferences. Product Recommendations - RAG can be used by e-commerce sites to enhance product recommendation engines, generate personalized descriptions, or highlight features based on the browsing and purchasing history of customers. Training and Education - RAG can be used by education organizations and corporate training programs to generate or customize learning materials based on specific needs and knowledge levels of the learners. With RAG, a much deeper level of internal knowledge from the organization can be incorporated into the educational curriculum in very customized ways to the individual or role. This book will help you understand how you can implement all of these game-changing initiatives in your company. Comparing RAG with Conventional Generative AI#\n",
      "Conventional Generative AI has already shown to be a revolutionary change for companies, helping their employees reach new levels of productivity. LLMs like ChatGPT are assisting users with a rapidly growing list of applications that include writing business plans, writing and improving code, writing marketing copy, and even providing healthier recipes for a specific type of diet. Ultimately, much of what users are doing is getting done faster. But conventional Generative AI does not know what it does not know. And that includes most of the internal data in your company. Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\n",
      "Established Large Language Models (LLM), what we call the foundation models, can learn in two ways:\n",
      " Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model's intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs.\n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(doc.page_content, end=f\"\\n\\n\\n{'='*300}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text content from the splits\n",
    "tfidf_documents = [split.page_content for split in splits]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tfidf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary, term frequencies, and corresponding IDF values\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "tf_values = tfidf_matrix.toarray()\n",
    "idf_values = tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples containing word, TF, and IDF values\n",
    "word_stats = list(zip(vocab, tf_values.sum(axis=0), idf_values))\n",
    "\n",
    "# Sort the list by IDF values in descending order\n",
    "word_stats.sort(key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tTF\t\tIDF\n",
      "----\t\t--\t\t---\n",
      "000         \t0.04\t\t1.41\n",
      "1024        \t0.01\t\t1.41\n",
      "123         \t0.00\t\t1.41\n",
      "13          \t0.01\t\t1.41\n",
      "15          \t0.00\t\t1.41\n",
      "16          \t0.00\t\t1.41\n",
      "192         \t0.02\t\t1.41\n",
      "1m          \t0.02\t\t1.41\n",
      "200         \t0.02\t\t1.41\n",
      "2024        \t0.00\t\t1.41\n"
     ]
    }
   ],
   "source": [
    "# Print the grid of top 10 words, TF, and IDF values\n",
    "print(\"Word\\t\\tTF\\t\\tIDF\")\n",
    "print(\"----\\t\\t--\\t\\t---\")\n",
    "for word, tf, idf in word_stats[:10]:\n",
    "    print(f\"{word:<12}\\t{tf:.2f}\\t\\t{idf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Top Document:\n",
      " \n",
      "\n",
      "      Introduction to Retrieval Augmented Generation (RAG)\n",
      "    \n",
      "Date: March 10, 2024  |  Estimated Reading Time: 15 min  |  Author: Keith Bourne\n",
      "\n",
      "  In the rapidly evolving field of artificial intelligence, Retrieval-Augmented Generation (RAG) is emerging as a significant addition to the Generative AI toolkit. RAG harnesses the strengths of Large Language Models (LLMs) and integrates them with internal data, offering a method to enhance organizational operations significantly. This book delves into the essential aspects of RAG, examining its role in augmenting the capabilities of LLMs and leveraging internal corporate data for strategic advantage. As it progresses, the book outlines the potential of RAG in business, suggesting how it can make AI applications smarter, more responsive, and aligned with organizational objectives. RAG is positioned as a key facilitator of customized, efficient, and insightful AI solutions, bridging the gap between Generative AI's potential and specific business needs. This exploration of RAG encourages readers to unlock the full potential of their corporate data, paving the way for an era of AI-driven innovation. What You Can Expect to Learn#\n",
      "Expect to launch a comprehensive journey to understand and effectively incorporate Retrieval Augmented Generation (RAG) into AI systems. You'll explore a broad spectrum of essential topics, including vector databases, the vectorization process, vector search techniques, prompt engineering and design, and the use of AI agents for RAG applications, alongside methods for evaluating and visualizing RAG outcomes. Through practical, working code examples utilizing the latest tools and technologies like LangChain and Chroma's vector database, you'll gain hands-on experience in implementing RAG in your projects. At the outset, you'll delve into the core principles of RAG, appreciating its significance in the broader landscape of Generative AI. This foundational knowledge equips you with the perspective needed to discern how RAG applications are designed and why they succeed, paving the way for innovative solution development and problem-solving in AI. You'll discover the symbiosis between Large Language Models (LLMs) and internal data to bolster organizational operations. By learning about the intricacies of this integration, particularly the process of vectorization, including the creation and management of vector databases for efficient information retrieval, you'll gain crucial skills for navigating and harnessing vast data landscapes effectively in today's data-driven environments. Gain expertise in vector search techniques, an essential skill set for identifying pertinent data within extensive datasets. Coupled with this, you'll learn strategies for prompt engineering and design, ensuring that you can craft queries that elicit precise and relevant AI responses. Explore how AI agents play a pivotal role in RAG applications, facilitating sophisticated data interaction and retrieval tasks. You'll also learn methods for evaluating and visualizing RAG implementation outcomes, providing a framework for assessing performance and impact critically. Throughout this journey, you'll engage in practical, hands-on learning, guided through the use of cutting-edge tools like LangChain and Chroma's vector database, supported by real, working code examples. These detailed coding demonstrations, grounded in current frameworks, offer a practical foray into implementing RAG in AI systems, providing a rich learning experience. Case studies and coding exercises strategically interspersed throughout your learning path highlight the application of RAG in various real-world scenarios. These insights into addressing common and complex challenges prepare you to navigate the application of RAG across diverse settings with confidence. The code will build off the same starting use case provided in the next chapter. For each topic that relates to code, we will add code that shows how that topic impacts the RAG pipeline, giving you an in-depth understanding about how your coding choices can impact the capabilities of your RAG-based application. You'll also explore optimization strategies for data retrieval and enhancing the interpretability of AI-generated content. These insights are pivotal for improving the usability and effectiveness of AI applications, ensuring they are more aligned with strategic business objectives and user needs. As you progress, you'll gain a deeper understanding of how RAG can revolutionize AI applications, making them more intelligent, responsive, and tailored to specific requirements. The potential of RAG to facilitate personalized, efficient, and insightful AI solutions is thoroughly examined, bridging the theoretical and practical divides. Throughout this learning experience, a spirit of exploration and experimentation is encouraged, aiming to unlock the full potential of data through RAG, fostering innovation, and advancing the domain of AI-driven solutions. By the end, you will have gained comprehensive knowledge and practical skills in RAG, equipping you to contribute to the evolution of AI technologies and applications in your business and beyond. Understanding RAG: Basics and Principles#\n",
      "Modern day large language models (LLM) are impressive, but they have never seen your company’s private data (hopefully!). This means the ability of an LLM to help your company fully utilize its own data is very limited. This very large barrier has given rise to the concept of Retrieval Augmented Generation (RAG), where you are using the power and capabilities of the LLM, but combining it with the knowledge and data contained within your company’s internal data repositories. This is the primary motivation for using RAG, to make new data available to the LLM and significantly increase the value you can extract from that data. Beyond internal data, it is also useful in cases where the LLM has not been trained on the data, even if it is public, like the most recent research papers or articles about a topic that is strategic to your company. In both cases, we are talking about data that was not present during the training of the LLM. You can have the latest LLM trained on the most tokens ever, exceeding 10 trillion, but if that data was not present for the training, then the LLM will be at a disadvantage to help you reach your full productivity. Ultimately, this highlights the fact that for most organizations, connecting to data an LLM is not yet familiar with is a central need for them to fully utilize that LLM. RAG is the most popular paradigm for doing this. This book focuses on showing you how to set up a RAG application with your data, as well as how to get the most out of it in various situations. I intend to give you an in-depth understanding of RAG and its importance in leveraging LLM within the context of a company's private or specific data needs. Advantages of RAG#\n",
      "Potential advantages of using RAG include improved accuracy and relevance, customization, flexibility, and expanding the model’s knowledge beyond the training data. Here is each advantage more thoroughly explored:\n",
      "\n",
      "Improved Accuracy and Relevance: RAG can significantly enhance the accuracy and relevance of responses that are generated by large language models (LLMs). RAG fetches and incorporates specific information from a database or dataset, typically in real time, and ensures that the output is based on both the model’s pre-existing knowledge and the most current and relevant data that you are providing directly. Customization and Flexibility: RAG can customize its responses based on your domain specific needs. By integrating a company's internal databases into the model's response generation process, RAG allows for outputs that are tailored to the unique context and requirements of the business. This level of customization is invaluable for creating personalized experiences and for applications requiring a high degree of specificity and detail. Expanding Model Knowledge Beyond Training Data: LLMs are limited by the scope of their training data. RAG overcomes this limitation by enabling models to access and utilize information that was not included in their initial training sets. This effectively expands the knowledge base of the model without the need for retraining, making LLMs more versatile and adaptable to new domains or rapidly evolving topics. Limitations of RAG#\n",
      "But there are some limitations to using RAG, which include dependency on the quality of the internal data, computational overhead, more complex integrations, and the potential for information overload. Dependency on Data Quality: \n",
      "When talking about how data can impact an AI model, the saying in data science circles is “garbage in, garbage out.”  Meaning, if you give a model bad data, it will give you bad results. RAG is no different. The effectiveness of RAG is directly tied to the quality of the data it retrieves. If the underlying database or dataset contains outdated, biased, or inaccurate information, the outputs generated by RAG will likely suffer from the same issues. Need for Data Manipulation and Cleaning:\n",
      "Data in the recesses of the company often has a lot of value to it, but it is not often in good, accessible shape. For example, data from PDF-based customer statements needs a lot of massaging to get into a format that can be useful to a RAG pipeline. Computational Overhead: \n",
      "A RAG pipeline introduces a host of new computational steps into the response generation process, including data retrieval, processing, and integration. LLMs are getting faster every day, but even the fastest response can be more than a second, and some can take several seconds. If you combine that with other data processing steps, and possibly multiple LLM calls, the result can be a very significant increase in the time it takes to receive a response. This all leads to increased computational overhead, affecting the efficiency and scalability of the entire system. As with any other IT initiative, an organization must balance the benefits of enhanced accuracy and customization against the resource requirements and potential latency introduced by these additional processes. Data Storage Explosion: Complexity in Integration and Maintenance: \n",
      "Traditionally, your data resides in a data source which is queried in various ways to be made available to your internal and external systems. But with RAG, your data resides in multiple forms and locations, such as vectors in a vector database, that represent the same data, but in a different format. Add in the complexity of connecting these various data sources to LLMs and relevant technical mechanisms like vector searches, and you have a significant increase in complexity. This increased complexity can be resource-intensive. Maintaining this integration over time, especially as data sources evolve or expand, adds even more complexity and cost. Organizations need to invest in technical expertise and infrastructure to effectively leverage RAG capabilities while accounting for the rapid increase in complexities these systems bring with them. Potential for Information Overload: \n",
      "It is very possible for RAG-based systems to pull in too much information. It is just as important to implement mechanisms to address this issue as it is to handle times when not enough relevant information is found. Determining the relevance and importance of retrieved information to be included in the final output requires sophisticated filtering and ranking mechanisms. Without these, the quality of the generated content could be compromised by an excess of unnecessary or marginally relevant details. RAG Vocabulary#\n",
      "Now is as good a time as any to review some vocabulary that should help you get familiar with the various concepts in RAG. This is not an exhaustive list, but understanding these core concepts should help you understand everything else we teach you about RAG in a more effective way:\n",
      "Large Language Model (LLM)\n",
      "Most of this book will deal with LLMs. LLMs are generative AI technologies that focus on generating text. There are also generative models that generate images from text prompts, while others generate video from text prompts. There are other models that generate text descriptions from images. We will talk about these other types of models in Chapter 16, Going Beyond the LLM. But for most of the book, I felt it would keep things simple and let you focus on the core principles of RAG if we focus on the type of model that most RAG pipelines use, the LLM. But I did want to make sure it was clear, that while the book focuses primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images and videos. Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta LLaMA models, Google’s PaLM and Gemini models, and Anthropic’s Claude models. Foundation model\n",
      "A foundation model is the base model for most LLMs. In the case of ChatGPT, the foundation model is based on the GPT (Generative Pre-trained Transformer) architecture, and it was fine-tuned for Chat. The specific model used for ChatGPT is not publicly disclosed. The base GPT model cannot talk with you in chatbot-style like ChatGPT does. It had to get further trained to gain that skill. But there are many other skills these foundation models can be fine-tuned for. LLaMA 2 is a foundation model and because it is open source, there are many spin offs that have been fine-tuned for many applications, such as medical research and conversation. Most of the models we talk about are very close to foundation models, but will likely be fine-tuned for at least conversational capabilities. Parameters and Biases\n",
      "We are trying to keep our focus on RAG primarily in this book, but it will be helpful for you to understand what parameters and biases are in LLM models. In machine learning models in general, including LLMs, parameters and biases are the learnable variables that the model adjusts during the training process to improve its performance on a given task. Parameters are the weights associated with the connections between neurons in the model's architecture. These weights determine the strength and importance of each connection and are updated during training to minimize the difference between the model's predictions and the expected outputs. Biases, on the other hand, are additional values added to the weighted sum of inputs at each neuron. They help the model learn an offset or shift in the data, allowing for more flexibility in fitting the training data. Together, parameters and biases form the learnable components of the model that are fine-tuned using the training data to improve the model's performance on specific tasks or domains. Understanding parameters and biases is important in the context of RAG because the process of training and fine-tuning LLMs involves adjusting these variables. I mention foundation models, where the initial training involves setting these parameters and biases. However, fine-tuning an LLM by further adjusting its parameters and biases can significantly impact its performance and behavior within a RAG pipeline. By adapting the model to specific domains, writing styles, or tasks, you can improve the accuracy and relevance of the LLM's responses when it is used to generate output based on the retrieved information. This, in turn, can lead to better overall performance of the RAG system in terms of providing more useful and context-appropriate answers to user queries. Therefore, choosing an LLM that is properly fine-tuned for your domain, or applying the fine-tuning yourself, is crucial for optimizing the most important element of your RAG pipeline. Prompting, Prompt Design, Prompt Engineering\n",
      "These terms are sometimes used interchangeably, but technically, while they all have to do with prompting, they do have fairly different meanings. Prompting is the act of sending a query or “prompt” into an LLM. Prompt design refers to the strategy you take to “design” the prompt you will send to the LLM. There are many different prompt design strategies that work in different scenarios, and we will review many of these in Chapter 13, Utilizing Prompt Engineering to Improve RAG Efforts. We will also review prompt engineering in chapter 13. Prompt engineering focuses more on the technical aspects surrounding the prompt that you use to improve the outputs from the LLM. For example, you may break up a complex query into two or three different LLM interactions, “engineering” it better to achieve superior results. Inference\n",
      "We will use the term ‘inference’ from time to time. Generally, this refers to the process of the LLM generating outputs or predictions based on given inputs using a pre-trained language model. But a key aspect of inference is that with an LLM, particularly with the cloud-based providers, you are charged based on the inference. Context Window\n",
      "A context window, in the context of LLMs, refers to the maximum number of tokens (words, subwords, or characters) that the model can process as input or generate as output in a single pass. It determines the amount of text the model can \"see\" or \"attend to\" at once when making predictions or generating responses. The context window size is a key parameter of the model architecture and is typically fixed during model training. It directly relates to the input size of the model, as it sets an upper limit on the number of tokens that can be fed into the model at a time. For example, if a model has a context window size of 4,096 tokens, it means that the model can process and generate sequences of up to 4,096 tokens. When processing longer texts, such as documents or conversations, the input needs to be divided into smaller segments that fit within the context window. This is often done using techniques like sliding windows or truncation. The size of the context window has implications for the model's ability to understand and maintain long-range dependencies and context. Models with larger context windows can capture and utilize more contextual information when generating responses, which can lead to more coherent and contextually relevant outputs. However, increasing the context window size also increases the computational resources required to train and run the model. In the context of RAG, the context window size is particularly important because it determines how much information from the retrieved documents can be effectively utilized by the model when generating the final response. Recent advancements in language models have led to the development of models with significantly larger context windows, enabling them to process and retain more information from the retrieved sources. Fine-Tuning in 2 Flavors: Full Model Fine Tuning (FMFT) and Parameter Efficient Fine Tuning (PEFT)\n",
      "Full model fine-tuning (FMFT) is where you take a foundation model and train it further to gain new capabilities. You could simply give it new knowledge for a specific domain, or you could give it a skill, like being a conversational chat-bot. FMFT updates all of the parameters and biases in the model. PEFT is a type of fine-tuning, where you focus only on specific parts of the parameters or biases when you fine-tune the model, but with a similar goal as general fine-tuning. The latest research in this area shows that you can achieve similar results to FMFT with far less cost, time commitment, and data. While this book is not focused on fine-tuning, it is a very valid strategy to try to use a model fine-tuned with your own data to give it more knowledge from your domain or to give it more of “voice” from your domain. For example, you could train it to talk more like a scientist than a generic foundation model, if using this in a scientific field. Or if you are developing in a legal field, you may want it to sound more like a lawyer. Vector Store or Vector Database? Both! All vector databases are vector stores, but not all vector stores are vector databases. Ok, while you get out your chalkboard to draw the Vinn diagram, I continue to explain this statement. There are ways to store vectors that are not full databases. They are simply storage devices for vectors. So to encompass all possible ways to store vectors, LangChain calls them all vector stores. Let’s do the same! Just know that not all of the “vector stores” that LangChain connects with are officially considered vector databases, but in general, most of them are and many people refer to all of them as vector databases. Phew, glad we cleared that up! Vectors, Vectors, Vectors! A vector is a mathematical representation of your data. They are often referred to as the embeddings when talking specifically about natural language processing and LLMs. Vectors are one of the most important concepts to understand and there are many different parts of a RAG pipeline that utilize vectors. I felt it was bigger than just a quick definition, so I go into much more depth in the much larger next section dedicated to vectors. And beyond that, we literally spend two chapters (6 & 7) going over vectors and how they are used to find similar content. Vectors#\n",
      "It could be argued that understanding vectors and all the ways they are used in RAG is the most important part of this entire book. As mentioned above, vectors are simply the mathematical representations of your external data, and they are often referred to as embeddings. These representations capture semantic information in a format that can be processed by algorithms, facilitating tasks such as similarity search, which is a crucial step in the RAG process. Vectors typically have a specific dimension based on how many numbers are represented by them. For example, this is a 4 dimensional vector: [0.123, 0.321, 0.312, 0.231]\n",
      "\n",
      "If you didn’t know we were talking about vectors and you saw this in Python code, you might recognize this as a list of 4 floating points, and you aren’t too far off. Typically though, when working with vectors in Python, you actually want to recognize them as a Numpy Array. Numpy Arrays are generally more machine learning friendly because they are optimized to be processed much faster and efficiently than python lists, and they are more broadly recognized as the defacto representation of embeddings across machine learning packages like SciPy, Pandas, Scikit-Learn, TensorFlow, Keras, Pytorch, and many others. Numpy also enables you to perform vectorized math directly on the Numpy Array, such as performing element-wise operations, without having to code in loops and other approaches you might have to use if using a different type of sequence. When working with vectors for vectorization, they are often hundreds, or thousands of dimensions, which refers to the number of floating points present in the vector. So a 1024 dimension vector literally has 1024 floating points in a Numpy Array. Higher dimensionality can capture more detailed semantic information, which is crucial for accurately matching query inputs with relevant documents or data in RAG applications. In chapter 7, we cover the key role vectors and vector databases play In RAG implementation. And then in chapter 8, we will dive more into the concept of similarity searches, which utilize vectors to conduct the search much faster and efficiently. These are key concepts that will help you gain a much deeper understanding into how to better implement a RAG pipeline. Implementing RAG in AI Applications#\n",
      "Retrieval Augmented Generation (RAG) is rapidly becoming a cornerstone of GenAI platforms in the corporate world. RAG combines the power of information retrieval of internal or “new” data with generative language models to enhance the quality and relevance of generated text. This technique can be particularly useful for companies across various industries to improve their products, services, and operational efficiencies. Some examples of how RAG can be used include:\n",
      " Customer Support and Chatbots - These can exist without RAG, but when integrated with RAG, it can connect those chatbots with past customer interactions, FAQs, support documents, and anything else that was specific to that customer. Automated Reporting - RAG can assist in creating initial drafts or summarizing existing articles, research papers, and other types of unstructured data into more digestible formats. Product Descriptions - For e-commerce companies, RAG can be used to help generate or enhance product descriptions by retrieving information from similar products or manufacturer specifications. Searchability and Utility of Internal Knowledge Bases - RAG can improve access to internal knowledge bases. This can be achieved through the generation of summaries of documents or by providing direct answers to queries based on the content of internal documents, emails, and other resources. Searchability and Utility of General Knowledge Bases - In areas like legal and compliance, where companies need to have an understanding of a massive and growing general knowledge base, RAG can be implemented to retrieve and summarize relevant laws, regulations, and compliance documents. Other areas where this is applicable include research and development, medical, academia, patents, and technical documents. Innovation Scouting - Similar to searching general knowledge bases, but with a focus on new innovation, companies can use RAG to scan and summarize information from quality sources to identify trends and potential areas for new innovations that are relevant to that company's specialization. Content Personalization - RAG can be used by media and content platforms to personalize content recommendations or create customized summaries by retrieving information based on a user's past interactions and preferences. Product Recommendations - RAG can be used by e-commerce sites to enhance product recommendation engines, generate personalized descriptions, or highlight features based on the browsing and purchasing history of customers. Training and Education - RAG can be used by education organizations and corporate training programs to generate or customize learning materials based on specific needs and knowledge levels of the learners. With RAG, a much deeper level of internal knowledge from the organization can be incorporated into the educational curriculum in very customized ways to the individual or role. This book will help you understand how you can implement all of these game-changing initiatives in your company. Comparing RAG with Conventional Generative AI#\n",
      "Conventional Generative AI has already shown to be a revolutionary change for companies, helping their employees reach new levels of productivity. LLMs like ChatGPT are assisting users with a rapidly growing list of applications that include writing business plans, writing and improving code, writing marketing copy, and even providing healthier recipes for a specific type of diet. Ultimately, much of what users are doing is getting done faster. But conventional Generative AI does not know what it does not know. And that includes most of the internal data in your company. Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\n",
      "Established Large Language Models (LLM), what we call the foundation models, can learn in two ways:\n",
      " Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model's intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What are the advantages of using RAG?\"\n",
    "\n",
    "# TD-IDF scoring for user query\n",
    "# User query embedding\n",
    "tfidf_user_query = [user_query]\n",
    "new_tfidf_matrix = tfidf_vectorizer.transform(tfidf_user_query)\n",
    "\n",
    "# Calculate cosine similarity between the new content and the original documents\n",
    "tfidf_similarity_scores = cosine_similarity(new_tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Find the index of the document with the highest similarity score\n",
    "tfidf_top_doc_index = tfidf_similarity_scores.argmax()\n",
    "\n",
    "# Print the text of the top document\n",
    "print(\"TF-IDF Top Document:\\n\", tfidf_documents[tfidf_top_doc_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING AND SAVING DOC2VEC MODEL\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text content from the splits\n",
    "doc2vec_documents = [split.page_content for split in splits]\n",
    "\n",
    "# Tokenize the documents\n",
    "doc2vec_tokenized_documents = [doc.lower().split() for doc in doc2vec_documents]\n",
    "\n",
    "# Create tagged documents for Doc2Vec\n",
    "doc2vec_tagged_documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(doc2vec_tokenized_documents)]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "# Use this version first.\n",
    "doc2vec_model = Doc2Vec(doc2vec_tagged_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# After running the previous version of model, comment the previous line out and uncomment this one. Try it with 1536D vectors.\n",
    "# doc2vec_model = Doc2Vec(doc2vec_tagged_documents, vector_size=1536, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model to a file\n",
    "doc2vec_model.save(\"bin/doc2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc2Vec Top Document:\n",
      " Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations? Once you have introduced the new knowledge, it will always have it! It is also how the model was originally created, by training with data, right? That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall. The reason is complicated, but in general, a model’s knowledge of facts is like a human’s long-term memory. If you memorize a long passage from a speech or book and then try to recall it a few months later, you will likely still understand the context of the information, but you may forget specific details. Whereas, adding knowledge through the input of the model is like our short-term memory, where the facts, details, and even the order of wording is all very fresh and available for recall. It is this latter scenario that lends itself better in a situation where you want successful factual recall. There is a trade-off though. Inputs are limited by the context window of the model. This is an area that is actively being addressed though. For example, ChatGPT 3.5 only had a 4,096 token context window, which is the equivalent of about 5 pages of text. When ChatGPT 4 was released, they expanded the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version that had a context window of 32,768 tokens  (40 pages). This issue is so important that they included the context window size in the name of the model. That is a strong indicator of how important the context window is! What about the latest Gemini 1.5 model? 1M token context window, or over 1,000 pages. As the context windows expand though, this has created another issue. Early models with expanded context windows were shown to lose a lot of the details, especially in the “middle” of the text. This issue is also being addressed. The Gemini 1.5 model with the 1 million token context window has performed well in tests called “needle-in-a-haystack” tests for “remembering” all details well throughout the text it can take as input. Unfortunately, the model did not perform as well in the “multiple needle’s in a haystack” tests. Anthropic’s largest version of their latest model, Claude 3 Opus, performs fairly well with contest windows of 200,000 words. Expect more effort in this area as these context windows get larger, and keep this in mind if you need to work with large amounts of text at a time. NOTE: It is important to note that token count differs from word count, as tokens include punctuation, symbols, numbers, and other text representations. How a compound word like “ice cream” is treated token-wise depends on the tokenization scheme and it can vary across LLM. But most well-known LLMs (like ChatGPT and Gemini) would be considered 2 tokens. Under certain circumstances in NLP you may argue that it should be one based on the concept that a token should represent a useful semantic unit for processing. Ultimately, when deciding between RAG and fine-tuning, consider your specific use case and requirements. RAG is generally superior for retrieving factual information that is not present in the LLM's training data or is private. It allows for dynamic integration of external knowledge without modifying the model's weights. Fine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. Keep in mind the limitations of context window sizes and the potential for overfitting when fine-tuning on a specific dataset. \n"
     ]
    }
   ],
   "source": [
    "# USING DOC2VEC SAVED MODEL\n",
    "\n",
    "# Load the saved model\n",
    "loaded_doc2vec_model = Doc2Vec.load(\"bin/doc2vec_model.bin\")\n",
    "\n",
    "# Calculate the document vectors\n",
    "doc2vec_document_vectors = [loaded_doc2vec_model.dv[str(i)] for i in range(len(doc2vec_documents))]\n",
    "\n",
    "# User query for embedding\n",
    "doc2vec_user_query = [user_query]\n",
    "\n",
    "# Tokenize the new content\n",
    "doc2vec_tokenized_user_query = [content.lower().split() for content in doc2vec_user_query]\n",
    "\n",
    "# Infer the vector for the new content\n",
    "doc2vec_user_query_vector = loaded_doc2vec_model.infer_vector(doc2vec_tokenized_user_query[0])\n",
    "\n",
    "# Calculate cosine similarity between the new content vector and the document vectors\n",
    "doc2vec_similarity_scores = cosine_similarity([doc2vec_user_query_vector], doc2vec_document_vectors)\n",
    "\n",
    "# Find the index of the document with the highest similarity score\n",
    "doc2vec_top_doc_index = doc2vec_similarity_scores.argmax()\n",
    "\n",
    "# Print the text of the top document\n",
    "print(\"\\nDoc2Vec Top Document:\\n\", doc2vec_documents[doc2vec_top_doc_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/kalista/git-cuongpiger/unlocking-data-with-generative-ai-and-rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# USING BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size of BERT (base-uncased) embeddings: 768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the text content from the splits\n",
    "bert_documents = [split.page_content for split in splits]\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get the vector size of the BERT embeddings\n",
    "bert_vector_size = bert_model.config.hidden_size\n",
    "print(f\"Vector size of BERT (base-uncased) embeddings: {bert_vector_size}\\n\")\n",
    "\n",
    "# Tokenize the documents\n",
    "bert_tokenized_documents = [bert_tokenizer(doc, return_tensors='pt', max_length=512, truncation=True) for doc in bert_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the document embeddings\n",
    "bert_document_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for doc in bert_tokenized_documents:\n",
    "        bert_outputs = bert_model(**doc)\n",
    "        bert_doc_embedding = bert_outputs.last_hidden_state[0, 0, :].numpy()\n",
    "        bert_document_embeddings.append(bert_doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New content (question) for embedding\n",
    "bert_user_query = [user_query]\n",
    "\n",
    "# Tokenize the new content\n",
    "bert_tokenized_user_query = bert_tokenizer(bert_user_query[0], return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "# Calculate the embedding for the new content\n",
    "bert_user_query_embedding = []\n",
    "with torch.no_grad():\n",
    "    bert_outputs = bert_model(**bert_tokenized_user_query)\n",
    "    bert_user_query_embedding = bert_outputs.last_hidden_state[0, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Top Document:\n",
      " Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations? Once you have introduced the new knowledge, it will always have it! It is also how the model was originally created, by training with data, right? That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall. The reason is complicated, but in general, a model’s knowledge of facts is like a human’s long-term memory. If you memorize a long passage from a speech or book and then try to recall it a few months later, you will likely still understand the context of the information, but you may forget specific details. Whereas, adding knowledge through the input of the model is like our short-term memory, where the facts, details, and even the order of wording is all very fresh and available for recall. It is this latter scenario that lends itself better in a situation where you want successful factual recall. There is a trade-off though. Inputs are limited by the context window of the model. This is an area that is actively being addressed though. For example, ChatGPT 3.5 only had a 4,096 token context window, which is the equivalent of about 5 pages of text. When ChatGPT 4 was released, they expanded the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version that had a context window of 32,768 tokens  (40 pages). This issue is so important that they included the context window size in the name of the model. That is a strong indicator of how important the context window is! What about the latest Gemini 1.5 model? 1M token context window, or over 1,000 pages. As the context windows expand though, this has created another issue. Early models with expanded context windows were shown to lose a lot of the details, especially in the “middle” of the text. This issue is also being addressed. The Gemini 1.5 model with the 1 million token context window has performed well in tests called “needle-in-a-haystack” tests for “remembering” all details well throughout the text it can take as input. Unfortunately, the model did not perform as well in the “multiple needle’s in a haystack” tests. Anthropic’s largest version of their latest model, Claude 3 Opus, performs fairly well with contest windows of 200,000 words. Expect more effort in this area as these context windows get larger, and keep this in mind if you need to work with large amounts of text at a time. NOTE: It is important to note that token count differs from word count, as tokens include punctuation, symbols, numbers, and other text representations. How a compound word like “ice cream” is treated token-wise depends on the tokenization scheme and it can vary across LLM. But most well-known LLMs (like ChatGPT and Gemini) would be considered 2 tokens. Under certain circumstances in NLP you may argue that it should be one based on the concept that a token should represent a useful semantic unit for processing. Ultimately, when deciding between RAG and fine-tuning, consider your specific use case and requirements. RAG is generally superior for retrieving factual information that is not present in the LLM's training data or is private. It allows for dynamic integration of external knowledge without modifying the model's weights. Fine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. Keep in mind the limitations of context window sizes and the potential for overfitting when fine-tuning on a specific dataset. \n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity between the new content embedding and the document embeddings\n",
    "bert_similarity_scores = cosine_similarity([bert_user_query_embedding], bert_document_embeddings)\n",
    "\n",
    "# Find the index of the document with the highest similarity score\n",
    "bert_top_doc_index = bert_similarity_scores.argmax()\n",
    "\n",
    "# Print the text of the top document\n",
    "print(\"BERT Top Document:\\n\", bert_documents[bert_top_doc_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OllamaEmbeddings(base_url=base_url, model=embedding_model))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Document:\n",
      " Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations? Once you have introduced the new knowledge, it will always have it! It is also how the model was originally created, by training with data, right? That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall. The reason is complicated, but in general, a model’s knowledge of facts is like a human’s long-term memory. If you memorize a long passage from a speech or book and then try to recall it a few months later, you will likely still understand the context of the information, but you may forget specific details. Whereas, adding knowledge through the input of the model is like our short-term memory, where the facts, details, and even the order of wording is all very fresh and available for recall. It is this latter scenario that lends itself better in a situation where you want successful factual recall. There is a trade-off though. Inputs are limited by the context window of the model. This is an area that is actively being addressed though. For example, ChatGPT 3.5 only had a 4,096 token context window, which is the equivalent of about 5 pages of text. When ChatGPT 4 was released, they expanded the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version that had a context window of 32,768 tokens  (40 pages). This issue is so important that they included the context window size in the name of the model. That is a strong indicator of how important the context window is! What about the latest Gemini 1.5 model? 1M token context window, or over 1,000 pages. As the context windows expand though, this has created another issue. Early models with expanded context windows were shown to lose a lot of the details, especially in the “middle” of the text. This issue is also being addressed. The Gemini 1.5 model with the 1 million token context window has performed well in tests called “needle-in-a-haystack” tests for “remembering” all details well throughout the text it can take as input. Unfortunately, the model did not perform as well in the “multiple needle’s in a haystack” tests. Anthropic’s largest version of their latest model, Claude 3 Opus, performs fairly well with contest windows of 200,000 words. Expect more effort in this area as these context windows get larger, and keep this in mind if you need to work with large amounts of text at a time. NOTE: It is important to note that token count differs from word count, as tokens include punctuation, symbols, numbers, and other text representations. How a compound word like “ice cream” is treated token-wise depends on the tokenization scheme and it can vary across LLM. But most well-known LLMs (like ChatGPT and Gemini) would be considered 2 tokens. Under certain circumstances in NLP you may argue that it should be one based on the concept that a token should represent a useful semantic unit for processing. Ultimately, when deciding between RAG and fine-tuning, consider your specific use case and requirements. RAG is generally superior for retrieving factual information that is not present in the LLM's training data or is private. It allows for dynamic integration of external knowledge without modifying the model's weights. Fine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. Keep in mind the limitations of context window sizes and the potential for overfitting when fine-tuning on a specific dataset. \n"
     ]
    }
   ],
   "source": [
    "# Retrieve the first result using the new content\n",
    "result = retriever.invoke(user_query)[0]\n",
    "\n",
    "# Print the retrieved document\n",
    "print(\"\\nRetrieved Document:\\n\", result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVAL and GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/kalista/git-cuongpiger/unlocking-data-with-generative-ai-and-rag/.venv/lib/python3.10/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prompt - ignore LangSmith warning, you will not need langsmith for this coding exercise\n",
    "prompt = hub.pull(\"jclemens24/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance check prompt\n",
    "relevance_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Given the following question and retrieved context, determine if the context is relevant to the question.\n",
    "    Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.\n",
    "    Return ONLY the numeric score, without any additional text or explanation.\n",
    "\n",
    "    Question: {question}\n",
    "    Retrieved Context: {retrieved_context}\n",
    "\n",
    "    Relevance Score:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(llm_output):\n",
    "    try:\n",
    "        score = float(llm_output.strip())\n",
    "        return score\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "# Chain it all together with LangChain\n",
    "def conditional_answer(x):\n",
    "    relevance_score = extract_score(x['relevance_score'])\n",
    "    if relevance_score < 4:\n",
    "        return \"I don't know.\"\n",
    "    else:\n",
    "        return x['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=base_url, \n",
    "                 model=embedding_model,\n",
    "                 temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | RunnableParallel(\n",
    "        {\"relevance_score\": (\n",
    "            RunnablePassthrough()\n",
    "            | (lambda x: relevance_prompt_template.format(question=x['question'], retrieved_context=x['context']))\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ), \"answer\": (\n",
    "            RunnablePassthrough()\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )}\n",
    "    )\n",
    "    | RunnablePassthrough().assign(final_answer=conditional_answer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: It seems like you've provided a large block of text about Retrieval Augmented Generation (RAG) and its applications in various industries. I'll try to summarize the key points for you:\n",
      "\n",
      "**What is RAG?**\n",
      "\n",
      "RAG combines information retrieval with generative language models to enhance the quality and relevance of generated text.\n",
      "\n",
      "**Applications of RAG:**\n",
      "\n",
      "1. Customer Support and Chatbots\n",
      "2. Automated Reporting\n",
      "3. Product Descriptions\n",
      "4. Searchability and Utility of Internal Knowledge Bases\n",
      "5. Searchability and Utility of General Knowledge Bases\n",
      "6. Innovation Scouting\n",
      "7. Content Personalization\n",
      "8. Product Recommendations\n",
      "9. Training and Education\n",
      "\n",
      "**Benefits of RAG:**\n",
      "\n",
      "1. Combines internal data with generative language models to create more accurate and relevant text.\n",
      "2. Can access all internal data resources, not just a small portion like conventional Generative AI.\n",
      "\n",
      "**Comparison with Conventional Generative AI:**\n",
      "\n",
      "Conventional Generative AI is limited by its inability to access internal company data. RAG can tap into this data to provide more comprehensive and personalized results.\n",
      "\n",
      "**Comparison with Model Fine-Tuning:**\n",
      "\n",
      "Fine-tuning involves adjusting the weights and biases of a pre-trained model based on new training data. While fine-tuning can improve a model's performance, it is limited by its reliance on external data sources. RAG, on the other hand, combines internal data with generative language models to create more accurate and relevant text.\n",
      "\n",
      "Let me know if you'd like me to clarify any of these points or provide further information!\n",
      "Final Answer:\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question - relevant question\n",
    "result = rag_chain_with_source.invoke(user_query)\n",
    "relevance_score = result['answer']['relevance_score']\n",
    "final_answer = result['answer']['final_answer']\n",
    "\n",
    "print(f\"Relevance Score: {relevance_score}\")\n",
    "print(f\"Final Answer:\\n{final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
