{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New OS parameter to avoid warnings.  \n",
    "# This will not have a material impact on your code, but prevents warnings from appearing related to new LangChain features.\n",
    "import os\n",
    "os.environ['USER_AGENT'] = 'RAGUserAgent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import bs4\n",
    "import chromadb\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain import hub\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "from modules import utils\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = utils.load_env_file(\"./../secrets/env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"vks.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "## Web loading and crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF and extract text\n",
    "pdf_reader = PdfReader(pdf_path)\n",
    "text = \"\"\n",
    "for page in pdf_reader.pages:\n",
    "    text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ThuyVT2\n",
      "2VKS\n",
      "VKS \u0000VNGCloud Kubernetes Service) is a managed service on VNGCloud that helps\n",
      "you simplify the deployment and management of container-based applications.\n",
      "Kubernetes is an open-source platform developed by Google, widely used for\n",
      "managing and deploying containerized applications in distributed environments.\n",
      "VKS Demo Video - Gi ả i Pháp Qu ả n Lý Kubernetes Toàn Di ệ n C ủ a VN VKS Demo Video - Gi ả i Pháp Qu ả n Lý Kubernetes Toàn Di ệ n C ủ a VN ……\n",
      "3What is VKS?\n",
      "VKS \u0000VNGCloud Kubernetes Service) is a managed service on VNGCloud that\n",
      "simplifies the deployment and management of container-based applications.\n",
      "Kubernetes, an open-source platform developed by Google, is widely used to\n",
      "manage and deploy containerized applications in distributed environments.\n",
      "•Fully Managed control plane: VKS will free you from the burden of managing\n",
      "the Kubernetes Control Plane, allowing you to focus on developing applications.\n",
      "Highlights of VKS\n",
      "4•Support for the latest Kubernetes versions: VKS is always updating to the\n",
      "latest Kubernetes versions (minor versions from 1.27, 1.28, 1.29) to ensure you\n",
      "can take advantage of the most advanced features.\n",
      "•Kubernetes Networking: VKS integrates Calico CNI, providing high efficiency\n",
      "and security.\n",
      "•Upgrade seamlessly: VKS supports easy and fast upgrades between\n",
      "Kubernetes versions, helping you stay updated with the latest improvements.\n",
      "•Scaling & Healing Automatically: VKS automatically scales the Node group\n",
      "when needed and repairs issues when nodes encounter problems, saving you\n",
      "time and effort in management.\n",
      "•Reduce costs and enhance reliability: VKS deploys the Kubernetes Control\n",
      "Plane in a highly available mode and completely for free, helping you save costs\n",
      "and improve system reliability.\n",
      "•Integration of Native Blockstore \u0000Container Storage Interface - CSI\u0000\u0000 VKS\n",
      "allows you to manage Blockstore through Kubernetes YAML, providing\n",
      "persistent storage for containers and supporting important features such as\n",
      "resizing, changing IOPS, and snapshotting volumes.\n",
      "•Integration of Load Balancer \u0000Network Load Balancer, Application Load\n",
      "Balancer) through built-in drivers such as VNGCloud Controller Manager,\n",
      "VNGCloud Ingress Controller: VKS provides the capability to manage NLB/ALB\n",
      "through Kubernetes YAML, making it easy to expose Services in Kubernetes to\n",
      "the outside.\n",
      "•Enhanced security: VKS allows you to create Private Node Groups with only\n",
      "Private IPs and control access to the cluster via the IP Whitelist feature,\n",
      "ensuring the safety of your system.\n",
      "In addition, VKS has the following advantages:\n",
      "•Easy to use: VKS provides a simple and user-friendly interface.\n",
      "•Affordable pricing: VKS offers competitive pricing for its services.\n",
      "5How VKS works?\n",
      "Below are the current concepts being provided to you by VKS\u0000\n",
      "When you create a Public Cluster with Public Node Group , the VKS system will:\n",
      "•Create a VM with Floating IP (ie Public IP\u0000. Now these VMs \u0000Nodes\u0000 can directly\n",
      "join the K8S cluster through this Public IP. By using Public Cluster and Public\n",
      "Node Group, you can easily create Kubernetes clusters and expose services\n",
      "without using Load Balancer. This will contribute to cost savings for your\n",
      "cluster.\n",
      "When you create a Public Cluster with a Private Node Group , the VKS system will:\n",
      "•Create VM without Floating IP (ie without Public IP\u0000. At this time, these VMs\n",
      "\u0000Nodes\u0000 cannot join the K8S cluster directly. In order for these VMs to join the\n",
      "K8S cluster, you need to use a NAT Gateway ( NATGW ). NATGW acts as a\n",
      "relay station, allowing VMs to connect to the K8S cluster without needing a\n",
      "Public IP. With VNG Cloud, we recommend you use Pfsense or Palo Alto as a\n",
      "NATGW for your Cluster. Pfsense will help you manage incoming and outgoing\n",
      "network traffic (inbound and outbound traffic) effectively, ensuring network\n",
      "security and access management. Besides, using Private Node Group will help\n",
      "you control applications in the cluster more securely, specifically you can limit\n",
      "control plane access rights through the Whitelist IP feature.\n",
      "1. Public Cluster\n",
      "6When you create a Public Cluster with Public/Private Node Group , the VKS\n",
      "system will:\n",
      "•To enhance the security of your cluster, we have introduced the private cluster\n",
      "model. The Private Cluster feature helps make your K8S cluster as secure as\n",
      "possible, all connections are completely private from the connection between\n",
      "nodes to the control plane, the connection from the client to the control plane,\n",
      "or the connection from nodes to products. Other services in VNG Cloud such\n",
      "as: vStorage, vCR, vMonitor, VNGCloud APIs,...Private Cluster is the ideal\n",
      "choice for services that require strict access control, ensuring compliance\n",
      "with security regulations and data privacy.\n",
      "Below is a comparison table between creating and using Public Cluster and Private\n",
      "Cluster on the VKS system:\n",
      "Criteria Public Cluster Private Cluster\n",
      "Connect Use Public IP addresses to\n",
      "communicate between\n",
      "nodes and control plane,\n",
      "between clients and control\n",
      "plane, between nodes andUse Private IP addresses to\n",
      "communicate between\n",
      "nodes and control plane,\n",
      "between clients and control\n",
      "plane, between nodes and2. Private Cluster\n",
      "3. Comparison between using Public Cluster\n",
      "and Private Cluster\n",
      "7other services in VNG\n",
      "Cloud.other services in VNG\n",
      "Cloud.\n",
      "Security Medium security since\n",
      "connections use Public IP.Higher security with all\n",
      "connections private and\n",
      "limited access.\n",
      "Access managementMore difficult to control,\n",
      "access can be managed\n",
      "through the Whitelist\n",
      "featureStrict access control, all\n",
      "connections are within VNG\n",
      "Cloud's private network,\n",
      "thereby minimizing the risk\n",
      "of external network\n",
      "attacks.\n",
      "Scalability\n",
      "\u0000AutoScaling)Easily scalable through \n",
      "Auto Scaling feature .Easily scalable through \n",
      "Auto Scaling feature .\n",
      "AutoHealing Automatically detect errors\n",
      "and restart the node ( Auto\n",
      "Healing )Automatically detect errors\n",
      "and restart the node ( Auto\n",
      "Healing )\n",
      "Accessibility from\n",
      "outsideEasy access from\n",
      "anywhere with internet.Access from outside must\n",
      "be through other security\n",
      "solutions.\n",
      "Configuration and\n",
      "deploymentSimpler because it does not\n",
      "require setting up an\n",
      "internal network.More complex, requires\n",
      "private and secure network\n",
      "configuration.\n",
      "Cost Usually lower because\n",
      "there is no need to set up a\n",
      "complex security\n",
      "infrastructure.Higher cost due to\n",
      "additional security and\n",
      "management components\n",
      "required. Specifically,\n",
      "when using a private\n",
      "cluster, you need to pay\n",
      "for 4 automatically created\n",
      "private service endpoints\n",
      "to connect to services on\n",
      "VNG Cloud.\n",
      "Flexibility High, easy to change and\n",
      "access services.More flexible in\n",
      "applications that require\n",
      "security, but less flexible\n",
      "for applications that require\n",
      "external access.\n",
      "8Therefore:\n",
      "•Public Cluster : Suitable for applications that do not require high security and\n",
      "need flexibility and access from multiple locations. Easy to deploy and manage\n",
      "but has higher security risks.\n",
      "•Private Cluster : Suitable for applications that require high security, strictly\n",
      "complying with security and privacy regulations. Provides stable and secure\n",
      "connectivity, but requires more complex configuration and management, as well\n",
      "as higher costs.\n",
      "9Announcements and Updates\n",
      "10Release notes\n",
      "VKS \u0000VNGCloud Kubernetes Service) has just released the latest update, bringing\n",
      "many new features to users. Here are the highlights of the update:\n",
      "New features:\n",
      "•Force-Upgrade, Auto-Upgrade : Automatically upgrade the Kubernetes version\n",
      "for the cluster/node group on schedule or when the current version is about to\n",
      "expire. For more details, please refer here .\n",
      "VKS \u0000VNGCloud Kubernetes Service) has just released the latest update, bringing\n",
      "many improvements to users. Here are the highlights of the update:\n",
      "Improve:\n",
      "•Support POC/ Stop POC for Cluster : Users can now perform POC/ Stop POC\n",
      "for resources on VKS such as Server, Volume, Load Balancer, Endpoint. This\n",
      "feature brings high flexibility to users who want to experience VKS. For more\n",
      "details, please refer here .\n",
      "•Upgrade VNGCloud BlockStorage CSI Driver Plugin: Bugs discovered in\n",
      "previous versions have been fixed, making the system run smoother and more\n",
      "reliably.\n",
      "•Freely choose/edit configuration with/without using VNGCloud Controller\n",
      "Manager plugin, VNGCloud Ingress Controller plugin on existing VKS cluster:\n",
      "The ability to customize plugin configuration allows users to optimize the VKS\n",
      "cluster according to their specific needs. This helps increase flexibility and\n",
      "meet the special requirements of each application.Dec 5, 2024\n",
      "Oct 23, 2024\n",
      "11•Additionally, in this update, we have also fixed some minor bugs to provide a\n",
      "better user experience.\n",
      "VKS \u0000VNGCloud Kubernetes Service) has just released its latest update, bringing\n",
      "many features and improvements to users. Here are the highlights of the update:\n",
      "New Region:\n",
      "•In addition to Region HCM03, VKS now supports Region HAN01. This addition\n",
      "gives customers more options in deploying applications, especially useful for\n",
      "businesses with data location requirements.\n",
      "New features:\n",
      "•Network Type: Cilium Overlay, Cilium VPC Native Routing: In addition to\n",
      "Calico Overlay, this release we have added two new network types: Cilium\n",
      "Overlay and Cilium VPC Native Routing. Cilium Overlay allows you to build\n",
      "flexible overlay networks, while Cilium VPC Native Routing integrates tightly\n",
      "with VNG Cloud's VPC, optimizing performance and security for your\n",
      "applications. For more details, please refer here .\n",
      "Improve:\n",
      "•Multiple Subnets for Clusters on VKS\u0000 VKS now supports using multiple\n",
      "subnets for a cluster. This allows you to configure each node group in the\n",
      "cluster to be located on different subnets within the same VPC, optimizing\n",
      "resource allocation and network management.\n",
      "•Edit Labels/Taints on an existing VKS cluster: With the ability to directly edit\n",
      "Labels/Taints on a deployed VKS cluster, you can control Pod scheduling, apply\n",
      "different policies to Node Groups, and customize node selection rules for\n",
      "applications. This helps manage and classify resources more efficiently.\n",
      "•Enable/Disable Private Service Endpoint usage option: Previously, when\n",
      "creating a private cluster on VKS, creating a private service endpoint was\n",
      "required. Now, you can easily enable/disable this feature, allowing services inOct 03, 2024\n",
      "12the VKS cluster to communicate via internal IP addresses, enhancing security\n",
      "and minimizing the risk of external attacks.\n",
      "•Enable/Disable Volume Encryption Option: Volume encryption feature allows\n",
      "you to protect sensitive data stored in the Persistent Volumes of the VKS\n",
      "cluster. This ensures data security and compliance with information protection\n",
      "regulations. Now, you can enable/disable encryption for each Volume as\n",
      "needed.\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the existing\n",
      "VKS, bringing many new features to users. Here are the details about the update:\n",
      "New features:\n",
      "•Private Cluster: Previously, public clusters on VKS were using Public IP\n",
      "addresses to communicate between nodes and the control plane. To improve\n",
      "the security of your cluster, we have launched the private cluster model. The\n",
      "Private Cluster feature helps your K8S cluster to be as secure as possible, all\n",
      "connections are completely private from the connection between the nodes to\n",
      "the control plane, the connection from the client to the control plane, or the\n",
      "connection from the nodes to other products and services in VNG Cloud such\n",
      "as: vStorage, vCR, vMonitor, VNGCloud APIs,... Private Cluster is the ideal\n",
      "choice for services that require strict access control, ensuring compliance\n",
      "with regulations on security and data privacy. For details on the two operating\n",
      "models of Cluster, you can refer to here and refer to the steps to create a\n",
      "private Cluster here .\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update for VKS, bringing\n",
      "numerous new improvements for users. Here are the details of the update:Aug 28, 2024\n",
      "Aug 26, 2024\n",
      "13Improve:\n",
      "•Kubernetes Version: VKS has added new images to optimize the size, features,\n",
      "and network compared to the old images. The creation of these images also\n",
      "aims to serve both Public and Private clusters that VKS is about to launch.\n",
      "Specifically, in this release, we have added the following images:\n",
      "◦Ubuntu-22.kube_v1.27.12-vks.1724605200\n",
      "◦Ubuntu-22.kube_v1.28.8-vks.1724605200\n",
      "◦Ubuntu-22.kube_v1.29.1-vks.1724605200\n",
      "Chú ý:\n",
      "•Để khởi tạ o m ộ t Private Cluster, bạn cần ch ọ n s ử  d ụ ng m ộ t trong 3\n",
      "image mới này. Đ ố i v ớ i Public Cluster, b ạ n có th ể  ch ọ n s ử  d ụ ng b ấ t k ỳ\n",
      "image cũ ho ặ c m ớ i tùy theo nhu c ầ u c ủ a b ạ n.\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the existing\n",
      "VKS, bringing many new improvements to users. Here are the details about the\n",
      "update:\n",
      "Improve:\n",
      "•Event History: VKS has added events for Auto Scaling and Auto Healing. Now,\n",
      "with Event History, you can track every change occurring within your Cluster,\n",
      "from automatic scaling to automatic healing. These events enhance your ability\n",
      "to monitor and manage your Kubernetes cluster.\n",
      "Aug 13, 2024\n",
      "Aug 01, 2024\n",
      "14VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the already\n",
      "available VKS, bringing many new features and improvements to users. Here are\n",
      "the details about the update:\n",
      "New feature:\n",
      "•VKS resource monitoring: Users can directly monitor the operating status of\n",
      "Cluster, Node, CPU usage, RAM, Memory,... status of Node through intuitive\n",
      "dashboards. To display data on the dashboard, users need to install it \n",
      "vmonitor-metric-agenton the cluster where they want to perform monitoring.\n",
      "For more details, refer here .\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the existing\n",
      "VKS, bringing many new improvements to users. Here are the details about the\n",
      "update:\n",
      "Improve:\n",
      "•Upgrade VKS management through Terraform: Users can simultaneously\n",
      "adjust the number of nodes and change the number of nodes for autoscale\n",
      "\u0000Minimum/ Maximum node Autoscale) right during the configuration editing\n",
      "process. With the ability to adjust multiple parameters at the same time,\n",
      "managing a Kubernetes cluster becomes more flexible and convenient. For\n",
      "more details, see examples here .\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the existing\n",
      "VKS, bringing many new improvements to users. Here are the details about the\n",
      "update:July 25, 2024\n",
      "July 23, 2024\n",
      "15Improve:\n",
      "•Upgrade VNGCloud Controller Manager Plugin, VNGCloud Ingress Controller\n",
      "Plugin: Errors discovered in previous versions have been fixed, helping the\n",
      "system operate smoother and more reliably.\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the existing\n",
      "VKS, bringing many new improvements to users. Here are the details about the\n",
      "update:\n",
      "Improve:\n",
      "•Upgrade VNGCloud BlockStorage CSI Driver Plugin: Errors discovered in\n",
      "previous versions have been fixed, helping the system operate smoother and\n",
      "more reliably.\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the already\n",
      "available VKS, bringing many improvements to users. Here are the details about the\n",
      "update:\n",
      "Improve:\n",
      "•Private Node Group : The MTU for Nodes belonging to the Private Node Group\n",
      "has been updated to 1450. This improves network performance for applications\n",
      "running in the Private Node Group.\n",
      "•Number of nodes and AutoScale : You can now edit both of these properties in\n",
      "the same API. This simplifies your Cluster management.July 18, 2024\n",
      "July 17, 2024\n",
      "16VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the already\n",
      "available VKS, bringing many new features and improvements to users. Here are\n",
      "the details about the update:\n",
      "New feature:\n",
      "•Stop POC support for Cluster : Users can now easily perform Stop POC for all\n",
      "resources being POC on a Cluster, instead of having to perform Stop POC\n",
      "individually for each resource. This helps save time and effort when moving\n",
      "Cluster from test resources to real resources. For more details, refer here .\n",
      "Improve:\n",
      "•Node Group status : Added \" Degraded \" status so users can monitor the\n",
      "operating status of the Node Group more accurately. This status will display\n",
      "when the number of active nodes is less than the actual number of replicas.\n",
      "•Timeout for Cluster and Node Group : Added timeout for Cluster and Node\n",
      "Group creation, this improvement ensures VKS operates smoothly and\n",
      "efficiently, while providing clear and timely information to users. use. Timeout\n",
      "for Cluster creation is 1 hour and for Node Group is 3 hours . If after this time\n",
      "your Cluster or Node Group has not been successfully created, we will update\n",
      "their status to ERROR. At this point, you can delete and create another Cluster\n",
      "or Node Group instead.\n",
      "•KubeConfig Access: Access to the KubeConfig file is now only allowed when\n",
      "the Cluster is Active. This improvement helps users avoid configuration errors\n",
      "when using Terraform to automate Kubernetes deployments.\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the existing\n",
      "VKS, bringing many new improvements to users. Here are the details about the\n",
      "update:July 02, 2024\n",
      "June 27, 2024\n",
      "17Improve:\n",
      "•Upgrade VNGCloud Controller Manager Plugin, VNGCloud Ingress Controller\n",
      "Plugin: Errors discovered in previous versions have been fixed, helping the\n",
      "system operate smoother and more reliably.\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the existing\n",
      "VKS, bringing many new improvements to users. Here are the details about the\n",
      "update:\n",
      "Improve:\n",
      "•Upgrade PVC size setting feature \u0000Persistent Volume Claim Size): Users can\n",
      "now specify the minimum size for CSI drives to be 1GB instead of the minimum\n",
      "size of 20GB as before. For more details, you can refer to Volume and Integrate\n",
      "with Container Storage Interface .\n",
      "•Change the default Storage Class used for Cluster: change the default from\n",
      "SSD type drives - IOPS 200 to the default SSD type drives - IOPS 3000.\n",
      "•Upgrade VNGCloud Controller Manager Plugin, VNGCloud Ingress Controller\n",
      "Plugin: plugin improvements help avoid duplicate Load Balancer naming.\n",
      "Attention:\n",
      "Because the old default Storage Class has been removed from the system, if you\n",
      "want to continue using and resize this storage class, you can:\n",
      "•Create a Storage Class named sc-iops-200-retain with the Volume Type you\n",
      "desire.\n",
      "•Resize Storage Class via command:\n",
      "Copy\n",
      "kubectl patch pvc sc-iops-200-retain -p '{\"spec\":{\"resources\":{\"requests\"June 19, 2024\n",
      "18For more details, refer to Integrate with Container Storage Interface .\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update to the already\n",
      "available VKS, bringing many new features and improvements to users. Here are\n",
      "the details about the update:\n",
      "New feature:\n",
      "•Support users working with VKS through Terraform: Users can easily create\n",
      "Clusters and Node Groups in VKS using Terraform. For more details, refer here .\n",
      "Improve:\n",
      "•Upgrade VNGCloud Controller Manager Plugin: Add Annotation to configure\n",
      "Load Balancer to support Proxy Protocol. For more details, refer here .\n",
      "We are extremely pleased to announce that the official release ( General\n",
      "Availability ) of VNGCloud Kubernetes Service is available. With this official\n",
      "release, in addition to the features we have provided on previous releases, this\n",
      "version will bring many new features and improvements to users. Here are the\n",
      "details about the update:\n",
      "New feature:\n",
      "•Re-activate: VKS allows you to request the system to automatically re-initialize\n",
      "the default IAM Service Account when you mistakenly delete or change\n",
      "previously created default IAM Service Account information. The default IAM\n",
      "Service Account is the IAM Service Account that is automatically created by theJune 12, 2024\n",
      "May 30, 2024\n",
      "19VKS system when you start working with VKS. We will use this IAM Service\n",
      "Account to initialize resources for your Cluster.\n",
      "•Event History : VKS will display the history of events that occur when users\n",
      "work with the Cluster or each Node Group. This will be a way to help you\n",
      "monitor activities occurring with your Cluster, thereby limiting unusual activities\n",
      "from occurring.\n",
      "•Volume : VKS has integrated the display of the Volume list at the Resource Tab,\n",
      "helping you easily manage the Volumes that are attached to your Cluster.\n",
      "•Load Balancer : VKS has integrated the display of the Load Balancer list in the\n",
      "Resource Tab, helping you easily manage the Load Balancers being used for\n",
      "your Cluster.\n",
      "Improve:\n",
      "•Performance : VKS has optimized performance when initializing the Cluster.\n",
      "Specifically, in the Alpha version, the time from the start of Cluster initialization\n",
      "(with Default Node Group) to the time the Cluster switches to ACTIVE state is\n",
      "about 04\u000000s to 04\u000030s. Currently, this time has been optimized by us to \n",
      "02\u000030s to 03\u000000s depending on each Cluster and each time you initialize.\n",
      "•Garbage collection of unused containers and images : VKS will automatically\n",
      "delete unused images when the disk reaches the usage limit (usage/quota ratio\n",
      ">= 85%\u0000.\n",
      "•In addition , this GA version has improved a few other issues such as:\n",
      "◦Changing the Node Name helps you easily use and manage your Cluster.\n",
      "Specifically, the Node Name will have additional information: Cluster Name ,\n",
      "Node Group Name .\n",
      "◦Delete User Builder on User's Worker Node.\n",
      "◦Change SSH mechanism from Port 22 to Port 234.\n",
      "If you encounter any problems with this official release, please contact VKS support\n",
      "for assistance.\n",
      "May 03, 2024\n",
      "20The latest update for VKS is available, bringing many new features and\n",
      "improvements to users. Here are the details about the update:\n",
      "New feature:\n",
      "•Supports Whitelist feature: VKS allows creating a Private Node Group with only\n",
      "Private IP and also allows any IP to connect to the Cluster through the Whitelist\n",
      "IP feature. For more details, please refer to Whitelist .\n",
      "Improve:\n",
      "•System optimization: Helps the system operate more smoothly and efficiently.\n",
      "•Bug Fixes: Fixed some minor bugs to provide a better user experience.\n",
      "If you encounter any problems after updating, please contact VKS support for\n",
      "assistance.\n",
      "We're excited to introduce a new update to the VKS service, giving you a more\n",
      "powerful and efficient Kubernetes management experience than ever before!\n",
      "Highlights:\n",
      "•Fully Managed control plane: VKS will free you from the burden of managing\n",
      "Kubernetes' Control Plane, helping you focus on application development.\n",
      "•Supports the latest versions of Kubernetes: VKS always updates the latest\n",
      "versions of Kubernetes (minor versions from 1.27, 1.28, 1.29) to ensure you\n",
      "always take advantage of the most advanced features.\n",
      "•Kubernetes Networking: VKS integrates Calico CNI, providing high efficiency\n",
      "and security.\n",
      "•Upgrade seamlessly: VKS supports upgrading between Kubernetes versions\n",
      "easily and quickly, helping you stay up to date with the latest improvements.April 17, 2024\n",
      "21•Scaling & Healing Automatically: VKS automatically expands the Node group\n",
      "when necessary and automatically fixes errors when the node has problems,\n",
      "helping you save time and effort on management.\n",
      "•Reduce costs and improve reliability: VKS deploys Control Plane of Kubernetes\n",
      "in high availability mode and is completely free, helping you save costs and\n",
      "improve system reliability.\n",
      "•Blockstore Native \u0000Container Storage Interface - CSI) integration: VKS allows\n",
      "you to manage Blockstore through Kubernetes' YAML, providing persistent\n",
      "storage for containers and supporting important features such as resizing, IOPS\n",
      "scaling and snapshot volumes.\n",
      "•Integrate Load Balancer \u0000Network Load Balancer, Application Load Balancer)\n",
      "through built-in drivers such as VNGCloud Controller Mananger, VNGCloud\n",
      "Ingress Controller: VKS provides the ability to manage NLB/ALB through YAML\n",
      "of Kubernetes, making it easy for you expose Service in Kubernetes to the\n",
      "outside.\n",
      "•Enhance security: VKS allows you to create a Private Node Group with only\n",
      "Private IP and control access to the cluster through the IP Whitelist feature,\n",
      "ensuring the safety of your system.\n",
      "With these breakthrough features, VKS promises to bring you a completely new\n",
      "Kubernetes management experience, helping you optimize efficiency and save\n",
      "costs!\n",
      "Please contact us for further advice and support!\n",
      "22Getting Started with VKS\n",
      "23Instructions for installing and\n",
      "configuring the kubectl in Kubenetes\n",
      "You need to use a kubectl version that is no more than one version different from\n",
      "the cluster version. For example, a client v1.2 should work with master v1.1, v1.2 and\n",
      "v1.3. Using the latest version of kubectl helps avoid unforeseen problems.\n",
      "Step 1\u0000 Download the latest version with the command:\n",
      "To download a specific version, replace the section in the command with a specific\n",
      "version.$(curl -s https://storage.\n",
      "googleapis.com/kubernetes-release/release/stable.txt)\n",
      "For example, to download version v1.17.0 on Linux, run the command:\n",
      "Step 2\u0000 Create kubectl binary executable via command:\n",
      "Step 3\u0000 Put the binary into your PATH environment variable via the command:curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl \n",
      "curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.\n",
      "chmod +x ./kubectlNecessary conditions\n",
      "Install kubectl on Linux\n",
      "Install kubectl binary with curl on Linux\n",
      "24Step 4\u0000 Check to make sure that the version you installed is the latest via command:\n",
      "•With CentOS, RHEL or Fedora operating systems, you can run the command:\n",
      "If you are using Ubuntu or another Linux distro that supports the snap package\n",
      "manager , kubectl is already available in snap .\n",
      "Step 1\u0000 Switch to user snap and execute the installation command:\n",
      "Step 2\u0000 Check the version you just installed is the latest:sudo mv ./kubectl /usr/local/bin/kubectl\n",
      "kubectl version\n",
      "sudo apt-get update && sudo apt-get install -y apt-transport-httpscurl -s \n",
      "https://apt.kubernetes.io/\n",
      " kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes\n",
      "sudo snap install kubectl --classic\n",
      "kubectl versionInstall kubectl using the package manager\n",
      "Install kubectl with snap\n",
      "Install kubectl on macOS\n",
      "Install kubectl binary with curl on macOS\n",
      "25Step 1\u0000 Download the latest version:\n",
      "To download a specific version, replace the section in the command with the\n",
      "specific version. For example, to download v1.17.0 on macOS, run the command:\n",
      "$(curl -s \n",
      "https://storage.googleapis.com/kubernetes-release/release/stable.txt)\n",
      "Step 2\u0000 Create kubectl binary executable via command:\n",
      "Step 3\u0000 Put the binary into your PATH environment variable via the command:\n",
      "Step 4\u0000 Check to make sure that the version you installed is the latest via command:\n",
      "If you are on macOS and using the Homebrew package manager , you can install\n",
      "kubectl with Homebrew.\n",
      "Step 1\u0000 Run the installation command:kubectl versioncurl -LO \"\n",
      "https://storage.googleapis.com/kubernetes-release/release/$(curl\n",
      " -s \n",
      "https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin\n",
      "\"\n",
      "curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.\n",
      "chmod +x ./kubectl\n",
      "sudo mv ./kubectl /usr/local/bin/kubectl\n",
      "kubectl version\n",
      "brew install kubernetes-cliInstall with Homebrew on macOS\n",
      "26or command:\n",
      "Step 2\u0000 Check to make sure the version you installed is the latest:\n",
      "If you are on macOS and using the Macports package manager , you can install\n",
      "kubectl with Macports.\n",
      "Step 1\u0000 Run the installation command:\n",
      "Step 2\u0000 Check to make sure the version you installed is the latest:\n",
      "Step 1\u0000 Download the latest version v1.17.0 from this link . Or if you have already\n",
      "installed it curl, use the following command:brew install kubectl\n",
      "kubectl version\n",
      "sudo port selfupdatesudo port install kubectl\n",
      "sudo port selfupdatesudo port install kubectl\n",
      "curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.Install with Macports on macOS\n",
      "Install kubectl on Windows\n",
      "Install kubectl binary with curl on Windows\n",
      "27To find out the latest stable version, see \n",
      "https://storage.googleapis.com/kubernetes-release/release/stable.txt .\n",
      "Step 2\u0000 Include the binary in your PATH environment variable.\n",
      "Step 3\u0000 Check to make sure the version kubectlis the same as the downloaded\n",
      "version:\n",
      "Note: Docker Desktop for Windowskubectl adds its own version to the PATH.\n",
      "If you have previously installed Docker Desktop, you may need to set your PATH\n",
      "before the Docker Desktop installation adds a PATH to or removes kubectl\n",
      "Docker Desktop's.\n",
      "If you are on Windows and using the Powershell Gallery package manager , you\n",
      "can install and update kubectl with Powershell.\n",
      "Step 1\u0000 Execute the following installation commands (make sure you define your\n",
      "own DownloadLocation):\n",
      "Note: If you do not define it DownloadLocation, kubectlit will be installed in\n",
      "the user's temp directory.\n",
      "The installation will generate $HOME/.kubeand instruct you to create a\n",
      "configuration file\n",
      "Step 1\u0000 Check to make sure the version you installed is the latest:kubectl version\n",
      "Install-Script -Name install-kubectl -Scope CurrentUser -Forceinstall-kub\n",
      "kubectl versionInstall with Powershell from PSGallery\n",
      "28Note: Installation updates will be performed when re-running the commands\n",
      "from step 1.\n",
      "Step 1\u0000 To install kubectl on Windows you can use the Chocolatey package\n",
      "manager or the Scoop command installer .\n",
      "•If you use Choco\n",
      "•If you use Scoop\n",
      "Step 2\u0000 Check to make sure the version you installed is the latest:\n",
      "Step 3\u0000 Move to your home directory:\n",
      "Step 4\u0000 Create folder .kube:\n",
      "Step 5\u0000 Move to the folder .kubeyou just created:\n",
      "Step 6\u0000 Configure kubectl to use a remote Kubernetes cluster:choco install kubernetes-cli\n",
      "scoop install kubectl\n",
      "kubectl version\n",
      "cd %USERPROFILE%\n",
      "mkdir .kube\n",
      "cd .kubeInstall on Windows using Chocolatey or Scoop\n",
      "29Note: Edit the configuration file with a text editor, such as Notepad.\n",
      "You can install kubectl from part of the Google Cloud SDK.\n",
      "Step 1\u0000 Install Google Cloud SDK .\n",
      "Step 2\u0000 Execute the installation command kubectl:\n",
      "Step 3\u0000 Check to make sure the version you installed is the latest:\n",
      "1.For kubectl to search and access your Kubernetes cluster, it needs a\n",
      "kubeconfig file, which is automatically created when you create a new cluster\n",
      "using kube-up.sh or successfully deploy a Minikube cluster. By default,\n",
      "kubectl's configuration is defined at ~/.kube/config.\n",
      "2.Check kubectl is configured correctly by viewing the cluster status: kubectl\n",
      "cluster-infoNew-Item config -type file\n",
      "gcloud components install kubectl\n",
      "kubectl version\n",
      "kubectl cluster-infoDownload from part of the Google Cloud\n",
      "SDK\n",
      "Verify kubectl configuration\n",
      "301.If you see a response URL, kubectl is properly configured to access your\n",
      "cluster.\n",
      "2.If you see a message similar to the one below, kuberctl is not configured\n",
      "correctly or cannot connect to the Kubernetes cluster. \n",
      "The connection to the server <server-name:port> was refused - did you \n",
      "specify the right host or port?\n",
      "For example, if you are planning to run a Kubernetes cluster on your laptop\n",
      "(locally), you will need a tool like Minikube installed previously and run the\n",
      "commands above again. If kubectl cluster-info returns the url but you cannot\n",
      "access your cluster, then check if it is configured correctly, by:\n",
      "•kubectl provides autocompletion support for Bash and Zsh, helping you reduce\n",
      "the need to type many commands.\n",
      "•Below are the steps to set up autocompletion for Bash (including the differences\n",
      "between Linux and macOS) and Zsh.\n",
      "•Kubelet completion script for Bash is generated with the command . After the\n",
      "script is created, you need to source (execute) the script to enable the\n",
      "autocompletion feature.kubectl completion bash\n",
      "•However, the completion script depends on bash-completion , so you must\n",
      "install bash-completion beforehand (check bash-completion exists with the\n",
      "command type _init_completion).kubectl cluster-info dump\n",
      "kubectl configuration options\n",
      "Enable shell autocompletion\n",
      "Bash on Linux\n",
      "Introduce\n",
      "311.bash-completion is provided by many package managers (see here ) . You can\n",
      "install with command apt-get install bash-completionor \n",
      "yum install bash-completion.\n",
      "2.The above commands generate \n",
      "/usr/share/bash-completion/bash_completion, which is the main script of\n",
      "bash-completion. Depending on your package manager, you may have to\n",
      "source this file in a ~/.bashrc.\n",
      "3.To find this file, reload your current shell and run the type _init_completion.\n",
      "If successful, you are done setting up, otherwise add the following to \n",
      "~/.bashrcyour file:\n",
      "1.Reload your shell and confirm that bash-completion is installed correctly with\n",
      "the command type _init_completion.\n",
      "Now you need to ensure that the kubectl completion script is sourced across all\n",
      "shell sessions. There are 2 ways to do this:\n",
      "•Source script in file ~/.bashrc:\n",
      "•Add script to folder /etc/bash_completion.d:\n",
      "•If you have an alias for kubectl, you can add another shell completion to that\n",
      "alias:\n",
      "Copysource /usr/share/bash-completion/bash_completion\n",
      "echo 'source <(kubectl completion bash)' >>~/.bashrc\n",
      "kubectl completion bash >/etc/bash_completion.d/kubectl\n",
      "echo 'alias k=kubectl' >>~/.bashrcecho 'complete -F __start_kubectl k'Install bash-completion\n",
      "Enable kubectl autocompletion\n",
      "32Note: bash-completion sources all completion scripts in \n",
      "/etc/bash_completion.d.\n",
      "The above methods are equally effective. After reloading the shell, kubectl\n",
      "autocompletion will work.\n",
      "Kubectl completion script on Bash is generated by kubectl completion bash. This\n",
      "source script will enable the kubectl completion feature.\n",
      "However, the kubectl completion script depends on the bash-completion you\n",
      "installed earlier.\n",
      "Warning: There are two versions of bash-completion, v1 and v2. V1 is for Bash\n",
      "3.2 (default Bash on macOS\u0000, and v2 is for Bash 4.1\u0000. Kubectl completion script \n",
      "does not work properly with bash-completion v1 and Bash 3.2. It is compatible\n",
      "with bash-completion v2 and Bash 4.1\u0000 . Therefore, to use kubectl completion\n",
      "correctly on macOS, you must install Bash 4.1\u0000 ( instructions ). The instructions\n",
      "that follow assume that you are using Bash 4.1\u0000 (that is, any Bash version 4.1 or\n",
      "later).\n",
      "Note: As mentioned, these instructions assume you are using Bash 4.1\u0000, which\n",
      "means that you will be installing bash-completion v2 (as opposed to Bash 3.2\n",
      "and bash-completion v1, in which case, kubectl completion will not work).\n",
      "1.You can check if bash-completion v2 was previously installed with the\n",
      "command type _init_completion. If not, you can install it with Homebrew:Bash on macOS\n",
      "Introduce\n",
      "Install bash-completion\n",
      "331.From the output of this command, add the following to ~/.bashrcyour file:\n",
      "export BASH_COMPLETION_COMPAT_DIR\u0000\"/usr/local/etc/bash_completion.d\"\n",
      "Reload your shell and verify that bash-completion v2 is installed correctly using the \n",
      "type _init_completion.\n",
      "Now you must ensure that the kubectl completion script has been sourced in all\n",
      "your shell sessions. There are many ways to achieve this:\n",
      "•Source completion script in file ~/.bashrc:\n",
      "•Add completion script to the folder /usr/local/etc/bash_completion.d:\n",
      "•If you have an alias for kubectl, you can extend the completion shell to work\n",
      "with that alias:\n",
      "Copy\n",
      "•If you have installed kubectl with Homebrew (as introduced above ) then the\n",
      "kubectl completion script will be included in the \n",
      "/usr/local/etc/bash_completion.d/kubectl. In this case, you don't need to\n",
      "do anything.\n",
      "Note: Installing the Homebrew way already sources all the files in the \n",
      "BASH_COMPLETION_COMPAT_DIR, which is why the following two methods work.brew install bash-completion@2\n",
      "export BASH_COMPLETION_COMPAT_DIR=\"/usr/local/etc/bash_completion.d\"[[ -r \n",
      "echo 'source <(kubectl completion bash)' >>~/.bashrc\n",
      "kubectl completion bash >/usr/local/etc/bash_completion.d/kubectl\n",
      "echo 'alias k=kubectl' >>~/.bashrcecho 'complete -F __start_kubectl k'Enable kubectl autocompletion\n",
      "34In any case, after reloading your shell, kubectl completion should work.\n",
      "1.Kubectl completion script for Zsh is generated with the command \n",
      "kubectl completion zsh. Source completion script in your shell will enable\n",
      "kubectl autocompletion. To make it work for all shells, add the following line to\n",
      "the file ~/.zshrc:\n",
      "1.If you have an alias for kubectl, you can extend the completion shell to work\n",
      "with that alias:\n",
      "After reloading the shell, kubectl autocompletion should work.\n",
      "1.If you get the error complete:13: command not found: compdef, add the\n",
      "following line at the beginning of the file ~/.zshrc:\n",
      "For more details, see kubernetes.iosource <(kubectl completion zsh)\n",
      "echo 'alias k=kubectl' >>~/.zshrcecho 'complete -F __start_kubectl k' >>~\n",
      "autoload -Uz compinitcompinitZsh\n",
      "35Create a Public Cluster\n",
      "When you create a Public Cluster with Public Node Group , the VKS system will:\n",
      "•Create a VM with Floating IP (ie Public IP\u0000. Now these VMs \u0000Nodes\u0000 can directly\n",
      "join the K8S cluster through this Public IP. By using Public Cluster and Public\n",
      "Node Group, you can easily create Kubernetes clusters and expose services\n",
      "without using Load Balancer. This will contribute to cost savings for your\n",
      "cluster.\n",
      "When you create a Public Cluster with a Private Node Group , the VKS system will:\n",
      "•Create VM without Floating IP (ie without Public IP\u0000. At this time, these VMs\n",
      "\u0000Nodes\u0000 cannot join the K8S cluster directly. In order for these VMs to join the\n",
      "K8S cluster, you need to use a NAT Gateway ( NATGW ). NATGW acts as a\n",
      "relay station, allowing VMs to connect to the K8S cluster without needing a\n",
      "Public IP. With VNG Cloud, we recommend you use Pfsense or Palo Alto as a\n",
      "NATGW for your Cluster. Pfsense will help you manage incoming and outgoing\n",
      "network traffic (inbound and outbound traffic) effectively, ensuring network\n",
      "security and access management. Besides, using Private Node Group will help\n",
      "you control applications in the cluster more securely, specifically you can limit\n",
      "control plane access rights through the Whitelist IP feature.\n",
      "Model\n",
      "36Create a Public Cluster\n",
      "with Public Node Group\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have a VPC\n",
      "or Subnet yet, please create a VPC or Subnet according to the instructions here \n",
      ".\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please create an SSH key according to the instructions here .\n",
      "•Installed and configured kubectl on your device. Please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use a kubectl\n",
      "version that is too old, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "connected together to run containerized applications. Cluster provides a unified\n",
      "environment to deploy, manage, and operate containers at scale.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully create your VKS account. After Activate\n",
      "successfully, select Create a ClusterPrerequisites\n",
      "Initialize Cluster\n",
      "37Step 4\u0000 At the Cluster initialization screen, we have set up information for the\n",
      "Cluster and a Default Node Group for you. You can keep these default values   or\n",
      "adjust the desired parameters for the Cluster and Node Group at Cluster\n",
      "Configuration, Default Node Group Configuration, Plugin. By default we will create\n",
      "a Public Cluster for you with Public Node Group.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the Cluster's status is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "After the Cluster is successfully initialized, you can connect and check the newly\n",
      "created Cluster information by following these steps:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the icon and select Download Config\n",
      "File to download the kubeconfig file. This file will give you full access to your\n",
      "Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config directory\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to test node\n",
      "•If the results are returned as below, it means your Cluster was successfully\n",
      "initialized with 3 nodes as below.kubectl get nodesConnect and check the newly created\n",
      "Cluster information\n",
      "38The following is a guide for you to deploy the nginx service on Kubernetes.\n",
      "•Create nginx-service.yaml file with the following content:NAME                                            STATUS     ROLES    AGE   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7   Ready      <none>   50m   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652   Ready      <none>   23m   \n",
      "ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0   Ready      <none>   28m   \n",
      "Deploy a Workload\n",
      "Step 1 : Create Deployment and Service for Nginx app\n",
      "39•Deploy This deployment equals:\n",
      "•Run the following command to test DeploymentapiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80\n",
      "kubectl apply -f nginx-service.yaml\n",
      "Step 2: Check Deployment and Service information\n",
      "before exposing it to the Internet.\n",
      "40•If the results are returned as below, it means you have successfully deployed\n",
      "the nginx service.\n",
      "•Run the following command to expose nginx-service to the internet:\n",
      "•If the results are returned as below, it means you have successfully exposed the\n",
      "Service to the Internet.kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S) \n",
      "service/kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP \n",
      "service/nginx-service   ClusterIP   10.96.178.229   <none>        80/TCP  \n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAI\n",
      "deployment.apps/nginx-app   1/1     1            1           74s   nginx  \n",
      "NAME                             READY   STATUS    RESTARTS   AGE   IP    \n",
      "pod/nginx-app-7f45b65946-5pcvz   1/1     Running   0          74s   172.1\n",
      "kubectl expose deployment nginx-app --type=NodePort --port=30080 --target\n",
      "NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S) \n",
      "service/kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP \n",
      "service/nginx-app       NodePort    10.96.215.192   <none>        30080:3\n",
      "service/nginx-service   ClusterIP   10.96.178.229   <none>        80/TCP  \n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTA\n",
      "deployment.apps/nginx-app   1/1     1            1           2m43s   ngin\n",
      "NAME                             READY   STATUS    RESTARTS   AGE     IP  \n",
      "pod/nginx-app-7f45b65946-5pcvz   1/1     Running   0          2m43s   172Step 3: Expose Nginx Service to the Internet\n",
      "Step 4: To access the just exported Nginx app, you can\n",
      "use the URL with the format:\n",
      "41Where node_ip can be the node_port address of any node in the cluster. You can\n",
      "get Node's External IP information at the vServer interface. Specifically, access at \n",
      "https://hcm-3.console.vngcloud.vn/vserver/v-server/cloud-server .\n",
      "For example, below I have successfully accessed the nginx app with the address: \n",
      "http://61.28.231.65\u000031007/\n",
      "If you want to expose this service through vLB Layer4, vLB Layer7, please refer to:\n",
      "•Expose a service through vLB Layer4\n",
      "•Expose a service through vLB Layer7http://<node_ip>:31289/\n",
      "42Create a Public Cluster\n",
      "with Private Node Group\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have a VPC\n",
      "or Subnet yet, please create a VPC or Subnet according to the instructions here \n",
      ".\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please create an SSH key according to the instructions here .\n",
      "•Installed and configured kubectl on your device. Please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use a kubectl\n",
      "version that is too old, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "Attention:\n",
      "•To ensure that VMs in NodeGroups on the subnet can go outbound to\n",
      "the internet and connect to the Control Plane, you must set up a NAT\n",
      "Gateway. For more details, please refer to the section below.\n",
      "Prerequisites\n",
      "Create Palo Alto or Pfsense as an alternative\n",
      "to NAT Gateway\n",
      "43Attention:\n",
      "•For the best support when using Palo Alto or Pfsense, please contact\n",
      "our team of experts via Hotline 1900 1549 or email \n",
      "support@vngcloud.vn\n",
      "Or you can choose to use Palo Alto or Pfsense to work with Private Node Group\n",
      "according to instructions at:\n",
      "•Palo Alto as a NAT Gateway\n",
      "•Pfsense as a NAT Gateway\n",
      "After Palo Alto, Pfsense is successfully initialized, you need to create a Route table\n",
      "to connect to different networks. Specifically, follow these steps to create a Route\n",
      "table:\n",
      "Step 1\u0000 Visit https://hcm-3.console.vngcloud.vn/vserver/network/route-table\n",
      "Step 2\u0000 In the navigation menu bar, select Network Tab/ Route table.\n",
      "Step 3\u0000 Select Create Route table.\n",
      "Step 4\u0000 Enter a descriptive name for the Route table. Route table names can include\n",
      "letters (az, AZ, 0\u00009, '_', '-'). The input data length is between 5 and 50. It must not\n",
      "include leading or trailing spaces.\n",
      "Step 5\u0000 Select VPC for your Route table. If you do not have a VPC, you need to\n",
      "create a new VPC according to the instructions on the VPC Page . The VPC used to\n",
      "set up the Route table must be the VPC selected for Palo Alto or Pfsense and your\n",
      "Cluster.\n",
      "Step 6 : Select Create to create a new Route table.\n",
      "Initialize Route Table\n",
      "44Step 7\u0000 Select the newly created Route table then select Edit Routes.\n",
      "Step 8\u0000 In the add new Route section , enter the following information:\n",
      "•For Destination, enter Destination CIDR as 0.0.0.0/0\n",
      "•For Target, enter Target CIDR as the corresponding Palo Alto or Pfsense\n",
      "Network Interface IP address.\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "connected together to run containerized applications. Cluster provides a unified\n",
      "environment to deploy, manage, and operate containers at scale.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully create your VKS account. After Activate\n",
      "successfully, select Create a Cluster\n",
      "Step 4\u0000 At the Cluster initialization screen, we have set up information for the\n",
      "Cluster and a Default Node Group for you. You can keep these default values   or\n",
      "adjust the desired parameters for the Cluster and Node Group at Cluster\n",
      "Configuration, Default Node Group Configuration, Plugin. By default we will create\n",
      "a Public Cluster for you with Public Node Group. You need to change your\n",
      "selection to Private Node Group .\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the Cluster's status is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .Initialize Cluster\n",
      "45After the Cluster is successfully initialized, you can connect and check the newly\n",
      "created Cluster information by following these steps:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the iconand select Download config file\n",
      "to download the kubeconfig file. This file will give you full access to your Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config directory\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to test node\n",
      "•If the results are returned as below, it means your Cluster was successfully\n",
      "initialized with 3 nodes as below.\n",
      "The following is a guide for you to deploy the nginx service on Kubernetes.kubectl get nodes\n",
      "NAME                                            STATUS     ROLES    AGE   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7   Ready      <none>   50m   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652   Ready      <none>   23m   \n",
      "ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0   Ready      <none>   28m   Connect and check the newly created\n",
      "Cluster information\n",
      "Deploy a Workload\n",
      "46•Create nginx-service-lb4.yaml file with the following content:\n",
      "◦Deploy This deployment equals:apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx\n",
      "  type: LoadBalancer \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80\n",
      "kubectl apply -f nginx-service-lb4.yamlStep 1 : Create Deployment for Nginx app.\n",
      "47•Run the following command to test Deployment\n",
      "•If the results are returned as below, it means you have successfully deployed\n",
      "the nginx service.\n",
      "You can get Load Balancer Public Endpoint information at the vLB interface.\n",
      "Specifically, access at https://hcm-3.console.vngcloud.vn/vserver/load-\n",
      "balancer/vlb/\n",
      "For example, below I have successfully accessed the nginx app with the address: \n",
      "http://180.93.181.20/kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT\n",
      "service/kubernetes      ClusterIP      10.96.0.1       <none>        443/T\n",
      "service/nginx-app       NodePort       10.96.215.192   <none>        3008\n",
      "service/nginx-service   LoadBalancer   10.96.179.221   <pending>     80:3\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTA\n",
      "deployment.apps/nginx-app   1/1     1            1           2m16s   ngin\n",
      "NAME                             READY   STATUS    RESTARTS   AGE     IP  \n",
      "pod/nginx-app-7f45b65946-t7d7k   1/1     Running   0          2m16s   172\n",
      "http://Endpoint/Step 2: Check Deployment and Service information\n",
      "before exposing it to the Internet.\n",
      "Step 3: To access the just exported nginx app, you can\n",
      "use the URL with the format:\n",
      "48\n",
      "49Palo Alto as a NAT Gateway\n",
      "Use the instructions below to work with a Private Node group through Palo Alto.\n",
      "To be able to use Palo Alto as NAT Gateway for Cluster on VKS system, you need:\n",
      "•A Windows server \u0000VM\u0000 has been initialized on the vServer system with the\n",
      "following configuration:\n",
      "•A Palo Alto server \u0000VM\u0000 is initialized on the vMarketPlace system according to\n",
      "the instructions below with the following configuration:Item Configuration\n",
      "Flavor 2\u00004\n",
      "Volume 20GB\n",
      "VPC 10.76.0.0/16\n",
      "Subnet 10.76.0.4/24\n",
      "Network Interface 1 10.76.0.3\n",
      "Item Configuration\n",
      "Flavor 2\u00008\n",
      "Volume 60GB\n",
      "VPC 10.76.0.0/16\n",
      "Network Interface 1 10.76.255.4\n",
      "Network Interface 2 10.76.0.4Prerequisites\n",
      "Initialize Palo Alto\n",
      "50Step 1\u0000 Visit https://marketplace.console.vngcloud.vn/\n",
      "Step 2\u0000 At the main screen, search for Palo Alto , at Palo Alto services , select \n",
      "Launch .\n",
      "Step 3\u0000 Now, you need to configure Palo Alto. Specifically, you can select the\n",
      "desired Volume, IOPS, Network, Security Group . You need to choose the same\n",
      "VPC and Subnet as the VPC and Subnet you choose to use for your Cluster. In\n",
      "addition, you also need to select an existing Server Group or select Dedicated\n",
      "SOFT ANTI AFFINITY group so we can automatically create a new server group.\n",
      "Step 4\u0000 Proceed to pay like normal resources on VNG Cloud.\n",
      "Step 1\u0000 After initializing Palo Alto from vMarketPlace according to the instructions\n",
      "above, you can access the vServer interface here to check if the server running\n",
      "Palo Alto has been initialized. Next, open the Any rule on the Security Group for\n",
      "the Palo Alto server you just created. Opening the Any rule on the Security Group\n",
      "will allow all traffic to the Palo Alto server.\n",
      "Step 2\u0000 After the server running Palo Alto is successfully initialized . To access\n",
      "the Palo Alto GUI you need a vServer running Windows. Then you access it using IP\n",
      "Internal Interface with the default login name and password: admin/admin\n",
      "Note: Go to the Network section of vServer Windows to access the Palo Alto GUI.\n",
      "You need to create the same VPC and use a different subnet than the subnet with\n",
      "priority 1 when initializing Palo Alto\n",
      "Configure parameters for Palo Alto\n",
      "51Step 3 : After logging in, you need to change your password for the first time.\n",
      "Please enter a new password according to your wishes.\n",
      "Step 4\u0000 You need to create 1 Zone Inside and 1 Zone Outside according to the\n",
      "instructions below:\n",
      "•Select Add pen\n",
      "•Name the Zone : Inside then select OK\n",
      "52•Do the same for Zone Outside\n",
      "Step 5 : Configure External Interface\n",
      "•Interface Type: Layer 3\n",
      "•Virtual Router: default\n",
      "•Security Zone: Outside\n",
      "53•Switch to IPv4 Tab and select Add to enter Static IP for External Interface\n",
      "•To get this IP information, go to Palo Alto 's Network Interface section to view\n",
      "the information\n",
      "54•Switch to the Advanced tab , in the MTU section you need to set it to 1400\n",
      "Step 6\u0000 Perform similar configuration for Internal Interfaces\n",
      "55•At the IPv4 tab: you proceed to set up Static IP\n",
      "•Switch to the Advanced tab , in the MTU section , set it to 1400\n",
      "56Step 7\u0000 Create static route\n",
      "•Go to Network \u0000 Virtual Routers \u0000 Select default \u0000 Switch to Static Routes\n",
      "•Create a route as shown below\n",
      "57Step 8\u0000 Create Security Policy Rule\n",
      "•Go to Policies \u0000 Security \u0000 Add\n",
      "•On the General tab , you need to name the rule\n",
      "•At the Source tab , set information such as Source Zone , Source Address , \n",
      "Source User, Source Device\n",
      "58•At the Destination tab , set information such as Destination Zone, Destination\n",
      "Address, Destination Device\n",
      "•At the Application tab , set information such as Application, Depend On\n",
      "59•At the Service/URL Category tab , set information such as Service, URL\n",
      "Category\n",
      "•At the Actions tab , set information such as Action, Log, Profile, Other Settings\n",
      "Step 9 : Create a NAT rule so that VMs can go out to the Internet\n",
      "•Go to Policies \u0000 NAT \u0000 Add\n",
      "•On the General tab , name the NAT rule\n",
      "60•At the Original Packet tab, select Source Zone, Destination Zone, Destination\n",
      "Interface, Service, Source Address, Destination Address\n",
      "•Create the Translated Packet tab and perform configuration as shown below\n",
      "Note: Need to change the IP Address to the Static IP address that you configured\n",
      "in step 6\n",
      "61Step 10 : Proceed to Commit\n",
      "After Palo Alto is successfully initialized and configured, you need to create a Route\n",
      "table to connect to different networks. Specifically, follow these steps to create a\n",
      "Route table:\n",
      "Step 1\u0000 Visit https://hcm-3.console.vngcloud.vn/vserver/network/route-table\n",
      "Step 2\u0000 In the navigation menu bar, select Network Tab/ Route table.\n",
      "Initialize Route Table\n",
      "62Step 3\u0000 Select Create Route table.\n",
      "Step 4\u0000 Enter a descriptive name for the Route table. Route table names can include\n",
      "letters (az, AZ, 0\u00009, '_', '-'). The input data length is between 5 and 50. It must not\n",
      "include leading or trailing spaces.\n",
      "Step 5\u0000 Select VPC for your Route table. If you do not have a VPC, you need to\n",
      "create a new VPC according to the instructions on the VPC Page . The VPC used to\n",
      "set up the Route table must be the VPC selected for your Palo Alto and Cluster.\n",
      "Step 6 : Select Create to create a new Route table.\n",
      "Step 7\u0000 Select the newly created Route table then select Edit Routes.\n",
      "Step 8\u0000 In the add new Route section , enter the following information:\n",
      "•For Destination, enter Destination CIDR as 0.0.0.0/0\n",
      "•For Target, enter Target CIDR as the Palo Alto Network Interface 2 IP address.\n",
      "For example:\n",
      "•Proceed to ping 8.8.8.8 or google.com\n",
      "Checking connection\n",
      "63\n",
      "64Pfsense as a NAT Gateway\n",
      "Use the instructions below to work with Private Node groups through Pfsense\n",
      "To be able to use Pfsense as NAT Gateway for Cluster on VKS system, you need:\n",
      "•A Pfsense server \u0000VM\u0000 is initialized on the vMarketPlace system according to\n",
      "the instructions below with the following configuration:\n",
      "Step 1\u0000 Visit https://marketplace.console.vngcloud.vn/\n",
      "Step 2\u0000 At the main screen, search for Pfsense , at Pfsense service , select Launch\n",
      ".\n",
      "Step 3\u0000 Now, you need to configure Pfsense. Specifically, you can select the\n",
      "desired Volume, IOPS, Network, Security Group . You need to choose the same\n",
      "VPC and Subnet as the VPC and Subnet you choose to use for your Cluster. In\n",
      "addition, you also need to select an existing Server Group or select Dedicated\n",
      "SOFT ANTI AFFINITY group so we can automatically create a new server group.\n",
      "Step 4\u0000 Proceed to pay like normal resources on VNG Cloud.Item Configuration\n",
      "Flavor 2\u00004\n",
      "Volume 80 GB\n",
      "VPC 10.3.0.0/16\n",
      "Network Interface 1 10.3.0.3Prerequisites\n",
      "Initialize Pfsense\n",
      "65Step 1\u0000 After initializing Pfsense from vMarketPlace according to the instructions\n",
      "above, you can access the vServer interface here to check whether the server\n",
      "running Pfsense has been initialized. Next, open the Any rule on the Security\n",
      "Group for the Pfsense server you just created. Opening the Any rule on the\n",
      "Security Group will allow all traffic to the Pfsense server.\n",
      "Step 2\u0000 After the server running Pfsense is successfully initialized . To access the\n",
      "Pfsense GUI, you need to use the IP address of the External Interface to log in with\n",
      "the default Username and password admin/pfsense.\n",
      "•To get this IP information, go to the Network Interface section of Pfsense to\n",
      "view the information\n",
      "Configure parameters for Pfsense\n",
      "66Step 3 : Open the rule on the firewall\n",
      "•Proceed to Add rule\n",
      "•You can open the rule as below to access the GUI using External Interface .\n",
      "Attention:\n",
      "•You should limit the IP Range allowed to connect to the Pfsense GUI to limit\n",
      "users allowed to access the Pfsense GUI.\n",
      "67•Select Save\n",
      "•Then select Apply changes\n",
      "Step 4 : Proceed with General Setup , please do as below\n",
      "68\n",
      "69•Configure WAN Interface\n",
      "70•Change password in GUI\n",
      "•Proceed to reload\n",
      "•General Setup completed\n",
      "71Step 5\u0000 Configure LAN Interface\n",
      "•Go to Interfaces \u0000 Assignments to add a LAN Interface\n",
      "•Click Add\n",
      "72•Then click Save\n",
      "•Go to Interfaces \u0000 Assignments to enable LAN Interface\n",
      "73•You make the configuration as below\n",
      "•Configure IP for LAN\n",
      "•Then proceed to Add a new gateway: enter Gateway for LAN Interface\n",
      "74•To get this IP information, go to the Network Interface section of the Pfsense\n",
      "server to view the information:\n",
      "•Proceed to Save again\n",
      "Step 6 : Review configuration information\n",
      "75Step 7 \u0000 Open the Internet outbound rule for the LAN interface\n",
      "•At source, select the IP range that is allowed to go out to the Internet\n",
      "76Step 8\u0000 Configure NAT so that vServers can go out to the Internet\n",
      "•Go to Firewall \u0000 NAT\n",
      "•Select NAT mode then proceed to configure NAT\n",
      "77•Click Add to add the rule\n",
      "•Select source , destination NAT\n",
      "78After Pfsense is successfully initialized and configured, you need to create a Route\n",
      "table to connect to different networks. Specifically, follow these steps to create a\n",
      "Route table:\n",
      "Step 1\u0000 Visit https://hcm-3.console.vngcloud.vn/vserver/network/route-table\n",
      "Step 2\u0000 In the navigation menu bar, select Network Tab/ Route table.\n",
      "Step 3\u0000 Select Create Route table.\n",
      "Step 4\u0000 Enter a descriptive name for the Route table. Route table names can include\n",
      "letters (az, AZ, 0\u00009, '_', '-'). The input data length is between 5 and 50. It must not\n",
      "include leading or trailing spaces.\n",
      "Step 5\u0000 Select VPC for your Route table. If you do not have a VPC, you need to\n",
      "create a new VPC according to the instructions on the VPC Page . The VPC used to\n",
      "set up the Route table must be the VPC selected for your Pfsense and Cluster.\n",
      "Step 6 : Select Create to create a new Route table.\n",
      "Step 7\u0000 Select the newly created Route table then select Edit Routes.\n",
      "Step 8\u0000 In the add new Route section , enter the following information:\n",
      "•For Destination, enter Destination CIDR as 0.0.0.0/0\n",
      "•For Target, enter Target CIDR as the Pfsense Network Interface 2 IP address.\n",
      "For example:\n",
      "Initialize Route Table\n",
      "79Proceed to ping google.com or 8.8.8.8 to check\n",
      "•Before Enable NAT the server could not access the internet\n",
      "•After configuring NAT, ping 8.8.8.8 to check\n",
      "Checking connection\n",
      "80Create a Private Cluster\n",
      "Previously, public clusters on VKS were using Public IP addresses to communicate\n",
      "between nodes and the control plane. To improve the security of your cluster, we\n",
      "have launched the private cluster model. The Private Cluster feature helps your K8S\n",
      "cluster to be as secure as possible, all connections are completely private from the\n",
      "connection between nodes to the control plane, the connection from the client to\n",
      "the control plane, or the connection from nodes to other products and services in\n",
      "VNG Cloud such as: vStorage, vCR, vMonitor, VNGCloud APIs,... Private Cluster is\n",
      "the ideal choice for services that require strict access control, ensuring compliance\n",
      "with regulations on security and data privacy.\n",
      "In which:\n",
      "•Control plane : Managed by VNG Cloud, responsible for coordinating and\n",
      "managing the entire cluster.\n",
      "•Nodes : When created, Nodes in the Cluster will only have internal IPs and\n",
      "cannot go to the public internet. If you want the node to access the internet, you\n",
      "need to use a NAT Gateway. For more details, refer here .\n",
      "•Private Load Balancer : Managed by VNG Cloud, responsible for helping Private\n",
      "Nodes communicate with Control Plane.\n",
      "•Private Service Endpoint : When you create a private cluster, the system\n",
      "automatically creates 4 endpoints to help connect to other services on VNG\n",
      "Cloud including:\n",
      "Model\n",
      "81◦Endpoint to connect to the IAM service \u0000Endpoint Name: vks-iam-\n",
      "endpoint-...)\n",
      "◦Endpoint to connect to vCR service \u0000Endpoint Name: vks-vcr-endpoint-...)\n",
      "◦Endpoint to connect to vServer service \u0000Endpoint Name: vks-vserver-\n",
      "endpoint-...)\n",
      "◦Endpoint to connect to vStorage service \u0000Endpoint Name: vks-vstorage-\n",
      "endpoint-...)\n",
      "You can view information about the 4 private service endpoints through the vServer\n",
      "portal by following the link here .\n",
      "Warning:\n",
      "•Do not delete Private Service Endpoints: To ensure stable operation\n",
      "of the cluster, you should not delete the 4 pre-created service\n",
      "endpoints. If you accidentally delete or edit these 4 endpoints, within a\n",
      "maximum of 5 minutes, the system will automatically recreate them but\n",
      "may cause disruption to running services. At this time, because the\n",
      "recreated service endpoint may have changed the Endpoint IP\n",
      "compared to the original, in order for the cluster to work, you need to\n",
      "manually add Endpoint IP to the previously running servers via\n",
      "command:\n",
      "vks-bootstraper add-host -i <IP> -d <DOMAIN>\n",
      "82Example, if you delete private service endpoint of vCR, you must add\n",
      "host via command:\n",
      "•Reuse Private Service Endpoints: Service endpoints can be used by\n",
      "multiple private clusters. When private clusters share a VPC, we will\n",
      "reuse them for these clusters.\n",
      "•Delete Private Service Endpoints automatically: When you delete a\n",
      "cluster, if there are no more clusters that reuse these service\n",
      "endpoints, the system will automatically delete them.\n",
      "•Cost of using Private Service Endpoint: Using a private cluster will\n",
      "incur additional costs for 4 private service endpoints, but it brings\n",
      "many security benefits to your project. Please carefully consider the\n",
      "factors to decide whether to use public or private for your cluster.\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have a VPC\n",
      "or Subnet yet, please create a VPC and Subnet according to the instructions\n",
      "here .\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please create an SSH key according to the instructions here .\n",
      "•Installed and configured kubectl on your device. Please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use a kubectl\n",
      "version that is too old, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.vks-boostraper add-host -i 10.10.10.10 -d vcr.vngcloud.vn\n",
      "Prerequisites\n",
      "Initialize Cluster\n",
      "83A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "connected together to run containerized applications. Cluster provides a unified\n",
      "environment to deploy, manage, and operate containers at scale.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully create your VKS account. After Activate\n",
      "successfully, select Create a Cluster\n",
      "Step 4\u0000 At the Cluster initialization screen, we have set up information for the\n",
      "Cluster and a Default Node Group for you. You can keep these default values   or\n",
      "adjust the desired parameters for the Cluster and Node Group at Cluster\n",
      "Configuration, Default Node Group Configuration, Plugin.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the Cluster's status is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "84Warning:\n",
      "•A Cluster can have many Node Groups , each Node Group can\n",
      "operate in Public/Private mode depending on your needs.\n",
      "•Because your Cluster is initialized in Private mode , to be able to\n",
      "access Control Plane's kube-api , you need to be in the VPC you\n",
      "choose to use for your Cluster.\n",
      "After the Cluster is successfully initialized, you can connect and check the newly\n",
      "created Cluster information by following these steps:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the Download icon and select \n",
      "Download config file to download the kubeconfig file. This file will give you full\n",
      "access to your Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config folder\n",
      "Step 4\u0000 Because your Cluster is initialized in Private mode, to be able to access\n",
      "kube-api, you need to be in the VPC you have chosen to use for your Cluster. For\n",
      "example, when you are not in the VPC and execute get nodes, the results will\n",
      "display as follows:\n",
      "kubectl get nodes\n",
      "E0821 14:27:03.793829   23348 memcache.go:265] couldn't get current serve\n",
      "E0821 14:27:05.866230   23348 memcache.go:265] couldn't get current serve\n",
      "E0821 14:27:07.922272   23348 memcache.go:265] couldn't get current serve\n",
      "E0821 14:27:09.989832   23348 memcache.go:265] couldn't get current serve\n",
      "E0821 14:27:12.055864   23348 memcache.go:265] couldn't get current serve\n",
      "Unable to connect to the server: dial tcp 10.7.8.9:6443: connectex: No coConnect and check the newly created\n",
      "Cluster information\n",
      "85In the example below I will stand at a server with a VPC along with the VPC used for\n",
      "the Cluster. You can perform SSH to the server according to instructions here .\n",
      "After SSH into the server, install kubectl according to the instructions here .\n",
      "•For example, I am using a linux server to perform get nodes, I can install kubectl\n",
      "via command:\n",
      "•Then I tested kubectl via command:\n",
      "•Create folder . kube via command:\n",
      "•Then, enter the kubeconfig file via the command:\n",
      "•Then, enter :wq to save the kubeconfig file and exit vim.\n",
      "•Run the following command to test the cluster\n",
      "•You should see a return similar to the following:\n",
      "•Run the following command to test nodesudo snap install kubectl --classic\n",
      "kubectl version\n",
      "mkdir -p .kube\n",
      "vim .kube/config\n",
      "kubectl get svc\n",
      "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   10m\n",
      "kubectl get nodes\n",
      "86•If the results are returned as below, it means your Cluster was successfully\n",
      "initialized with 1 node as below.\n",
      "Because Private Cluster can only connect privately to the vContainer Registry (vCR)\n",
      "system and cannot connect to other Container Registry outside the internet, you\n",
      "need to pull/push the image to vCR to use according to the following instructions:\n",
      "Step 1\u0000 Install Docker\n",
      "•Perform docker installation according to instructions here .\n",
      "Step 2\u0000 Initialize Public Repository and Repository User on vContainer Registry\n",
      "Portal:\n",
      "•Log in to the vCR portal at the link: https://vcr.console.vngcloud.vn/list\n",
      "•Perform Repository and Repository initialization according to instructions here .\n",
      "For example in the image below, I have initialized demo_repo with demo_user\n",
      "who can pull/push images:NAME                                    STATUS   ROLES    AGE     VERSION\n",
      "vks-demo-cluster-nodegroup-demo-7c9aa   Ready    <none>   8m11s   v1.28.8\n",
      "Use Docker to Pull/Push images\n",
      "87Warning:\n",
      "•If you want to create a Private Reposity, to pull an image from the\n",
      "Private Reposity, you need to create a secret key according to the\n",
      "instructions here .\n",
      "Step 3\u0000 Pull the nginx image according to the command:\n",
      "docker pull nginx:latest\n",
      "88Step 4\u0000 Log in to vCR via command:\n",
      "•For example, the command below I use to login to the demo repo:\n",
      "Step 5\u0000 Assign tags to the nginx image\n",
      "•For example, the command below I use to assign tags to the nginx image:\n",
      "Step 6\u0000 Push the image to the repo via command:\n",
      "•For example, the command below I use to push images to demo_repo:\n",
      "The following are instructions for you to deploy the nginx service and expose this\n",
      "service via Network Load Balancer\n",
      "Step 1\u0000 Create the nginx-service-lb4.yaml file via the command:docker login vcr.vngcloud.vn -u <repository_user>\n",
      "docker login vcr.vngcloud.vn -u 53461-user_demo\n",
      "docker tag SOURCE_IMAGE[:TAG] vcr.vngcloud.vn/REPO_NAME/IMAGE[:TAG]\n",
      "docker tag nginx:latest vcr.vngcloud.vn/53461-repo_demo/nginx-demo:latest\n",
      "docker push vcr.vngcloud.vn/REPO_NAME/IMAGE[:TAG]\n",
      "docker push vcr.vngcloud.vn/53461-repo_demo/nginx-demo:latest\n",
      "vi nginx.yamlDeploy a Workload\n",
      "89Then, enter the content for this file as follows: you need to replace the image with\n",
      "the image path saved on the vCR that you pushed in the step above:\n",
      "•Enter :wq to save this file.\n",
      "•Deploy This deployment equals:apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: vcr.vngcloud.vn/53461-repo_demo/nginx-demo:latest\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx\n",
      "  type: LoadBalancer \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80\n",
      "kubectl apply -f nginx-service-lb4.yaml\n",
      "90Step 2\u0000 Check Deployment and Service information before exposing it to the\n",
      "Internet.\n",
      "•Run the following command to test Deployment\n",
      "•If the results are returned as below, it means you have successfully deployed\n",
      "the nginx service.\n",
      "At this time, the vLB system will initialize a Network Load Balancer, you can view\n",
      "this LB information through the vLB portal here .kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE           CLUSTER-IP     EXTERNAL-IP      PO\n",
      "service/kubernetes      ClusterIP      10.96.0.1      <none>           44\n",
      "service/nginx-service   LoadBalancer   10.96.81.236   116.118.88.236   80\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTA\n",
      "deployment.apps/nginx-app   1/1     1            1           3m32s   ngin\n",
      "NAME                             READY   STATUS    RESTARTS   AGE     IP  \n",
      "pod/nginx-app-56bbc8fdd8-4pz68   1/1     Running   0          3m32s   172\n",
      "91Step 3\u0000 To access the just exported nginx app, you can use the Endpoint of Load\n",
      "Balancer URL with the format:\n",
      "You can get Load Balancer Public Endpoint information at the vLB interface.\n",
      "Specifically, access at https://hcm-3.console.vngcloud.vn/vserver/load-\n",
      "balancer/vlb/\n",
      "For example, below I have successfully accessed the nginx app with the address: \n",
      "http://116.118.88.236/\n",
      "A few other notes:\n",
      "•Above is an example showing you how to expose a service through\n",
      "vLB Layer 4. You can expose a service through vLB Layer 7 according\n",
      "to the instructions here .\n",
      "•To ensure the private cluster works effectively, we have automatically\n",
      "added the Subnet you choose to use for the Cluster to the cluster's\n",
      "Whitelist. You can use the Whitelist feature to limit the Subnets in the\n",
      "VPC that have access to kube-api. Details on how to use the Whitelist\n",
      "feature please refer here .http://Endpoint/\n",
      "92Expose a service through vLB Layer4\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have a VPC\n",
      "or Subnet yet, please create a VPC or Subnet according to the instructions here \n",
      ".\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please create an SSH key according to the instructions here .\n",
      "•Installed and configured kubectl on your device. Please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use a kubectl\n",
      "version that is too old, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "connected together to run containerized applications. Cluster provides a unified\n",
      "environment to deploy, manage, and operate containers at scale.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully create your VKS account. After Activate\n",
      "successfully, select Create a Cluster\n",
      "Step 4\u0000 At the Cluster initialization screen, we have set up information for the\n",
      "Cluster and a Default Node Group for you. You can keep these default values   orPrerequisites\n",
      "Initialize Cluster\n",
      "93adjust the desired parameters for the Cluster and Node Group at Cluster\n",
      "Configuration, Default Node Group Configuration, Plugin. When you choose to\n",
      "enable option, by default we will pre-install this plugin into your Cluster.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the Cluster's status is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "After the Cluster is successfully initialized, you can connect and check the newly\n",
      "created Cluster information by following these steps:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the iconand select Download Config\n",
      "File to download the kubeconfig file. This file will give you full access to your\n",
      "Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config directory\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to test node\n",
      "•If the results are returned as below, it means your Cluster was successfully\n",
      "initialized with 3 nodes as below.kubectl get nodesConnect and check the newly created\n",
      "Cluster information\n",
      "94\n",
      "Note:\n",
      "When you initialize the Cluster according to the instructions above, if you\n",
      "have not enabled the Enable vLB Native Integration Driver option , by\n",
      "default we will not pre-install this plugin into your Cluster. You need to\n",
      "manually create Service Account and install VNGCloud Controller Manager\n",
      "according to the instructions below. If you have enabled the Enable vLB\n",
      "Native Integration Driver option , then we have pre-installed this plugin\n",
      "into your Cluster, skip the Service Account Initialization step, install\n",
      "VNGCloud Controller Manager and continue following the instructions\n",
      "from Deploy once. Workload.\n",
      "Initialize Service Account\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vLBFullAccess , vServerFullAccess . To create a service account,\n",
      "go here and follow these steps:\n",
      "◦Select \" Create a Service Account \", enter a name for the\n",
      "Service Account and click Next Step to assign permissions to the\n",
      "Service Account\n",
      "◦Find and select Policy: vLBFullAccess and Policy:\n",
      "vServerFullAccess , then click \" Create a Service Account \" to\n",
      "create Service Account, Policy: vLBFullAccess and Policy:NAME                                            STATUS     ROLES    AGE   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7   Ready      <none>   50m   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652   Ready      <none>   23m   \n",
      "ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0   Ready      <none>   28m   \n",
      "Instructions for creating Service Account and installing VNGCloud\n",
      "Controller Manager\n",
      "Create Service Account and install\n",
      "VNGCloud Controller Manager\n",
      "95\n",
      "vServerFullAccess created by VNG Cloud, you cannot delete\n",
      "these policies.\n",
      "◦After successful creation, you need to save the Client_ID and \n",
      "Secret_Key of the Service Account to perform the next step.\n",
      "•Uninstall cloud-controller-manager\n",
      "•Besides, you can delete the Service Account being used for the\n",
      "cloud-controller-manager you just removed\n",
      "Install VNGCloud Controller Manager\n",
      "•Install Helm version 3.0 or higher. Refer to \n",
      "https://helm.sh/docs/intro/install/ for instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Replace your K8S cluster's ClientID, Client Secret, and ClusterID\n",
      "information and continue running:\n",
      "•After the installation is complete, check the status of vngcloud-\n",
      "Integrate-controller pods:kubectl get daemonset -n kube-system | grep -i \"cloud-controller\n",
      "# if your output is similar to the following, you MUST delete th\n",
      "kubectl delete daemonset cloud-controller-manager -n kube-system\n",
      "kubectl get sa -n kube-system | grep -i \"cloud-controller-manage\n",
      "# if your output is similar to the above, you MUST delete this s\n",
      "kubectl delete sa cloud-controller-manager -n kube-system --forc\n",
      "helm repo add vks-helm-charts https://vngcloud.github.io/vks-hel\n",
      "helm repo update\n",
      "helm install  vngcloud-controller-manager vks-helm-charts/vngclo\n",
      "  --namespace kube-system \\\n",
      "  --set cloudConfig.global.clientID= <L ấ y ClientID c ủ a Service A\n",
      "  --set cloudConfig.global.clientSecret= <L ấ y ClientSecret c ủ a S\n",
      "  --set cluster.clusterID= <L ấ y Cluster ID c ủ a cluster mà b ạ n đ ã\n",
      "kubectl get pods -n kube-system | grep vngcloud-controller-manag\n",
      "96\n",
      "For example, in the image below you have successfully installed\n",
      "vngcloud-controller-manager:\n",
      "The following is a guide for you to deploy the nginx service on Kubernetes.\n",
      "•Create nginx-service-lb4.yaml file with the following content:NAME                                          READY   STATUS    \n",
      "vngcloud-controller-manager-8864c754c-bqhvz   1/1     Running   \n",
      "Deploy a Workload\n",
      "Step 1 : Create Deployment, Service for Nginx app.\n",
      "97•Deploy this Service using:\n",
      "•Run the following command to test DeploymentapiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx\n",
      "  type: LoadBalancer \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80\n",
      "kubectl apply -f nginx-service-lb4.yaml\n",
      "Step 2: Check the Deployment and Service information\n",
      "just deployed\n",
      "98•If the results are returned as below, it means you have deployed Deployment\n",
      "successfully.\n",
      "At this point, the vLB system will automatically create a corresponding LB for the\n",
      "deployed nginx app, for example:\n",
      "You can get Load Balancer Public Endpoint information at the vLB interface.\n",
      "Specifically, access at https://hcm-3.console.vngcloud.vn/vserver/load-\n",
      "balancer/vlb/\n",
      "For example, below I have successfully accessed the nginx app with the address: \n",
      "http://180.93.181.20/kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(\n",
      "service/kubernetes      ClusterIP      10.96.0.1      <none>        443/T\n",
      "service/nginx-service   LoadBalancer   10.96.74.154   <pending>     80:31\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAI\n",
      "deployment.apps/nginx-app   0/1     1            0           2s    nginx  \n",
      "NAME                             READY   STATUS              RESTARTS   A\n",
      "pod/nginx-app-7f45b65946-bmrcf   0/1     ContainerCreating   0          2\n",
      "http://Endpoint/Step 3: To access the just exported nginx app, you can\n",
      "use the URL with the format:\n",
      "99You can see more about ALB at Working with Network load balancing \u0000NLB\u0000 .\n",
      "Attention:\n",
      "•Changing the name or size \u0000Rename, Resize) of the Load Balancer\n",
      "resource on vServer Portal can cause incompatibility with resources\n",
      "on the Kubernetes Cluster. This can lead to resources becoming\n",
      "inactive on the Cluster, or resources being resynchronized, or resource\n",
      "information between vServer Portal and the Cluster not matching. To\n",
      "prevent this problem, use kubectlCluster resource management.\n",
      "100Expose a service through vLB Layer7\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have a VPC\n",
      "or Subnet yet, please create a VPC or Subnet according to the instructions here \n",
      ".\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please create an SSH key according to the instructions here .\n",
      "•Installed and configured kubectl on your device. Please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use a kubectl\n",
      "version that is too old, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "connected together to run containerized applications. Cluster provides a unified\n",
      "environment to deploy, manage, and operate containers at scale.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully create your VKS account. After Activate\n",
      "successfully, select Create a Cluster\n",
      "Step 4\u0000 At the Cluster initialization screen, we have set up information for the\n",
      "Cluster and a Default Node Group for you. You can keep these default values   orPrerequisites\n",
      "Initialize Cluster\n",
      "101adjust the desired parameters for the Cluster and Node Group at Cluster\n",
      "Configuration, Default Node Group Configuration, Plugin. When you choose to\n",
      "enable option, by default we will pre-install this plugin into your Cluster.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the Cluster's status is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "After the Cluster is successfully initialized, you can connect and check the newly\n",
      "created Cluster information by following these steps:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the iconand select Download Config\n",
      "File to download the kubeconfig file. This file will give you full access to your\n",
      "Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config directory\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to test node\n",
      "•If the results are returned as below, it means your Cluster was successfully\n",
      "initialized with 3 nodes as below.kubectl get nodesConnect and check the newly created\n",
      "Cluster information\n",
      "102\n",
      "Attention:\n",
      "When you initialize the Cluster according to the instructions above, if you\n",
      "have not enabled the Enable vLB Native Integration Driver option , by\n",
      "default we will not pre-install this plugin into your Cluster. You need to\n",
      "manually create Service Account and install VNGCloud Ingress Controller\n",
      "according to the instructions below. If you have enabled the Enable vLB\n",
      "Native Integration Driver option , then we have pre-installed this plugin\n",
      "into your Cluster, skip the Service Account Initialization step, install\n",
      "VNGCloud Ingress Controller and continue following the instructions from\n",
      "Deploy once. Workload.\n",
      "Initialize Service Account\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vLBFullAccess , vServerFullAccess . To create a service account,\n",
      "go here and follow these steps:\n",
      "◦Select \" Create a Service Account \", enter a name for the\n",
      "Service Account and click Next Step to assign permissions to the\n",
      "Service Account\n",
      "◦Find and select Policy: vLBFullAccess and Policy:\n",
      "vServerFullAccess , then click \" Create a Service Account \" to\n",
      "create Service Account, Policy: vLBFullAccess and Policy:NAME                                            STATUS     ROLES    AGE   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7   Ready      <none>   50m   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652   Ready      <none>   23m   \n",
      "ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0   Ready      <none>   28m   \n",
      "Create Service Account and install VNGCloud Ingress Controller\n",
      "Create Service Account and install\n",
      "VNGCloud Ingress Controller\n",
      "103\n",
      "vServerFullAccess created by VNG Cloud, you cannot delete\n",
      "these policies.\n",
      "◦After successful creation, you need to save the Client_ID and \n",
      "Secret_Key of the Service Account to perform the next step.\n",
      "Install VNGCloud Ingress Controller\n",
      "•Install Helm version 3.0 or higher. Refer to \n",
      "https://helm.sh/docs/intro/install/ for instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Replace your K8S cluster's ClientID, Client Secret, and ClusterID\n",
      "information and continue running:\n",
      "•After the installation is complete, check the status of vngcloud-\n",
      "ingress-controller pods:\n",
      "For example, in the image below you have successfully installed\n",
      "vngcloud-controller-manager:\n",
      "The following is a guide for you to deploy the nginx service on Kubernetes.helm repo add vks-helm-charts https://vngcloud.github.io/vks-hel\n",
      "helm repo update\n",
      "helm install vngcloud-ingress-controller vks-helm-charts/vngclou\n",
      "  --namespace kube-system \\\n",
      "  --set cloudConfig.global.clientID= <L ấ y ClientID c ủ a Service A\n",
      "  --set cloudConfig.global.clientSecret= <L ấ y ClientSecret c ủ a S\n",
      "  --set cluster.clusterID= <L ấ y Cluster ID c ủ a cluster mà b ạ n đ ã\n",
      "kubectl get pods -n kube-system | grep vngcloud-ingress-controll\n",
      "NAME                                      READY   STATUS    REST\n",
      "vngcloud-ingress-controller-0             1/1     Running   0   \n",
      "Deploy a Workload\n",
      "104•Create nginx-service-lb7.yaml file with the following content:\n",
      "•Deploy This deployment equals:apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx \n",
      "  type: NodePort \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80\n",
      "kubectl apply -f nginx-service-lb7.yamlStep 1 : Create Deployment for Nginx app.\n",
      "105•Run the following command to test Deployment\n",
      "•If the results are returned as below, it means you have deployed Deployment\n",
      "successfully.\n",
      "•Create nginx-ingress.yaml file with the following content:kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)  \n",
      "service/kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP  \n",
      "service/nginx-service   NodePort    10.96.25.133   <none>        80:32572\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTA\n",
      "deployment.apps/nginx-app   1/1     1            1           2m50s   ngin\n",
      "NAME                             READY   STATUS    RESTARTS   AGE     IP  \n",
      "pod/nginx-app-7f45b65946-6wlgw   1/1     Running   0          2m49s   172Step 2: Check the Deployment and Service information\n",
      "just deployed\n",
      "Create Ingress Resource\n",
      "106•Run the following command to deploy Ingress\n",
      "At this time, the vLB system will automatically create a LB corresponding to the\n",
      "Ingress resource above, for example:apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: nginx-ingress\n",
      "spec:\n",
      "  ingressClassName: \"vngcloud\"\n",
      "  defaultBackend:\n",
      "    service:\n",
      "      name: nginx-service\n",
      "      port:\n",
      "        number: 80\n",
      "  rules:\n",
      "    - http:\n",
      "        paths:\n",
      "          - path: /path1\n",
      "            pathType: Exact\n",
      "            backend:\n",
      "              service:\n",
      "                name: nginx-service\n",
      "                port:\n",
      "                  number: 80               \n",
      "kubectl apply -f nginx-ingress.yaml\n",
      "107Attention:\n",
      "•Currently Ingress only supports TLS port 443 and is the termination\n",
      "point for TLS \u0000TLS termination). TLS Secret must contain fields with\n",
      "key names tls.crt and tls.key, which are the certificate and private key\n",
      "to use for TLS. If you want to use a Certificate for a host, please\n",
      "upload the Certificate according to the instructions at \u0000Upload a\n",
      "certificate] and use them as an annotation. For example:\n",
      "108You can get Load Balancer Public Endpoint information at the vLB interface.\n",
      "Specifically, access at\n",
      "For example, below I have successfully accessed the nginx app with the address: \n",
      "http://180.93.181.129/apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: example-ingress\n",
      "  annotations:\n",
      "    # kubernetes.io/ingress.class: \"vngcloud\" # this annotation is deprec\n",
      "    vks.vngcloud.vn/load-balancer-id: \"lb-6cdea8fd-4589-410e-933f-c3bc46f\n",
      "    vks.vngcloud.vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484\n",
      "spec:\n",
      "  ingressClassName: \"vngcloud\"\n",
      "  defaultBackend:\n",
      "    service:\n",
      "      name: apache-service\n",
      "      port:\n",
      "        number: 80\n",
      "  tls:\n",
      "    - hosts:\n",
      "        - host.example.com\n",
      "  rules:\n",
      "    - host: host.example.com\n",
      "      http:\n",
      "        paths:\n",
      "          - path: /path1\n",
      "            pathType: Exact\n",
      "            backend:\n",
      "              service:\n",
      "                name: nginx-service\n",
      "                port:\n",
      "                  number: 80\n",
      "http://Endpoint/To access the nginx app, you can use the Load\n",
      "Balancer Endpoint that the system has created.\n",
      "109You can see more about ALB at Working with Application Load Balancer \u0000ALB \u0000.\n",
      "Attention:\n",
      "•Changing the name or size \u0000Rename, Resize) of the Load Balancer\n",
      "resource on vServer Portal can cause incompatibility with resources\n",
      "on the Kubernetes Cluster. This can lead to resources becoming\n",
      "inactive on the Cluster, or resources being resynchronized, or resource\n",
      "information between vServer Portal and the Cluster not matching. To\n",
      "prevent this problem, use kubectlCluster resource management.\n",
      "110Preserve Source IP when using\n",
      "NLB and Nginx Ingress Controller\n",
      "Preserve Source IP when using vLB Layer 4 and Nginx Ingress Controller in\n",
      "Kubernetes is the process of maintaining the client's original IP address when\n",
      "traffic is forwarded through the load balancer and into the Kubernetes cluster. This\n",
      "is important in some cases when you need detailed information about the client's\n",
      "connection, such as the client's original IP address and root port, to be able to\n",
      "make traffic handling or logging decisions. Exactly. Below are our specific\n",
      "instructions to help you implement this usecase.\n",
      "•You have initialized the Cluster on the VKS system according to the instructions\n",
      "here and VNGCloud Controller Manager has been installed on your cluster with\n",
      "appversion from v0.2.1 or higher. If your appversion is lower than this standard\n",
      "version, you can perform the upgrade according to the following instructions:\n",
      "◦First, you need to get the release name of vngcloud-controller-manager\n",
      "installed on your cluster:\n",
      "◦Then, please upgrade to the latest version via the command:\n",
      "•Next, you need to install nginx-ingress-controller with the command:$ helm list -A | grep vngcloud-controller-manager\n",
      "vngcloud-controller-manager-1716448250          kube-system     10    \n",
      "helm upgrade vngcloud-controller-manager-1716448250 oci://vcr.vngcloud\n",
      "  --namespace kube-system\n",
      "helm install nginx-ingress-controller oci://ghcr.io/nginxinc/charts/nginxPrerequisites\n",
      "111•Add to Nginx Ingress Controller's ConfigMap the settings to enable proxy\n",
      "protocol via command:\n",
      "•The code you need to add is as follows:\n",
      "•Next, you need to configure vLB Layer4 to allow the use of proxy protocol for\n",
      "the Load Balancer Nginx service. The input value is a list of service names in\n",
      "Load Balancer using Proxy Protocol.\n",
      "•Finally, please perform NLB testing on vLB Portal until these Load Balancers are\n",
      "ACTIVE with full listener and pool.\n",
      "•Suppose, you have a service prometheus-node-exporter with port 9100 in the\n",
      "default namespace, you can apply the following yaml to make it accessible viakubectl edit cm -n kube-system nginx-ingress-controller\n",
      "data:\n",
      "  proxy-protocol: \"True\"\n",
      "  real-ip-header: proxy_protocol\n",
      "  real-ip-recursive: \"True\"\n",
      "  set-real-ip-from: 0.0.0.0/0\n",
      "kubectl annotate service -n kube-system nginx-ingress-controller-controll\n",
      "   vks.vngcloud.vn/enable-proxy-protocol=\"http,https\"ConfigMap for Nginx Ingress Controller\n",
      "Configure vLB Layer 4\n",
      "Using\n",
      "112NLB\n",
      "•Then I use IP 103.245.252.75 to curl to host kkk.example.com as follows:\n",
      "•The recorded log result has this Client IP information as shown:apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: test-ingress\n",
      "  namespace: default\n",
      "spec:\n",
      "  ingressClassName: nginx\n",
      "  rules:\n",
      "  - host: kkk.example.com\n",
      "    http:\n",
      "      paths:\n",
      "      - backend:\n",
      "          service:\n",
      "            name: prometheus-node-exporter\n",
      "            port:\n",
      "              number: 9100\n",
      "        path: /metrics\n",
      "        pathType: Exact\n",
      "113Integrate with Container\n",
      "Storage Interface (CSI)\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have a VPC\n",
      "or Subnet yet, please create a VPC or Subnet according to the instructions here \n",
      ".\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please create an SSH key according to the instructions here .\n",
      "•Installed and configured kubectl on your device. Please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use a kubectl\n",
      "version that is too old, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "connected together to run containerized applications. Cluster provides a unified\n",
      "environment to deploy, manage, and operate containers at scale.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully create your VKS account. After Activate\n",
      "successfully, select Create a ClusterPrerequisites\n",
      "Initialize Cluster\n",
      "114Step 4\u0000 At the Cluster initialization screen, we have set up information for the\n",
      "Cluster and a Default Node Group for you. You can keep these default values   or\n",
      "adjust the desired parameters for the Cluster and Node Group at Cluster\n",
      "Configuration, Default Node Group Configuration, Plugin. When you choose to\n",
      "enable the Enable vLB Native Integration Driver option , by default we will pre-\n",
      "install this plugin into your Cluster.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the Cluster's status is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "After the Cluster is successfully initialized, you can connect and check the newly\n",
      "created Cluster information by following these steps:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select Download Config File to download the\n",
      "kubeconfig file. This file will give you full access to your Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config directory\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to test node\n",
      "•If the results are returned as below, it means your Cluster was successfully\n",
      "initialized with 3 nodes as below.kubectl get nodesConnect and check the newly created\n",
      "Cluster information\n",
      "115\n",
      "Attention:\n",
      "•When you initialize the Cluster according to the instructions above, if\n",
      "you have not enabled the Enable BlockStore Persistent Disk CSI\n",
      "Driver option , by default we will not pre-install this plugin into your\n",
      "Cluster. You need to manually create Service Account and install\n",
      "VNGCloud BlockStorage CSI Driver according to the instructions\n",
      "below. If you have enabled the Enable BlockStore Persistent Disk CSI\n",
      "Driver option , we have pre-installed this plugin into your Cluster, skip\n",
      "the Service Account Initialization step, install VNGCloud BlockStorage\n",
      "CSI Driver and continue following the instructions from now on. Deploy\n",
      "a Workload.\n",
      "•VNGCloud BlockStorage CSI Driveronly supports attaching volumes\n",
      "to a single node \u0000VM) throughout the life of that volume. If you have a\n",
      "need for ReadWriteMany, you may consider using the NFS CSI Driver,\n",
      "as it allows multiple nodes to Read and Write on the same volume at\n",
      "the same time. This is very useful for applications that need to share\n",
      "data between multiple pods or services in Kubernetes.\n",
      "Initialize Service Account\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vServerFullAccess . To create a service account, go here and follow\n",
      "these steps:NAME                                            STATUS     ROLES    AGE   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7   Ready      <none>   50m   \n",
      "ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652   Ready      <none>   23m   \n",
      "ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0   Ready      <none>   28m   \n",
      "Create Service Account and install VNGCloud BlockStorage CSI Driver\n",
      "Create Service Account and install\n",
      "VNGCloud BlockStorage CSI Driver\n",
      "116\n",
      "◦Select \" Create a Service Account \", enter a name for the\n",
      "Service Account and click Next Step to assign permissions to the\n",
      "Service Account\n",
      "◦Find and select Policy: vServerFullAccess , then click \" Create a\n",
      "Service Account \" to create a Service Account, Policy:\n",
      "vLBFullAccess and Policy: vServerFullAccess are created by VNG\n",
      "Cloud, you cannot delete these policies.\n",
      "◦After successful creation, you need to save the Client_ID and \n",
      "Secret_Key of the Service Account to perform the next step.\n",
      "Install VNGCloud BlockStorage CSI Driver\n",
      "•Install Helm version 3.0 or higher. Refer to \n",
      "https://helm.sh/docs/intro/install/ for instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Replace your K8S cluster's ClientID, Client Secret, and ClusterID\n",
      "information and continue running:\n",
      "•After the installation is complete, check the status of vngcloud-\n",
      "blockstorage-csi-driver pods:\n",
      "For example, in the image below you have successfully installed\n",
      "vngcloud-controller-manager:helm repo add vks-helm-charts https://vngcloud.github.io/vks-hel\n",
      "helm repo update\n",
      "helm install vngcloud-blockstorage-csi-driver vks-helm-charts/vn\n",
      "  --replace --namespace kube-system \\\n",
      "  --set vngcloudAccessSecret.keyId=${VNGCLOUD_CLIENT_ID} \\\n",
      "  --set vngcloudAccessSecret.accessKey=${VNGCLOUD_CLIENT_SECRET}\n",
      "  --set vngcloudAccessSecret.vksClusterId=${VNGCLOUD_VKS_CLUSTER\n",
      "kubectl get pods -n kube-system | grep vngcloud-ingress-controll\n",
      "NAME                                           READY   STATUS   \n",
      "vngcloud-csi-controller-56bd7b85f-ctpns        7/7     Running  \n",
      "vngcloud-csi-controller-56bd7b85f-npp9n        7/7     Running  \n",
      "vngcloud-csi-node-c8r2w                        3/3     Running  \n",
      "117The following is a guide for you to deploy the nginx service on Kubernetes.\n",
      "•Create nginx-service.yaml file with the following content:\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80Deploy a Workload\n",
      "Step 1 : Create Deployment for Nginx app.\n",
      "118•Deploy This deployment equals:\n",
      "•Run the following command to test Deployment\n",
      "•If the results are returned as below, it means you have deployed Deployment\n",
      "successfully.\n",
      "•Create a persistent-volume.yaml file with the following content:kubectl apply -f nginx-service.yaml\n",
      "kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT\n",
      "service/kubernetes      ClusterIP      10.96.0.1       <none>        443/T\n",
      "service/nginx-app       NodePort       10.96.215.192   <none>        3008\n",
      "service/nginx-service   LoadBalancer   10.96.179.221   <pending>     80:3\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAI\n",
      "deployment.apps/nginx-app   1/1     1            1           16s   nginx  \n",
      "NAME                             READY   STATUS    RESTARTS   AGE   IP    \n",
      "pod/nginx-app-7f45b65946-t7d7k   1/1     Running   0          16s   172.1Step 2: Check the Deployment and Service information\n",
      "just deployed\n",
      "Create Persistent Volume\n",
      "119•Run the following command to deploy IngressapiVersion: storage.k8s.io/v1\n",
      "kind: StorageClass\n",
      "metadata:\n",
      "  name: my-expansion-storage-class                    # [1] The StorageCl\n",
      "provisioner: bs.csi.vngcloud.vn                       # The VNG-CLOUD CSI \n",
      "parameters:\n",
      "  type: vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018    # The volume type U\n",
      "allowVolumeExpansion: true                            # MUST set this val\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: my-expansion-pvc                           # [2] The PVC name, CA\n",
      "spec:\n",
      "  accessModes:\n",
      "    - ReadWriteOnce\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 20Gi                                # [3] The PVC size, CA\n",
      "  storageClassName: my-expansion-storage-class     # [4] The StorageClass \n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: nginx                                      # [5] The Pod name, CA\n",
      "spec:\n",
      "  containers:\n",
      "    - image: nginx\n",
      "      imagePullPolicy: IfNotPresent\n",
      "      name: nginx\n",
      "      ports:\n",
      "        - containerPort: 80\n",
      "          protocol: TCP\n",
      "      volumeMounts:\n",
      "        - mountPath: /var/lib/www/html\n",
      "          name: my-volume-name                     # MUST be the same as \n",
      "  volumes:\n",
      "    - name: my-volume-name                         # [6] The volume name, \n",
      "      persistentVolumeClaim:\n",
      "        claimName: my-expansion-pvc                # MUST be the same as \n",
      "        readOnly: false\n",
      "120At this time, the vServer system will automatically create a Volume corresponding\n",
      "to the yaml file above, for example:\n",
      "Snapshot is a low-cost, convenient and effective data backup method and can be\n",
      "used to create images, restore data and distribute copies of data. If you are a new\n",
      "user who has never used the Snapshot service, you will need to Activate Snapshot\n",
      "Service before you can create a Snapshot for your Persistent Volume.\n",
      "To be able to create Snapshots, you need to perform Activate Snapshot Service.\n",
      "You will not be charged for activating the snapshot service. After you create\n",
      "snapshots, costs will be calculated based on the storage capacity and storage time\n",
      "of these snapshots. Follow these steps to enable the Snapshot service:\n",
      "Step 1\u0000 Visit https://hcm-3.console.vngcloud.vn/vserver/block-\n",
      "store/snapshot/overviewkubectl apply -f persistent-volume.yaml\n",
      "Create Snapshots\n",
      "Activate Snapshot Service\n",
      "121Step 2\u0000 Select Activate Snapshot Service .\n",
      "For example:\n",
      "•Install Helm version 3.0 or higher. Refer to https://helm.sh/docs/intro/install/ for\n",
      "instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Continue running:\n",
      "•After the installation is complete, check the status of vngcloud-blockstorage-\n",
      "csi-driver pods:\n",
      "helm repo add vks-helm-charts https://vngcloud.github.io/vks-helm-charts\n",
      "helm repo update\n",
      "helm install vngcloud-snapshot-controller vks-helm-charts/vngcloud-snapsh\n",
      "  --replace --namespace kube-system\n",
      "kubectl get pods -n kube-systemInstall VNGCloud Snapshot Controller\n",
      "122For example, in the image below you have successfully installed vngcloud-\n",
      "controller-manager:\n",
      "•Run the following command to deploy Ingress\n",
      "•After applying the file successfully, you can check the service and pvc list via:NAME                                           READY   STATUS             \n",
      "snapshot-controller-7fdd984f89-745tg           0/1     ContainerCreating  \n",
      "snapshot-controller-7fdd984f89-k94wq           0/1     ContainerCreating  \n",
      "apiVersion: snapshot.storage.k8s.io/v1\n",
      "kind: VolumeSnapshotClass\n",
      "metadata:\n",
      "  name: my-snapshot-storage-class  # [2] The name of the volume snapshot \n",
      "driver: bs.csi.vngcloud.vn\n",
      "deletionPolicy: Delete\n",
      "parameters:\n",
      "  force-create: \"false\"\n",
      "---\n",
      "apiVersion: snapshot.storage.k8s.io/v1\n",
      "kind: VolumeSnapshot\n",
      "metadata:\n",
      "  name: my-snapshot-pvc  # [4] The name of the snapshot, CAN be changed\n",
      "spec:\n",
      "  volumeSnapshotClassName: my-snapshot-storage-class  # MUST match with [\n",
      "  source:\n",
      "    persistentVolumeClaimName: my-expansion-pvc  # MUST match with [3]\n",
      "kubectl apply -f snapshot.yaml\n",
      "kubectl get sc,pvc,pod -owideCreate a snapshot.yaml file with the following content:\n",
      "Check the newly created PVC and Snapshot\n",
      "123To change the IOPS parameters of the newly created Persistent Volume, follow\n",
      "these steps:\n",
      "Step 1\u0000 Run the command below to list the PVCs in your Cluster\n",
      "Step 2\u0000 Edit the PVC YAML file according to the command\n",
      "•If you have not edited the IOPS of the Persistent Volume before, when you run\n",
      "the above command, add an annotation bs.csi.vngcloud.vn/volume-type:\n",
      "\"volume-type-id\" . For example, below I am changing the Persistent Volume\n",
      "IOPS from 200 \u0000Volume type id = vtype-61c3fc5b-f4e9\u000045b4\u00008957\u0000\n",
      "8aa7b6029018) to 1000 \u0000Volume type id = vtype-85b39362-a360\u00004bbb-9afa-\n",
      "a36a40cea748 )NAME                                                       PROVISIONER    \n",
      "storageclass.storage.k8s.io/my-expansion-storage-class     bs.csi.vngclou\n",
      "storageclass.storage.k8s.io/sc-iops-200-retain (default)   bs.csi.vngclou\n",
      "NAME                                     STATUS   VOLUME                  \n",
      "persistentvolumeclaim/my-expansion-pvc   Bound    pvc-14456f4a-ee9e-435d-\n",
      "NAME                             READY   STATUS    RESTARTS   AGE   IP    \n",
      "pod/nginx                        1/1     Running   0          10m   172.1\n",
      "pod/nginx-app-7f45b65946-t7d7k   1/1     Running   0          94m   172.1\n",
      "kubectl get persistentvolumes\n",
      "kubectl edit pvc my-expansion-pvcChange the IOPS parameters of the newly created\n",
      "Persistent Volume\n",
      "124•If you have edited the IOPS of the Persistent Volume before, when you run the\n",
      "above command, your yaml file will already have the annotation\n",
      "bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Now, edit this annotation to\n",
      "the Volume type id with the IOPS you desire.apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  annotations:\n",
      "    bs.csi.vngcloud.vn/volume-type: \"vtype-85b39362-a360-4bbb-9afa-a36a40\n",
      "    kubectl.kubernetes.io/last-applied-configuration: |\n",
      "      {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"anno\n",
      "    pv.kubernetes.io/bind-completed: \"yes\"\n",
      "    pv.kubernetes.io/bound-by-controller: \"yes\"\n",
      "    volume.beta.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn\n",
      "    volume.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn\n",
      "  creationTimestamp: \"2024-04-21T14:16:53Z\"\n",
      "  finalizers:\n",
      "  - kubernetes.io/pvc-protection\n",
      "  name: my-expansion-pvc\n",
      "  namespace: default\n",
      "  resourceVersion: \"11041591\"\n",
      "  uid: 14456f4a-ee9e-435d-a94f-5a2e820954e9\n",
      "spec:\n",
      "  accessModes:\n",
      "  - ReadWriteOnce\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 20Gi\n",
      "  storageClassName: my-expansion-storage-class\n",
      "  volumeMode: Filesystem\n",
      "  volumeName: pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9\n",
      "status:\n",
      "  accessModes:\n",
      "  - ReadWriteOnce\n",
      "  capacity:\n",
      "    storage: 20Gi\n",
      "  phase: Bound\n",
      "Change the Disk Volume of the newly created Persistent\n",
      "Volume\n",
      "125To change the Disk Volume of the newly created Persistent Volume, run the\n",
      "following command:\n",
      "For example, initially the PVC created was 20 Gi in size, now I will increase it to 30\n",
      "Gi\n",
      "Attention:\n",
      "•You can only increase Disk Volume but cannot reduce Disk Volume\n",
      "size.\n",
      "To restore Persistent Volume from Snapshot, follow these steps:\n",
      "•Create file restore-volume.yaml with the following content:kubectl patch pvc my-expansion-pvc -p '{\"spec\":{\"resources\":{\"requests\":{\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: my-restore-pvc  # The name of the PVC, CAN be changed\n",
      "spec:\n",
      "  storageClassName: my-expansion-storage-class  \n",
      "  dataSource:\n",
      "    name: my-snapshot-pvc # MUST match with [4] from the section 5.2\n",
      "    kind: VolumeSnapshot\n",
      "    apiGroup: snapshot.storage.k8s.io\n",
      "  accessModes:\n",
      "    - ReadWriteOnce\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 20GiRestore Persistent Volume from Snapshot\n",
      "126Upgrading Control Plane Version\n",
      "Currently, our VKS system has supported you to upgrade Control Plane Version,\n",
      "you can:\n",
      "•Upgrade to a newer Minor Version (e.g. 1.24 to 1.25\u0000\n",
      "•Upgrade to a newer Patch Version (for example: 1.24.2\u0000VKS.100 to 1.24.5\u0000\n",
      "VKS.200\u0000\n",
      "To upgrade the Control Plane version, you can follow these instructions:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu.\n",
      "Step 3\u0000 Select the icon and select Upgrade control plane version to upgrade the\n",
      "control plane version.\n",
      "Step 4\u0000 You can select a new version for the control plane. The new version needs\n",
      "to be valid and compatible with the current version of the cluster. Specifically: you\n",
      "can choose:\n",
      "•Upgrade to a newer Minor Version (e.g. 1.24 to 1.25\u0000\n",
      "•Upgrade to a newer Patch Version (for example: 1.24.2\u0000VKS.100 to 1.24.5\u0000\n",
      "VKS.200\u0000\n",
      "Step 4\u0000 The VKS system will upgrade the Control Plane components of the Cluster\n",
      "to the new version. After the upgrade is complete, the Cluster status returns to \n",
      "ACTIVE .\n",
      "Attention:\n",
      "•Upgrading Control Plane Version is optional and independent of\n",
      "upgrading Node Group Version. However, Control Plane Version and\n",
      "Node Group Version in the same Cluster cannot differ by more than 1\n",
      "minor version. Besides, the VKS system automatically upgrades\n",
      "127Control Plane Version when the current K8S Version used for your\n",
      "Cluster exceeds the supplier's support period.\n",
      "•During the Control Plane Version upgrade, you cannot perform other\n",
      "actions on your Cluster.\n",
      "•Below are a few notes before, during and after the upgrade process,\n",
      "please refer to:\n",
      "Before getting into work:\n",
      "•Back up data: You should back up cluster data before upgrading to\n",
      "ensure safety in case the upgrade fails.\n",
      "•Check the current version: Visit Releases for a list of supported\n",
      "versions. Select a new version that is valid and compatible with the\n",
      "current version of the cluster.\n",
      "•Ensure cluster availability: Cluster must be in ACTIVE status and all\n",
      "nodes must be HEALTHY.\n",
      "•Stop running tasks: Stop running tasks on the cluster to avoid affecting\n",
      "the upgrade process.\n",
      "While doing:\n",
      "•Monitor cluster status: Monitor cluster status during the upgrade\n",
      "process. The cluster status will change to UPDATING and after\n",
      "completion will return to ACTIVE.\n",
      "•Check system logs: Check system logs to detect any errors or\n",
      "warnings during the upgrade process.\n",
      "After implementation:\n",
      "•Check cluster availability: Confirm that the cluster has been upgraded\n",
      "successfully and all nodes are operating normally.\n",
      "•Test applications: Test applications running on the cluster to ensure\n",
      "they work properly after the upgrade.\n",
      "Note:\n",
      "•Upgrading Control Plane Version may take some time depending on\n",
      "the size and complexity of the cluster.\n",
      "128•In some rare cases, the Control Plane Version upgrade may fail. If this\n",
      "happens, the VKS system will automatically rollback the cluster to the\n",
      "current version.\n",
      "129Upgrading Node Group Version\n",
      "Currently, our VKS system has supported you to upgrade Node Group Version, you\n",
      "can upgrade Node Group Version to:\n",
      "•Control Plane Version \u0000For example upgrade from 1.24 (current Node Group\n",
      "version) to 1.25 (current Control Plane Version), but cannot upgrade to other\n",
      "versions.\n",
      "To perform a Node Group Version upgrade, you can follow these instructions:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu. Select a \n",
      "Cluster where you want to upgrade Node Group Version .\n",
      "Step 3\u0000 Select the icon and select Upgrade Node Group Version to upgrade the\n",
      "node group version.\n",
      "Step 4\u0000 You can select the new version for all Node Groups. The new version needs\n",
      "to be valid and compatible with the current version of the cluster. Specifically: you\n",
      "can choose:\n",
      "•Upgrade Node Group to the same version as Control Plane Version (for\n",
      "example: 1.24 to 1.25\u0000\n",
      "Step 5\u0000 The VKS system will upgrade all Node Groups to the Control Plane version.\n",
      "After the upgrade is complete, the Node Group status returns to ACTIVE .\n",
      "Attention:\n",
      "•Upgrading Node Group Version is optional and independent of\n",
      "upgrading Control Plane Version. However, all Node Groups in a\n",
      "Cluster will be upgraded at the same time, as well as Control Plane\n",
      "Version and Node Group Version in the same Cluster cannot differ by\n",
      "more than 1 minor version. Besides, the VKS system automatically\n",
      "130upgrades the Node Group Version when the current K8S Version being\n",
      "used for your Cluster exceeds the supplier's support period.\n",
      "•During the Node Group Version upgrade, you cannot perform other\n",
      "actions on your Node Group.\n",
      "•Below are a few notes before, during and after the upgrade process,\n",
      "please refer to:\n",
      "Before getting into work:\n",
      "•Check the current version: Visit Releases for a list of supported\n",
      "versions. Select a new version that is valid and compatible with the\n",
      "current version of the cluster.\n",
      "•Ensure Node Group availability: Node Group must be in ACTIVE status\n",
      "and all nodes must be HEALTHY.\n",
      "•Stop running tasks: Stop running tasks on the cluster to avoid affecting\n",
      "the upgrade process.\n",
      "While doing:\n",
      "•Monitor Node Group status: Monitor Node Group status during the\n",
      "upgrade process. Node Group status will change to UPDATING and\n",
      "after completion will return to ACTIVE.\n",
      "•Check system logs: Check system logs to detect any errors or\n",
      "warnings during the upgrade process.\n",
      "After implementation:\n",
      "•Check Node Group Availability: Confirm that the Node Group has been\n",
      "upgraded successfully and all nodes are operating normally.\n",
      "•Test applications: Test applications running on the cluster to ensure\n",
      "they work properly after the upgrade.\n",
      "Note:\n",
      "•Upgrading Node Group Version may take some time depending on the\n",
      "size and complexity of the Node Group.\n",
      "•In some rare cases, upgrading Node Group Version may fail. If this\n",
      "happens, the VKS system will automatically rollback the cluster to the\n",
      "current version.\n",
      "131Use Terraform to create a\n",
      "Cluster and Node Group\n",
      "Terraform is an open source tool used to automate the provisioning and\n",
      "management of infrastructure such as virtual machines, networking, storage, and\n",
      "Kubernetes.\n",
      "With Terraform, you can describe your desired infrastructure in code, then\n",
      "Terraform will perform the necessary operations to create or update the\n",
      "infrastructure to match your description.\n",
      "To initialize a Kubernetes Cluster using Terraform, you need to perform the\n",
      "following steps:\n",
      "1.Access the IAM Portal here , create a Service Account with VKS Full Access\n",
      "authority . Specifically, at the IAM site, you can:\n",
      "•Select \" Create a Service Account \", enter a name for the Service Account\n",
      "and click Next Step to assign permissions to the Service Account.\n",
      "•Find and select Policy: VKSFullAccess then click \" Create a Service\n",
      "Account \" to create a Service Account, Policy: VKSFullAccess is created\n",
      "by VNG Cloud, you cannot delete these policies.\n",
      "•After successful creation, you need to save the Client_ID and Secret_Key of\n",
      "the Service Account to perform the next step.\n",
      "2.Access the VKS Portal here , Activate the VKS service on the Overview tab.\n",
      "Please wait until we successfully create your VKS account.\n",
      "3.Install Terraform:Overview\n",
      "Initialize Cluster and Node Group\n",
      "132•Download and install Terraform for your operating system from \n",
      "https://developer.hashicorp.com/terraform/install .\n",
      "4.Initialize Terraform configuration:\n",
      "•Create a file variable.tfand declare Service Account information in this\n",
      "file.\n",
      "•Create a file main.tfand define the Kubernetes Cluster resources you\n",
      "want to create.\n",
      "For example:\n",
      "•The file variable.tf:you need to replace the Client ID and Client Secret\n",
      "created in step 1 in this file.\n",
      "•The file main.tf:in this example I initialize a Cluster and a Node Group has the\n",
      "following information:\n",
      "◦Cluster name: my-cluster\n",
      "◦K8S Version: v1.28.8\n",
      "◦Mode: Public Cluster and Public Node Group\n",
      "◦Node Group name: my-nodegroup\n",
      "◦Turn on AutoScaling: scale from 0 to 5 nodes\n",
      "Attention:\n",
      "•In the main.tf file, to initialize a cluster with a node group, you must pass in the\n",
      "following parameters:variable \"client_id\" {\n",
      "  type = string\n",
      "  default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
      "}\n",
      "variable \"client_secret\" {\n",
      "  type = string\n",
      "  default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
      "}\n",
      "133•The sample file main.tfhelps you create Cluster and Node Group according to\n",
      "the configuration above:  vpc_id    = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "134terraform {\n",
      "  required_providers {\n",
      "    vngcloud = {\n",
      "      source  = \"vngcloud/vngcloud\"\n",
      "      version = \"1.2.2\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "provider \"vngcloud\" {\n",
      "  token_url        = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/to\n",
      "  client_id        = var.client_id\n",
      "  client_secret    = var.client_secret\n",
      "  vserver_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vserver-gatew\n",
      "  vlb_base_url     = \"https://hcm-3.api.vngcloud.vn/vserver/vlb-gateway\"\n",
      "}\n",
      "resource \"vngcloud_vks_cluster\" \"primary\" {\n",
      "  name      = \"my-cluster\"\n",
      "  description = \"VNGCLOUD uses terraform\"\n",
      "  version = \"v1.28.8\"\n",
      "  cidr      = \"172.16.0.0/16\"\n",
      "  enable_private_cluster = false\n",
      "  network_type = \"CALICO\"\n",
      "  vpc_id    = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  enabled_load_balancer_plugin = true\n",
      "  enabled_block_store_csi_plugin = true\n",
      "}\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" {\n",
      "  cluster_id= vngcloud_vks_cluster.primary.id\n",
      "  name= \"my-nodegroup\"\n",
      "  num_nodes\n",
      "  auto_scale_config {\n",
      "    min_size = 0\n",
      "    max_size = 5\n",
      "  }\n",
      "  upgrade_config {\n",
      "    strategy = \"SURGE\"\n",
      "    max_surge = 1\n",
      " max_unavailable = 0\n",
      "  }\n",
      "  image_id = \"img-983d55cf-9b5b-44cf-aa72-23f3b25d43ce\"\n",
      "  flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6dc4b\"\n",
      "  disk_size = 20\n",
      "  disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\"\n",
      "enableprivatenodes=false\n",
      "1351.Initialize Terraform:\n",
      "•Run the command terraform init.This command will download the\n",
      "necessary plugins and initialize the Terraform state.\n",
      "1.Apply Terraform configuration:\n",
      "•Run command terraform apply.This command will create a Kubernetes\n",
      "Cluster as described in the main.tf.\n",
      "See more about how to use Terraform to work with VKS here .eabe_p ae_odes ase\n",
      "  ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  labels = {\n",
      "    \"mylabel\" = \"vngcloud\"\n",
      "  }\n",
      "  taint {\n",
      "    key    = \"mykey\"\n",
      "    value  = \"myvalue\"\n",
      "    effect = \"PreferNoSchedule\"\n",
      "  }\n",
      "}\n",
      "136Working with NVIDIA\n",
      "GPU Node Group\n",
      "•The NVIDIA GPU Operator is an operator that simplifies the deployment and\n",
      "management of GPU nodes in Kubernetes clusters. It provides a set of\n",
      "Kubernetes custom resources and controllers that work together to automate\n",
      "the management of GPU resources in a Kubernetes cluster.\n",
      "•In this guide, we will show you how to:\n",
      "◦Create a nodegroup with NVIDIA GPUs in a VKS cluster.\n",
      "◦Install the NVIDIA GPU Operator in a VKS cluster.\n",
      "◦Deploy your GPU workload in a VKS cluster.\n",
      "◦Configure GPU Sharing in a VKS cluster.\n",
      "◦Monitor GPU resources in a VKS cluster.\n",
      "◦Autoscale GPU resources in a VKS cluster.\n",
      "•A VKS cluster with at least one NVIDIA GPU nodegroup.\n",
      "•kubectl command-line tool installed on your machine. For more information,\n",
      "see Install and Set Up kubectl.\n",
      "•helm command-line tool installed on your machine. For more information, see \n",
      "Installing Helm.\n",
      "•\u0000Optional) Other tools and libraries that you can use to monitor and manage\n",
      "your Kubernetes resources:\n",
      "◦kubectl-view-allocations plugin for monitoring cluster resources. For\n",
      "more information, see kubectl-view-allocations.\n",
      "•The image below shows my machine setup, it will be used in this guide:Overview\n",
      "Create a nodegroup with NVIDIA GPUs in a\n",
      "VKS cluster\n",
      "137•And this is my VKS cluster with 1 NVIDIA GPU nodegroup, it will be used in this\n",
      "guide, execute the following command to check the nodegroup in your cluster:\n",
      "•This guide only focus on installing the NVIDIA GPU Operator, for more\n",
      "information about the NVIDIA GPU Operator, see NVIDIA GPU Operator\n",
      "Documentation. We manually install the NVIDIA GPU Operator in a VKS cluster\n",
      "by using Helm charts, execute the following command to install the NVIDIA GPU\n",
      "Operator in your VKS cluster:\n",
      "•You MUST wait for the installation to complete (about 5\u000010 minutes), execute\n",
      "the following command to check all the pods in the gpu-operator namespace\n",
      "are running:\n",
      "•The operator will label the node with the nvidia.com/gpu label, which can be\n",
      "used to filter the nodes that have GPUs. The nvidia.com/gpu label is used by\n",
      "the NVIDIA GPU Operator to identify nodes that have GPUs. The NVIDIA GPU# Check kubectl CLI version\n",
      "kubectl version\n",
      "# Check Helm version\n",
      "helm version\n",
      "# Check kubectl-view-allocations version\n",
      "kubectl-view-allocations --version\n",
      "kubectl get nodes -owide\n",
      "helm install nvidia-gpu-operator --wait --version v24.3.0 \\\n",
      "  -n gpu-operator --create-namespace \\\n",
      "  oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/gpu-operator \\\n",
      "  --set dcgmExporter.serviceMonitor.enabled=true\n",
      "kubectl -n gpu-operator get pods -owideInstalling the GPU Operator\n",
      "138Operator will only deploy the NVIDIA GPU device plugin on nodes that have the \n",
      "nvidia.com/gpu label.\n",
      "•For the above result, the single node in the cluster has the \n",
      "nvidia.com/gpu label, which means that the node has GPUs.\n",
      "•These labels also tell that this node is using 1 card of RTX 2080Ti GPU,\n",
      "number of available GPUs, the GPU Memory and other information.\n",
      "•On the pod nvidia-device-plugin-daemonset in the gpu-operator\n",
      "namespace, you can execute nvidia-smi command to check the GPU\n",
      "information of the node:\n",
      "•In this section, we will show you how to deploy a GPU workload in a VKS\n",
      "cluster. We will use the cuda-vectoradd-test workload as an example. The \n",
      "cuda-vectoradd-test workload is a simple CUDA program that adds two\n",
      "vectors together. The program is provided as a container image that you can\n",
      "deploy in your VKS cluster. See file cuda-vectoradd-test.yaml.kubectl get node -o json | jq '.items[].metadata.labels' | grep \"nvidi\n",
      "POD_NAME=$(kubectl -n gpu-operator get pods -l app=nvidia-device-plugi\n",
      "kubectl -n gpu-operator exec -it $POD_NAME -- nvidia-smi\n",
      "Deploy your GPU workload\n",
      "Cuda VectorAdd Test\n",
      "139•In this section, we apply a Deployment manifest for a TensorFlow GPU\n",
      "application. The purpose of this Deployment is to create and manage a single\n",
      "pod running a TensorFlow container that utilizes GPU resource for executing the\n",
      "sum of random values from a normal distribution of size \\( 100000 \\) by \\(\n",
      "100000 \\). For more detail about the manifest, see file tensorflow-gpu.yaml# Apply the manifest\n",
      "kubectl apply -f \\\n",
      "https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main\n",
      "# Check the pods\n",
      "kubectl get pods\n",
      "# Check the logs of the pod\n",
      "kubectl logs cuda-vectoradd\n",
      "# [Optional] Clean the resources\n",
      "kubectl delete deploy cuda-vectoradd\n",
      "# Apply the manifest\n",
      "kubectl apply -f \\\n",
      "  https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/ma\n",
      "# Check the pods\n",
      "kubectl get pods\n",
      "# Check processes are running using nvidia-smi\n",
      "kubectl -n gpu-operator exec -it <put-your-nvidia-driver-daemonset-pod\n",
      "# Check the logs of the TensorFlow pod\n",
      "kubectl logs <put-your-tensorflow-gpu-pod-name> --tail 20\n",
      "# [Optional] Clean the resources\n",
      "kubectl delete deploy tensorflow-gpuTensorFlow Test\n",
      "Configure GPU Sharing\n",
      "140•GPU sharing strategies allow multiple containers to efficiently use your attached\n",
      "GPUs and save running costs. The following tables summarizes the difference\n",
      "between the GPU sharing modes supported by NVIDIA GPUs:\n",
      "Sharing\n",
      "modeSupporte\n",
      "d by VKSWorkload\n",
      "isolation\n",
      "levelPros Cons Suitable\n",
      "for these\n",
      "workload\n",
      "s\n",
      "Multi-\n",
      "instance\n",
      "GPU\n",
      "\u0000MIG\u0000❌ Best•Proce\n",
      "sses\n",
      "are\n",
      "execu\n",
      "ted in\n",
      "parall\n",
      "el\n",
      "•Full\n",
      "isolati\n",
      "on\n",
      "(dedi\n",
      "cated\n",
      "mem\n",
      "ory\n",
      "and\n",
      "comp\n",
      "ute\n",
      "resou\n",
      "rces)•Supp\n",
      "orted\n",
      "by\n",
      "fewer\n",
      "GPU\n",
      "model\n",
      "s\n",
      "(only\n",
      "Ampe\n",
      "re or\n",
      "more\n",
      "recen\n",
      "t\n",
      "archit\n",
      "ectur\n",
      "es)\n",
      "•Coars\n",
      "e-\n",
      "grain\n",
      "ed\n",
      "contr\n",
      "ol\n",
      "over\n",
      "mem\n",
      "ory\n",
      "and\n",
      "comp\n",
      "ute\n",
      "resou\n",
      "rces•Reco\n",
      "mmen\n",
      "ded\n",
      "for\n",
      "workl\n",
      "oads\n",
      "runni\n",
      "ng in\n",
      "parall\n",
      "el and\n",
      "that\n",
      "need\n",
      "certai\n",
      "n\n",
      "resilie\n",
      "ncy\n",
      "and\n",
      "QoS.\n",
      "For\n",
      "exam\n",
      "ple,\n",
      "when\n",
      "runni\n",
      "ng AI\n",
      "infere\n",
      "nce\n",
      "workl\n",
      "oads,\n",
      "multi-\n",
      "instan\n",
      "ce\n",
      "GPU\n",
      "multi-\n",
      "instan\n",
      "ce\n",
      "GPU\n",
      "141allow\n",
      "s\n",
      "multip\n",
      "le\n",
      "infere\n",
      "nce\n",
      "queri\n",
      "es to\n",
      "run\n",
      "simult\n",
      "aneou\n",
      "sly\n",
      "for\n",
      "quick\n",
      "respo\n",
      "nses,\n",
      "witho\n",
      "ut\n",
      "slowi\n",
      "ng\n",
      "each\n",
      "other\n",
      "down.\n",
      "GPU\n",
      "Time-\n",
      "slicing✅ None•Proce\n",
      "sses\n",
      "are\n",
      "execu\n",
      "ted\n",
      "concu\n",
      "rrentl\n",
      "y\n",
      "•Supp\n",
      "orted\n",
      "by\n",
      "older\n",
      "GPU\n",
      "archit\n",
      "ectur\n",
      "es\n",
      "\u0000Pasc\n",
      "al or\n",
      "newer\n",
      ")•No\n",
      "resou\n",
      "rce\n",
      "limits\n",
      "•No\n",
      "mem\n",
      "ory\n",
      "isolati\n",
      "on\n",
      "•Lower\n",
      "perfor\n",
      "manc\n",
      "e due\n",
      "to\n",
      "conte\n",
      "xt-\n",
      "switc\n",
      "hing\n",
      "overh\n",
      "ead•Reco\n",
      "mmen\n",
      "ded\n",
      "for\n",
      "burst\n",
      "y and\n",
      "intera\n",
      "ctive\n",
      "workl\n",
      "oads\n",
      "that\n",
      "have\n",
      "idle\n",
      "perio\n",
      "ds.\n",
      "Thes\n",
      "e\n",
      "workl\n",
      "oads\n",
      "are\n",
      "not\n",
      "cost-\n",
      "142effect\n",
      "ive\n",
      "with a\n",
      "fully\n",
      "dedic\n",
      "ated\n",
      "GPU.\n",
      "By\n",
      "using\n",
      "time-\n",
      "sharin\n",
      "g,\n",
      "workl\n",
      "oads\n",
      "get\n",
      "quick\n",
      "acces\n",
      "s to\n",
      "the\n",
      "GPU\n",
      "when\n",
      "they\n",
      "are in\n",
      "active\n",
      "phase\n",
      "s.\n",
      "•GPU\n",
      "time-\n",
      "sharin\n",
      "g is\n",
      "optim\n",
      "al for\n",
      "scena\n",
      "rios\n",
      "to\n",
      "avoid\n",
      "idling\n",
      "costly\n",
      "GPUs\n",
      "wher\n",
      "e full\n",
      "isolati\n",
      "on\n",
      "and\n",
      "contin\n",
      "143uous\n",
      "GPU\n",
      "acces\n",
      "s\n",
      "might\n",
      "not\n",
      "be\n",
      "neces\n",
      "sary,\n",
      "for\n",
      "exam\n",
      "ple,\n",
      "when\n",
      "multip\n",
      "le\n",
      "users\n",
      "test\n",
      "or\n",
      "protot\n",
      "ype\n",
      "workl\n",
      "oads.\n",
      "•Workl\n",
      "oads\n",
      "that\n",
      "use\n",
      "time-\n",
      "sharin\n",
      "g\n",
      "need\n",
      "to\n",
      "tolera\n",
      "te\n",
      "certai\n",
      "n\n",
      "perfor\n",
      "manc\n",
      "e and\n",
      "latenc\n",
      "y\n",
      "comp\n",
      "romis\n",
      "es.\n",
      "144Multi-\n",
      "process\n",
      "server\n",
      "\u0000MPS\u0000✅ Medium•Proce\n",
      "sses\n",
      "are\n",
      "execu\n",
      "ted\n",
      "parall\n",
      "el\n",
      "•Fine-\n",
      "grain\n",
      "ed\n",
      "contr\n",
      "ol\n",
      "over\n",
      "mem\n",
      "ory\n",
      "and\n",
      "comp\n",
      "ute\n",
      "resou\n",
      "rces\n",
      "alloca\n",
      "tion•No\n",
      "error\n",
      "isolati\n",
      "on\n",
      "and\n",
      "mem\n",
      "ory\n",
      "prote\n",
      "ction•Reco\n",
      "mmen\n",
      "ded\n",
      "for\n",
      "batch\n",
      "proce\n",
      "ssing\n",
      "for\n",
      "small\n",
      "jobs\n",
      "becau\n",
      "se\n",
      "MPS\n",
      "maxi\n",
      "mizes\n",
      "the\n",
      "throu\n",
      "ghput\n",
      "and\n",
      "concu\n",
      "rrent\n",
      "use\n",
      "of a\n",
      "GPU.\n",
      "MPS\n",
      "allow\n",
      "s\n",
      "batch\n",
      "jobs\n",
      "to\n",
      "efficie\n",
      "ntly\n",
      "proce\n",
      "ss in\n",
      "parall\n",
      "el for\n",
      "small\n",
      "to\n",
      "mediu\n",
      "m\n",
      "sized\n",
      "workl\n",
      "oads.\n",
      "•NVIDI\n",
      "A\n",
      "145MPS\n",
      "is\n",
      "optim\n",
      "al for\n",
      "coop\n",
      "erativ\n",
      "e\n",
      "proce\n",
      "sses\n",
      "acting\n",
      "as a\n",
      "single\n",
      "applic\n",
      "ation.\n",
      "For\n",
      "exam\n",
      "ple,\n",
      "MPI\n",
      "jobs\n",
      "with\n",
      "inter-\n",
      "MPI\n",
      "rank\n",
      "parall\n",
      "elism.\n",
      "With\n",
      "these\n",
      "jobs,\n",
      "each\n",
      "small\n",
      "CUDA\n",
      "proce\n",
      "ss\n",
      "(typic\n",
      "ally\n",
      "MPI\n",
      "ranks\n",
      ") can\n",
      "run\n",
      "concu\n",
      "rrentl\n",
      "y on\n",
      "the\n",
      "GPU\n",
      "to\n",
      "146•VKS uses the built-in timesharing ability provided by the NVIDIA GPU and the\n",
      "software stack. Starting with the Pascal architecture, NVIDIA GPUs support\n",
      "instruction level preemption. When doing context switching between processes\n",
      "running on a GPU, instruction-level preemption ensures every process gets a\n",
      "fair timeslice. GPU time-sharing provides software-level isolation between the\n",
      "workloads in terms of address space isolation, performance isolation, and error\n",
      "isolation.fully\n",
      "satur\n",
      "ate\n",
      "the\n",
      "whole\n",
      "GPU.\n",
      "•Workl\n",
      "oads\n",
      "that\n",
      "use\n",
      "CUDA\n",
      "MPS\n",
      "need\n",
      "to\n",
      "tolera\n",
      "te the\n",
      "mem\n",
      "ory\n",
      "prote\n",
      "ction\n",
      "and\n",
      "error\n",
      "contai\n",
      "nmen\n",
      "t\n",
      "limitat\n",
      "ions.\n",
      "GPU time-slicing\n",
      "Configure GPU time-slicing\n",
      "147•To enable GPU time-slicing, you need to configure a ConfigMap with the\n",
      "following settings:\n",
      "•The above manifest allows 4 pods to share the GPU. The replicas field\n",
      "specifies the number of pods that can share the GPU. The replicas field\n",
      "should be less than the number of GPUs on the node. The nvidia.com/gpu\n",
      "label is used to filter the nodes that have GPUs. The migStrategy field is set to \n",
      "none to disable MIG.\n",
      "•This configuration will apply to all nodes in the cluster that have the \n",
      "nvidia.com/gpu label. To apply the configuration, execute the following\n",
      "command:\n",
      "•And then you need to patch the ClusterPolicy to enable GPU time-slicing\n",
      "using the any setting:apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: gpu-sharing-config\n",
      "data:\n",
      "  any: |-\n",
      "    version: v1\n",
      "    flags:\n",
      "      migStrategy: none            # Disable MIG, MUST be none in the \n",
      "    sharing:\n",
      "      timeSlicing:\n",
      "        resources:\n",
      "        - name: nvidia.com/gpu     # Only apply for the node with the \n",
      "          replicas: 4              # Allow 4 pods to share the GPU, SH\n",
      "kubectl -n gpu-operator create -f \\\n",
      "  https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/ma\n",
      "# Patch the ClusterPolicy\n",
      "kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n",
      "  -n gpu-operator --type merge \\\n",
      "  -p '{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"gpu-sharing-confi\n",
      "# Disable DCGM exporter, time-slicing not support DCGM exporter\n",
      "kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n",
      "  -n gpu-operator --type merge \\\n",
      "  -p '{\"spec\": {\"dcgmExporter\": {\"enabled\": false}}}'\n",
      "148•Your new configuration will be applied to all nodes in the cluster that\n",
      "have the nvidia.com/gpu label.\n",
      "•The configuration is considered successful if the ClusterPolicy\n",
      "STATUS is ready.\n",
      "•Because of the sharing.timeSlicing.resources.replicas is set to 4,\n",
      "you can deploy up to 4 pods that share the GPU.\n",
      "•My cluster has only 1 GPU node, so I can deploy up to 4 pods that share\n",
      "the GPU.\n",
      "•Until now, we have configured the GPU time-slicing, now we will deploy 5 pods\n",
      "that share the GPU using Deployment, because of only 4 pods can share the\n",
      "GPU, the 5th pod will be in Pending state. See file time-slicing-\n",
      "verification.yaml.\n",
      "•VKS uses NVIDIA's Multi-Process Service \u0000MPS\u0000. NVIDIA MPS is an alternative,\n",
      "binary-compatible implementation of the CUDA API designed to transparently\n",
      "enable co-operative multi-process CUDA workloads to run concurrently on a# Apply the manifest\n",
      "kubectl apply -f \\\n",
      "  https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/ma\n",
      "# Check the pods\n",
      "kubectl get pods\n",
      "# Check the logs of the TensorFlow pod\n",
      "kubectl logs <put-your-time-slicing-verification-pod-name> --tail 10\n",
      "# Get the event of pending pod\n",
      "kubectl events | grep \"FailedScheduling\"\n",
      "# [Optional] Clean the resources\n",
      "kubectl delete deploy time-slicing-verificationVerify GPU time-slicing\n",
      "Multi-process server (MPS)\n",
      "149single GPU device. GPU with NVIDIA MPS provides software-level isolation in\n",
      "terms of resource limits (active thread percentage and pinned device memory).\n",
      "•To enable GPU MPS, you need to update the previous ConfigMap with the\n",
      "following settings:\n",
      "•Now let's apply this new ConfigMap and then patching the ClusterPolicy like\n",
      "the way at the GPU time-slicing section.apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: gpu-sharing-config\n",
      "data:\n",
      "  any-mps: |-\n",
      "    version: v1\n",
      "    flags:\n",
      "      migStrategy: none            # MIG strategy is not used, this fi\n",
      "    sharing:\n",
      "      mps:                         # Enable MPS for the GPU\n",
      "        resources:\n",
      "        - name: nvidia.com/gpu     # Only apply for the node with the \n",
      "          replicas: 4              # Allow 4 pods to share the GPU\n",
      "# Delete the old configmap\n",
      "kubectl -n gpu-operator delete cm gpu-sharing-config\n",
      "kubectl -n gpu-operator create -f \\\n",
      "  https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/ma\n",
      "# Patch the ClusterPolicy\n",
      "kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n",
      "  -n gpu-operator --type merge \\\n",
      "  -p '{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"gpu-sharing-confi\n",
      "# Disable DCGM exporter, MPS not support DCGM exporter\n",
      "kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n",
      "  -n gpu-operator --type merge \\\n",
      "  -p '{\"spec\": {\"dcgmExporter\": {\"enabled\": false}}}'\n",
      "# Check MPS server is running or not\n",
      "kubectl -n gpu-operator get podsConfigure MPS\n",
      "150•Your new configuration will be applied to all nodes in the cluster that\n",
      "have the nvidia.com/gpu label.\n",
      "•The configuration is considered successful if the ClusterPolicy\n",
      "STATUS is ready.\n",
      "•Because of the sharing.mps.resources.replicas is set to 4, you can\n",
      "deploy up to 4 pods that share the GPU.\n",
      "•Until now, we have configured the GPU MPS, now we will deploy 5 pods that\n",
      "share the GPU using Deployment, because of only 4 pods can share the GPU,\n",
      "the 5th pod will be in Pending state. See file mps-verification.yaml.\n",
      "•An alternative to applying one cluster-wide configuration is to specify multiple\n",
      "time-slicing configurations in the ConfigMap and to apply labels node-by-\n",
      "node to control which configuration is applied to which nodes.\n",
      "•In this guideline, I add a new RTX\u00004090 into the cluster.\n",
      "•This configuration should be greate if your cluster have multiple nodes with\n",
      "different GPU models. For example:\n",
      "◦NodeGroup 1 includes the instance of GPU RTX 2080Ti.\n",
      "◦NodeGroup 2 includes the instance of GPU RTX 4090.# Apply the manifest\n",
      "kubectl apply -f \\\n",
      "  https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/ma\n",
      "# Check the pods\n",
      "kubectl get pods\n",
      "# Check the logs of the TensorFlow pod\n",
      "kubectl logs -l job-name=nbody-sample\n",
      "# [Optional] Clean the resources\n",
      "kubectl delete job nbody-sampleVerify MPS\n",
      "Applying Multiple Node-Specific Configurations\n",
      "151•And if you want to run multiple GPU sharing strategies in the same cluster, you\n",
      "can apply multiple configurations to the same node by using labels. For\n",
      "example:\n",
      "◦NodeGroup 1 includes the instance of GPU RTX 2080Ti with 4 pods sharing\n",
      "the GPU using time-slicing.\n",
      "◦NodeGroup 2 includes the instance of GPU RTX 4090 with 8 pods sharing\n",
      "the GPU using MPS.\n",
      "•To using this feature, you need to update the previous ConfigMap with the\n",
      "following settings:\n",
      "•Apply the above configure.apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: gpu-multi-sharing-config\n",
      "data:\n",
      "  rtx-2080ti: |-                                # Same the name with t\n",
      "    version: v1\n",
      "    flags:\n",
      "      migStrategy: none                         # MIG strategy is not \n",
      "    sharing:\n",
      "      timeSlicing:\n",
      "        resources:\n",
      "        - name: nvidia.com/gpu\n",
      "          replicas: 4                           # Allow the node using\n",
      "  rtx-4090: |-                                  # Same the name with t\n",
      "    version: v1\n",
      "    flags:\n",
      "      migStrategy: none                         # MIG strategy is not \n",
      "    sharing:\n",
      "      mps:\n",
      "        resources:\n",
      "        - name: nvidia.com/gpu\n",
      "          replicas: 8                           # Allow the node usingConfigure Multiple Node-Specific Configurations\n",
      "152•Now, we need to label the node with the name that you specified in the \n",
      "ConfigMap:\n",
      "•In this example, we will training MNIST model in TensorFlow using the GPU RTX\n",
      "2080Ti and RTX 4090. The RTX 2080Ti will be shared by 4 pods using time-\n",
      "slicing and the RTX 4090 will be shared by 8 pods using MPS. See file \n",
      "tensorflow-mnist-sample.yaml.kubectl -n gpu-operator create -f \\\n",
      "  https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/ma\n",
      "# Patch the ClusterPolicy\n",
      "kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n",
      "  -n gpu-operator --type merge \\\n",
      "  -p '{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"gpu-multi-sharing\n",
      "# Disable DCGM exporter\n",
      "kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n",
      "  -n gpu-operator --type merge \\\n",
      "  -p '{\"spec\": {\"dcgmExporter\": {\"enabled\": false}}}'\n",
      "# Check the ClusterPolicy\n",
      "kubectl get clusterpolicy\n",
      "# Get the node names\n",
      "kubectl get nodes\n",
      "# Label the node with the name that you specified in the ConfigMap\n",
      "kubectl label node <node-name> nvidia.com/device-plugin.config=rtx-208\n",
      "kubectl label node <node-name> nvidia.com/device-plugin.config=rtx-409\n",
      "Verify Multiple Node-Specific Configurations\n",
      "153•The pods are running on the node with the GPU RTX 2080Ti and RTX\n",
      "4090 within different GPU sharing strategies.\n",
      "•Monitoring NVIDIA GPU resources in a Kubernetes cluster is essential for\n",
      "ensuring optimal performance, efficient resource utilization, and proactive issue\n",
      "resolution. This overview provides a comprehensive guide to setting up and\n",
      "leveraging Prometheus and the NVIDIA Data Center GPU Manager \u0000DCGM) to\n",
      "monitor GPU resources in a Kubernetes environment.\n",
      "•Firstly, we need to install Prometheus Stack and Prometheus Adapter to\n",
      "integrate with the Kubernetes API server. Execute the following command to\n",
      "install the Prometheus Stack and Prometheus Adapter in your VKS cluster:# Apply the manifest\n",
      "kubectl apply -f \\\n",
      "  https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-g\n",
      "# Check the pods\n",
      "kubectl get pods -owide\n",
      "# Check the logs of the TensorFlow pod\n",
      "kubectl logs <put-your-favourite-tensorflow-mnist-pod-name> --tail 20\n",
      "# [Optional] Clean the resources\n",
      "kubectl delete deploy tensorflow-mnist\n",
      "Monitoring GPU Resources\n",
      "154•After the installation is complete, execute the following command to check the\n",
      "resources of Prometheus are running:\n",
      "•Now, we need to enable the DCGM exporter to monitor the GPU resources in\n",
      "the VKS cluster. Execute the following command to enable the DCGM exporter\n",
      "in your VKS cluster:\n",
      "•Let's forward the Prometheus Adapter to your local machine to check the GPU\n",
      "metrics by visit http://localhost:9090\u0000# Install Prometheus Stack using Helm\n",
      "helm install --wait prometheus-stack \\\n",
      "  --namespace prometheus --create-namespace \\\n",
      "  oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/kube-prometheus-\n",
      "  --version 60.0.2 \\\n",
      "  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmVal\n",
      "# Install and configure Prometheus Adapter using Helm \n",
      "prometheus_service=$(kubectl get svc -n prometheus -lapp=kube-promethe\n",
      "helm install --wait prometheus-adapter \\\n",
      "  --namespace prometheus --create-namespace \\\n",
      "  oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/prometheus-adapt\n",
      "  --version 4.10.0 \\\n",
      "  --set prometheus.url=http://${prometheus_service}.prometheus.svc.clu\n",
      "# Check the resources of Prometheus are running\n",
      "kubectl -n prometheus get all \n",
      "# Enable the DCGM exporter\n",
      "kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n",
      "  -n gpu-operator --type merge \\\n",
      "  -p '{\"spec\": {\"dcgmExporter\": {\"enabled\": true}}}'\n",
      "# Confirm Prometheus can scrape the DCGM exporter metrics, sometime yo\n",
      "# (about 1-3 mins) for the DCGM exporter to be ready\n",
      "kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq -r . | grep\n",
      "# Forward the Prometheus Adapter to your local machine\n",
      "kubectl -n prometheus \\\n",
      "  port-forward svc/prometheus-stack-kube-prom-prometheus 9090:9090\n",
      "155•The following table lists some observable GPU metrics. For details about more\n",
      "metrics, see Field Identifiers.\n",
      "◦Table 1: Usage\n",
      "◦Table 2: Memory\n",
      "◦Table 3: Temperature and power\n",
      "Metric Name Metric Type Unit Description\n",
      "DCGM_FI_DEV_G\n",
      "PU_UTILGauge Percentage GPU usage.\n",
      "DCGM_FI_DEV_M\n",
      "EM_COPY_UTILGauge Percentage Memory usage.\n",
      "DCGM_FI_DEV_E\n",
      "NC_UTILGauge Percentage Encoder usage.\n",
      "DCGM_FI_DEV_D\n",
      "EC_UTILGauge Percentage Decoder usage.\n",
      "Metric Name Metric Type Unit Description\n",
      "DCGM_FI_DEV_F\n",
      "B_FREEGauge MB Number of\n",
      "remaining frame\n",
      "buffers. The\n",
      "frame buffer is\n",
      "called VRAM.\n",
      "DCGM_FI_DEV_F\n",
      "B_USEDGauge MB Number of used\n",
      "frame buffers.\n",
      "The value is the\n",
      "same as the\n",
      "value of\n",
      "memory-usage\n",
      "in the nvidia-smi\n",
      "command.\n",
      "Metric Name Metric Type Unit Description\n",
      "DCGM_FI_DEV_G\n",
      "PU_TEMPGauge °C Current GPU\n",
      "temperature of\n",
      "the device.\n",
      "156•To enable this feature, you MUST:\n",
      "◦Enable Autoscale for GPU Nodegroups that you want to scale on the VKS\n",
      "portal.\n",
      "◦Install Keda using Helm chart in your VKS cluster.\n",
      "•In the case you DO NOT install Keda in your cluster, VKS autoscaler feature will\n",
      "detect the Pending pods and scale the GPU Nodegroup automatically. This\n",
      "happens when the number of replicas of the Deployment is greater than the\n",
      "number of available GPUs that you configured in the ConfigMap.\n",
      "•If you already installed Keda in your cluster, you can use the ScaledObject to\n",
      "scale the GPU Nodegroup based on the metrics that you want. For example, you\n",
      "can scale the GPU Nodegroup based on the GPU usage, memory usage, or any\n",
      "other metrics that you want. For example:DCGM_FI_DEV_P\n",
      "OWER_USAGEGauge W Power usage of\n",
      "the device.\n",
      "apiVersion: keda.sh/v1alpha1\n",
      "kind: ScaledObject\n",
      "metadata:\n",
      "  name: scaled-object\n",
      "spec:\n",
      "  scaleTargetRef:\n",
      "    name: scaling-app   # The name of the Deployment, MUST in same nam\n",
      "  minReplicaCount: 1   # Optional. Default: 0\n",
      "  maxReplicaCount: 3   # Optional. Default: 100\n",
      "  triggers: # Will be trigger if either of these triggers is true\n",
      "    - type: prometheus\n",
      "      metadata: # prometheus-stack-kube-prom-prometheus\n",
      "        serverAddress: http://prometheus-stack-kube-prom-prometheus.pr\n",
      "        metricName: engine_active\n",
      "        query: sum(DCGM_FI_DEV_GPU_UTIL) / count(DCGM_FI_DEV_GPU_UTIL)\n",
      "        threshold: '0.5'  # Scale the GPU Nodegroup when the GPU usage\n",
      "    - type: prometheus\n",
      "      metadata: # prometheus-stack-kube-prom-prometheus\n",
      "        serverAddress: http://prometheus-stack-kube-prom-prometheus.pr\n",
      "        metricName: engine_active\n",
      "        query: sum(DCGM_FI_DEV_MEM_COPY_UTIL) / count(DCGM_FI_DEV_MEM_\n",
      "        threshold: '0.5'  # Scale the GPU Nodegroup when the GPU memorAutoscaling GPU Resources\n",
      "157•The above manifest scales the GPU Nodegroup based on the GPU usage and\n",
      "memory usage. The query field specifies the query to fetch the metrics from\n",
      "Prometheus. The threshold field specifies the threshold value to scale the\n",
      "GPU Nodegroup. The minReplicaCount and maxReplicaCount fields specify\n",
      "the minimum and maximum number of replicas that the GPU Nodegroup can\n",
      "scale to.\n",
      "•Now let's install Keda in your cluster by executing the below command:\n",
      "•Apply scaling-app.yaml manifest to generate resources for testing the\n",
      "autoscaling feature. This manifest run 1 pod of CUDA VectorAdd Test and the\n",
      "GPU Nodegroup will be scaled to 3 when the GPU usage is greater than 50%.\n",
      "•Apply scale-gpu.yaml manifest to create the ScaleObject for the above\n",
      "application. This manifest will scale the GPU Nodegroup based on the GPU\n",
      "usage.\n",
      "•When the ScaledObjectReady value is True, the GPU Nodegroup will\n",
      "be scaled based on the GPU usage.helm install --wait kedacore \\\n",
      "  --namespace keda --create-namespace \\\n",
      "  oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/keda \\\n",
      "  --version 2.14.2\n",
      "kubectl -n keda get all\n",
      "kubectl apply -f \\\n",
      "  https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-g\n",
      "kubectl apply -f \\\n",
      "  https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-g\n",
      "kubectl get deploy\n",
      "# Check the ScaledObject\n",
      "kubectl get scaledobject\n",
      "158Clusters\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "connected together to run containerized applications. Cluster provides a unified\n",
      "environment to deploy, manage, and operate containers at scale.\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have a VPC\n",
      "or Subnet yet, please create a VPC and Subnet according to the instructions\n",
      "here .\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please create an SSH key according to the instructions here .\n",
      "•Installed and configured kubectl on your device. Please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use a kubectl\n",
      "version that is too old, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully create your VKS account. After Activate\n",
      "successfully, select Create a Cluster.\n",
      "Step 4\u0000 At the Cluster initialization screen, we have set up information for the\n",
      "Cluster and a Default Node Group for you. You can keep these default values   orPrerequisites:\n",
      "Initialize Cluster\n",
      "159adjust the desired parameters for the Cluster and Node Group at Cluster\n",
      "Configuration, Default Node Group Configuration, Plugin.\n",
      "•Cluster Configuration:\n",
      "◦Cluster Information:\n",
      "▪Cluster Name: Name for your Cluster. The name can only contain\n",
      "alphanumeric characters (az, AZ, 0\u00009, '_', '-'). Your input data length\n",
      "must be from 5 to 50. The name must be unique within the Region and\n",
      "VNG Cloud account you are creating the Cluster for.\n",
      "▪Kubernetes Version: The version of Kubernetes that will be used for\n",
      "your Cluster. We recommend that you choose the latest version, unless\n",
      "you need an older version.\n",
      "▪Description: Enter the information you want to note for the Cluster to\n",
      "create a separate mark for easier management in the future.\n",
      "◦Network Setting:\n",
      "▪Network type, Encapsulation Mode are selected by default by the\n",
      "system and you cannot change them, however you can re-enter Calico\n",
      "CIDR parameters (note that IP must be private and can be selected\n",
      "according to the following options ( 10.0.0.0 \u0000 10.255.0.0 / 172.16.0.0 \u0000\n",
      "172.24.0.0 / 192.168.0.0\u0000\n",
      "▪VPC\u0000 Select an existing VPC that meets K8S requirements to create your\n",
      "Cluster. Before choosing a VPC, we recommend that you familiarize\n",
      "yourself with all VPC requirements and considerations as well as Subnet\n",
      "requirements and considerations. You cannot change which VPC you\n",
      "want to use after creating the Cluster. If no VPC is listed, you need to\n",
      "create one first. For more information, see Create a VPC .\n",
      "▪Subnet: By default, all available subnets in the VPC specified in the\n",
      "previous field will be randomly selected in the first order, you can\n",
      "choose another Subnet again, but only 1 can be selected.\n",
      "•Default Node group Configuration:\n",
      "◦Node Group Information:\n",
      "▪Node Group Name : A memorable name for your Node Group.\n",
      "▪Number of nodes: Enter the number of Worker nodes for your Cluster,\n",
      "note that the number of nodes needs to be greater than or equal to 1 and\n",
      "less than or equal to 100.\n",
      "160◦Node Group Automation Setting:\n",
      "▪Auto Healing: By default we will enable the HA feature in your Cluster.\n",
      "When a node or pod fails, Kubernetes will automatically restart or create\n",
      "a new pod to replace it, ensuring your application always operates\n",
      "without interruption.\n",
      "▪Auto Scaling: Enable auto-scaling in your Cluster. Auto scaling helps\n",
      "automatically adjust the number of pods (application deployment units)\n",
      "based on actual usage needs, avoiding wasting resources when demand\n",
      "is low or overloading when demand is high.\n",
      "•Minimum node : minimum number of nodes that the Cluster needs\n",
      "to have.\n",
      "•Maximum node : maximum number of nodes that the Cluster can\n",
      "scale to.\n",
      "▪Node Group upgrade strategy: Node Group upgrade strategy. When you\n",
      "set up a Node Group Upgrade Strategy via the Surge upgrade method\n",
      "for a Node Group in VKS, the VKS system will update sequentially to\n",
      "upgrade the nodes, in an unspecified order .\n",
      "•Max surge: limits the number of nodes that can be upgraded\n",
      "simultaneously (the number of new nodes (surge) that can be\n",
      "created at the same time). Default Max surge = 1 - upgrade only one\n",
      "node at a time. with maxUnavailable\n",
      "•Max unavailable : limits the number of nodes that cannot be reached\n",
      "during the upgrade (the number of existing nodes that can be offline\n",
      "at the same time). Default Max unavailable = 0 - ensures all nodes\n",
      "are accessible during the upgrade.\n",
      "◦Node Group Setting:\n",
      "▪Image : By default we provide one type of Image, Ubuntu with\n",
      "containerd.\n",
      "▪Instance type : select the appropriate configuration instance type for\n",
      "the Worker node according to your usage needs.\n",
      "◦Node Group Volume Setting: Boot Volume Configuration – Parameters are\n",
      "set by default by the system to help optimize your Cluster\n",
      "◦Node Group Network Setting: You can choose Public Node Group or \n",
      "Private Node Group depending on your Cluster usage needs.\n",
      "161◦Node Group Security Setting: You can choose Security Group and SSH Key\n",
      "for your Node Group.\n",
      "◦Node Group Metadata Setting: You can enter the corresponding Metadata\n",
      "for the Node Group.\n",
      "•Plugins\n",
      "◦Enable BlockStore Persistent Disk CSI Driver : enable us to automatically\n",
      "install CSI Controller on your Cluster.\n",
      "◦Enable vLB Native Integration Driver : enable us to automatically install LB\n",
      "Controller on your Cluster.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the Cluster's status is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu.\n",
      "Step 3\u0000 In the successfully created Cluster , select the icon and select Download\n",
      "Config File.\n",
      "Step 4\u0000 The config file will be saved to your computer, now you can use Kubectl to\n",
      "manage your Cluster on your personal device.\n",
      "Attention:\n",
      "Download the Kube Config file\n",
      "Delete a Cluster\n",
      "162When you no longer need to use a Kubernetes Cluster, you should delete\n",
      "the resources associated with that cluster so you don't incur any\n",
      "unnecessary costs. When deleting a Kubernetes Cluster the following\n",
      "resources will be deleted:\n",
      "•Control Plane Resource of the Cluster.\n",
      "•All nodes in the Cluster \u0000VM\u0000\n",
      "•Which Pods are all running on the nodes.\n",
      "•The default Security Group is created for that Cluster.\n",
      "•The default Load Balancer is created for that Cluster.\n",
      "•ETCD.\n",
      "The system may not delete the following resources:\n",
      "•The Load Balancer is integrated into the Cluster by you.\n",
      "•Persistent Volume is integrated into the Cluster by you.\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu.\n",
      "Step 3\u0000 In the successfully created Cluster , select the cluster you want to delete\n",
      "and select Delete\n",
      "Step 4\u0000 Select Delete to completely delete your Cluster.\n",
      "163Public Cluster and Private Cluster\n",
      "Below are the current concepts being provided to you by VKS\u0000\n",
      "When you create a Public Cluster with Public Node Group , the VKS system will:\n",
      "•Create a VM with Floating IP (ie Public IP\u0000. Now these VMs \u0000Nodes\u0000 can directly\n",
      "join the K8S cluster through this Public IP. By using Public Cluster and Public\n",
      "Node Group, you can easily create Kubernetes clusters and expose services\n",
      "without using Load Balancer. This will contribute to cost savings for your\n",
      "cluster.\n",
      "When you create a Public Cluster with a Private Node Group , the VKS system will:\n",
      "•Create VM without Floating IP (ie without Public IP\u0000. At this time, these VMs\n",
      "\u0000Nodes\u0000 cannot join the K8S cluster directly. In order for these VMs to join the\n",
      "K8S cluster, you need to use a NAT Gateway ( NATGW ). NATGW acts as a\n",
      "relay station, allowing VMs to connect to the K8S cluster without needing a\n",
      "Public IP. With VNG Cloud, we recommend you use Pfsense or Palo Alto as a\n",
      "NATGW for your Cluster. Pfsense will help you manage incoming and outgoing\n",
      "network traffic (inbound and outbound traffic) effectively, ensuring network\n",
      "security and access management. Besides, using Private Node Group will help\n",
      "you control applications in the cluster more securely, specifically you can limit\n",
      "control plane access rights through the Whitelist IP feature.\n",
      "1. Public Cluster\n",
      "164When you create a Public Cluster with Public/Private Node Group , the VKS\n",
      "system will:\n",
      "•To enhance the security of your cluster, we have introduced the private cluster\n",
      "model. The Private Cluster feature helps make your K8S cluster as secure as\n",
      "possible, all connections are completely private from the connection between\n",
      "nodes to the control plane, the connection from the client to the control plane,\n",
      "or the connection from nodes to products. Other services in VNG Cloud such\n",
      "as: vStorage, vCR, vMonitor, VNGCloud APIs,...Private Cluster is the ideal\n",
      "choice for services that require strict access control, ensuring compliance\n",
      "with security regulations and data privacy.\n",
      "Below is a comparison table between creating and using Public Cluster and Private\n",
      "Cluster on the VKS system:\n",
      "Criteria Public Cluster Private Cluster\n",
      "Connect Use Public IP addresses to\n",
      "communicate between\n",
      "nodes and control plane,\n",
      "between clients and control\n",
      "plane, between nodes andUse Private IP addresses to\n",
      "communicate between\n",
      "nodes and control plane,\n",
      "between clients and control\n",
      "plane, between nodes and2. Private Cluster\n",
      "3. Comparison between using Public Cluster\n",
      "and Private Cluster\n",
      "165other services in VNG\n",
      "Cloud.other services in VNG\n",
      "Cloud.\n",
      "Security Medium security since\n",
      "connections use Public IP.Higher security with all\n",
      "connections private and\n",
      "limited access.\n",
      "Access managementMore difficult to control,\n",
      "access can be managed\n",
      "through the Whitelist\n",
      "featureStrict access control, all\n",
      "connections are within VNG\n",
      "Cloud's private network,\n",
      "thereby minimizing the risk\n",
      "of external network\n",
      "attacks.\n",
      "Scalability\n",
      "\u0000AutoScaling)Easily scalable through \n",
      "Auto Scaling feature .Easily scalable through \n",
      "Auto Scaling feature .\n",
      "AutoHealing Automatically detect errors\n",
      "and restart the node ( Auto\n",
      "Healing )Automatically detect errors\n",
      "and restart the node ( Auto\n",
      "Healing )\n",
      "Accessibility from\n",
      "outsideEasy access from\n",
      "anywhere with internet.Access from outside must\n",
      "be through other security\n",
      "solutions.\n",
      "Configuration and\n",
      "deploymentSimpler because it does not\n",
      "require setting up an\n",
      "internal network.More complex, requires\n",
      "private and secure network\n",
      "configuration.\n",
      "Cost Usually lower because\n",
      "there is no need to set up a\n",
      "complex security\n",
      "infrastructure.Higher cost due to\n",
      "additional security and\n",
      "management components\n",
      "required. Specifically,\n",
      "when using a private\n",
      "cluster, you need to pay\n",
      "for 4 automatically created\n",
      "private service endpoints\n",
      "to connect to services on\n",
      "VNG Cloud.\n",
      "Flexibility High, easy to change and\n",
      "access services.More flexible in\n",
      "applications that require\n",
      "security, but less flexible\n",
      "for applications that require\n",
      "external access.\n",
      "166Therefore:\n",
      "•Public Cluster : Suitable for applications that do not require high security and\n",
      "need flexibility and access from multiple locations. Easy to deploy and manage\n",
      "but has higher security risks.\n",
      "•Private Cluster : Suitable for applications that require high security, strictly\n",
      "complying with security and privacy regulations. Provides stable and secure\n",
      "connectivity, but requires more complex configuration and management, as well\n",
      "as higher costs.\n",
      "167Upgrading Control Plane Version\n",
      "Currently, our VKS system has supported you to upgrade Control Plane Version,\n",
      "you can:\n",
      "•Upgrade to a newer Minor Version (e.g. 1.24 to 1.25\u0000\n",
      "•Upgrade to a newer Patch Version (for example: 1.24.2\u0000VKS.100 to 1.24.5\u0000\n",
      "VKS.200\u0000\n",
      "To upgrade the Control Plane version, you can follow these instructions:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu.\n",
      "Step 3\u0000 Select the icon and select Upgrade control plane version to upgrade the\n",
      "control plane version.\n",
      "Step 4\u0000 You can select a new version for the control plane. The new version needs\n",
      "to be valid and compatible with the current version of the cluster. Specifically: you\n",
      "can choose:\n",
      "•Upgrade to a newer Minor Version (e.g. 1.24 to 1.25\u0000\n",
      "•Upgrade to a newer Patch Version (for example: 1.24.2\u0000VKS.100 to 1.24.5\u0000\n",
      "VKS.200\u0000\n",
      "Step 4\u0000 The VKS system will upgrade the Control Plane components of the Cluster\n",
      "to the new version. After the upgrade is complete, the Cluster state returns to \n",
      "ACTIVE .\n",
      "Attention:\n",
      "•Upgrading Control Plane Version is optional and independent of\n",
      "upgrading Node Group Version. However, Control Plane Version and\n",
      "Node Group Version in the same Cluster cannot differ by more than 1\n",
      "minor version. Besides, the VKS system automatically upgrades\n",
      "168Control Plane Version when the current K8S Version used for your\n",
      "Cluster exceeds the supplier's support period.\n",
      "•During the Control Plane Version upgrade, you cannot perform other\n",
      "actions on your Cluster.\n",
      "•Below are a few notes before, during and after the upgrade process,\n",
      "please refer to:\n",
      "Before getting into work:\n",
      "•Back up data: You should back up cluster data before upgrading to\n",
      "ensure safety in case the upgrade fails.\n",
      "•Check the current version: Visit Releases for a list of supported\n",
      "versions. Select a new version that is valid and compatible with the\n",
      "current version of the cluster.\n",
      "•Ensure cluster availability: Cluster must be in active state \u0000ACTIVE) and\n",
      "all nodes must be HEALTHY.\n",
      "•Stop running tasks: Stop running tasks on the cluster to avoid affecting\n",
      "the upgrade process.\n",
      "While doing:\n",
      "•Monitor cluster status: Monitor cluster status during the upgrade\n",
      "process. The cluster status will change to UPDATING and after\n",
      "completion will return to ACTIVE.\n",
      "•Check system log: Check the system log for any errors or warnings\n",
      "during the upgrade process.\n",
      "After implementation:\n",
      "•Check cluster availability: Confirm that the cluster has been upgraded\n",
      "successfully and all nodes are operating normally.\n",
      "•Test applications: Test applications running on the cluster to ensure\n",
      "they work properly after the upgrade.\n",
      "Note:\n",
      "•Upgrading Control Plane Version may take some time depending on\n",
      "the size and complexity of the cluster.\n",
      "169•In some rare cases, the Control Plane Version upgrade may fail. If this\n",
      "happens, the VKS system will automatically rollback the cluster to the\n",
      "current version.\n",
      "170Whitelist\n",
      "The IP Whitelist feature on VKS's Private Node Group mode allows you to only allow\n",
      "specific IP addresses to connect to your Cluster. This helps increase security for\n",
      "applications and sensitive data by restricting access from unknown sources.\n",
      "Benefit\n",
      "•Enhanced security: IP Whitelist helps protect your data and applications from\n",
      "potential threats on public networks, such as cyberattacks and data breaches.\n",
      "•Minimize risk: By restricting access to sensitive nodes, Whitelist IP helps\n",
      "minimize the risk of spreading a data breach to other parts of your network.\n",
      "•Greater control: Whitelist IP allows you to tightly control access to your nodes,\n",
      "ensuring only authorized users and applications can access.\n",
      "Recommendations for Using Whitelist in Cluster Models:\n",
      "•Recommendation : Not recommended to use whitelist.\n",
      "•If you need to use Whitelist IP for security, please allow vServer's IP Range\n",
      "Public list according to the following list:\n",
      "CopyOverview\n",
      "1. Public Cluster Only Includes Public Node\n",
      "Group\n",
      "171•Recommendation : Can use whitelist feature.\n",
      "•Need to whitelist additional IP of NAT Gateway.\n",
      "•Recommendation: Can use whitelist feature.\n",
      "To use the IP Whitelist feature on Private Node Group mode, you need to perform\n",
      "the following steps:103.245.249.0/24\n",
      "103.245.251.0/24\n",
      "116.118.95.0/24\n",
      "58.84.1.0/24\n",
      "58.84.2.0/24\n",
      "61.28.226.0/24\n",
      "61.28.227.0/24\n",
      "61.28.229.0/24\n",
      "61.28.230.0/24\n",
      "61.28.231.0/24\n",
      "180.93.182.0/24\n",
      "61.28.233.0/24\n",
      "61.28.235.0/24\n",
      "61.28.236.0/24\n",
      "61.28.238.0/24\n",
      "180.93.183.0/24\n",
      "2. Public Cluster Includes Private Node\n",
      "Group Going Through NAT Gateway\n",
      "(Pfsense, PaloAlto)\n",
      "3. Private Cluster Includes Public Node\n",
      "Group or Private Node Group \n",
      "Edit Whitelist\n",
      "172Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu.\n",
      "Step 3\u0000 Select the icon and select Edit Whitelist or select the Edit icon when\n",
      "viewing details of a Cluster to add a Whitelist to your Cluster.\n",
      "Step 4\u0000 Now, the Edit Whitelist screen displays, you can enter the IP address you\n",
      "want to allow access to the Cluster then select Add .\n",
      "Step 5\u0000 Repeat step 4 if you want to add more Whitelist IPs to your Cluster. You\n",
      "can also select Delete to delete the Whitelist IP you added previously.\n",
      "Step 6\u0000 Select Save to save the information or Cancel to cancel saving these\n",
      "parameters.\n",
      "173Stop POC\n",
      "Before learning how to Stop POC for your resources on VKS, you should clearly\n",
      "understand the concepts and actions you can take on POC resources. For more\n",
      "details, refer here .\n",
      "Before your POC wallet for Cluster expires, you have two main options:\n",
      "•Delete this POC Cluster and recreate another normal Cluster.\n",
      "•Perform Stop POC to renew the POC Cluster into a normal Cluster.\n",
      "To continue using the resource that just stopped POC as a normal resource (for\n",
      "the purpose of keeping the configuration intact), the user can do:\n",
      "Step 1\u0000 Access VKS Portal , select the Cluster where you want to Stop POC.\n",
      "Step 2\u0000 Select the Stop POC button on the top right corner of the screen.\n",
      "Step 3\u0000 At this time, the screen displays a list of all Servers and Volumes in the\n",
      "Cluster ( including the Boot Volume and PVC that you attached to the node in your\n",
      "Cluster) that have POC status. You can check the information then select Stop POC\n",
      "174Step 4 : Proceed to pay for resources with real money, you can select the desired\n",
      "usage cycle, turn on and off Auto-renew, enter Coupon if available and select \n",
      "Continue to make Resource Payment\n",
      "Step 5 : Make payment using credit balance or other forms of payment if available.\n",
      "175To ensure VKS works correctly, Stop POC implementation needs to be performed\n",
      "on VKS Portal instead of doing it individually on each resource server or volume\n",
      "on vServer Portal. If you have previously performed an individual POC stop for\n",
      "each resource on vServer Portal, you still need to perform a POC Stop for the\n",
      "Cluster at VKS Portal. At this time, the screen will display as follows. Please press\n",
      "Stop to turn off the POC option for your Cluster.\n",
      "Attention:\n",
      "•After stopping POC on VKS, the \"Stop POC\" button will continue to\n",
      "display if there are still resources that have not been Stop POC after\n",
      "doing so on VKS Portal. You can continue to select and execute Stop\n",
      "POC until all resources are converted to real resources.\n",
      "176•Currently, VKS only applies payment by POC for Server and Volume\n",
      "and has not yet applied payment for Load Balancer and Snapshot by\n",
      "POC. Therefore, you do not need to perform Stop POC for these two\n",
      "resource types Load Balancer and Snapshot.c\n",
      "177Node Groups\n",
      "Node Group is an important concept in Kubernetes, used to manage groups of \n",
      "nodes \u0000VMs\u0000 with the same configuration in a cluster. For a Node Group, you can:\n",
      "To initialize a Node Group, follow the steps below:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the previously created Cluster, select Create a Node group.\n",
      "Step 3\u0000 At the Node Group initialization screen, we have set up information for your\n",
      "Node Group. You can keep these default values   or adjust the desired parameters\n",
      "for your Node Group at:\n",
      "•Node Group Information:\n",
      "◦Node Group Name : A memorable name for your Node Group.\n",
      "◦Number of nodes: Enter the number of Worker nodes for your Cluster, note\n",
      "that the number of nodes needs to be greater than or equal to 1 and less\n",
      "than or equal to 100.\n",
      "•Node Group Automation Setting:\n",
      "◦Auto Healing: By default we will enable the HA feature in your Cluster.\n",
      "When a node or pod fails, Kubernetes will automatically restart or create a\n",
      "new pod to replace it, ensuring your application always operates without\n",
      "interruption.\n",
      "◦Auto Scaling: Enable auto-scaling in your Cluster. Auto scaling helps\n",
      "automatically adjust the number of pods (application deployment units)\n",
      "based on actual usage needs, avoiding wasting resources when demand is\n",
      "low or overloading when demand is high.\n",
      "▪Minimum node : minimum number of nodes that the Cluster needs to\n",
      "have.Create a Node Group\n",
      "178▪Maximum node : maximum number of nodes that the Cluster can scale\n",
      "to.\n",
      "◦Node Group upgrade strategy: Node Group upgrade strategy. When you set\n",
      "up a Node Group Upgrade Strategy via the Surge upgrade method for a\n",
      "Node Group in VKS, the VKS system will update sequentially to upgrade the\n",
      "nodes, in an unspecified order .\n",
      "▪Max surge: limits the number of nodes that can be upgraded\n",
      "simultaneously (the number of new nodes (surge) that can be created at\n",
      "the same time). Default Max surge = 1 - upgrade only one node at a\n",
      "time. with maxUnavailable\n",
      "▪Max unavailable : limits the number of nodes that cannot be reached\n",
      "during the upgrade (the number of existing nodes that can be offline at\n",
      "the same time). Default Max unavailable = 0 - ensures all nodes are\n",
      "accessible during the upgrade.\n",
      "•Node Group Setting:\n",
      "◦Image : By default we provide one type of Image, Ubuntu with containerd.\n",
      "◦Instance type : select the appropriate configuration instance type for the\n",
      "Worker node according to your usage needs.\n",
      "•Node Group Volume Setting: Boot Volume Configuration – Parameters are set\n",
      "by default by the system to help optimize your Cluster\n",
      "•Node Group Network Setting: You can choose Public Node Group or Private\n",
      "Node Group depending on your Cluster usage needs.\n",
      "•Node Group Security Setting: You can choose Security Group and SSH Key for\n",
      "your Node Group.\n",
      "•Node Group Metadata Setting: You can enter the corresponding Metadata for\n",
      "the Node Group.\n",
      "Step 5\u0000 Select Create Node Group. Please wait a few minutes for us to initialize\n",
      "your Node Group. The status of the Node Group is currently Creating .\n",
      "Step 6\u0000 When the Node Group status is Active , you can view Node Group\n",
      "information by selecting Node Group Name on the main screen.\n",
      "179For Node Group, you can edit the parameters: Number of Nodes, Auto Scaling,\n",
      "Upgrade Strategy, Security Group in each separate edit . Specifically, you can\n",
      "follow these steps:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 In the previously created Cluster, select the Cluster you want to edit the\n",
      "Node group.\n",
      "Step 3\u0000 On the screen containing the list of existing Node Groups, in the Node\n",
      "Group you want to edit, select one of the options:\n",
      "•Resize feature : you can change\n",
      "◦Number of nodes: Enter the number of Worker nodes for your Cluster, note\n",
      "that the number of nodes needs to be greater than or equal to 1 and less\n",
      "than or equal to 100.\n",
      "•Edit Auto Scaling feature : you can change\n",
      "◦Auto Scaling: Enable auto-scaling in your Cluster. Auto scaling helps\n",
      "automatically adjust the number of pods (application deployment units)\n",
      "based on actual usage needs, avoiding wasting resources when demand is\n",
      "low or overloading when demand is high.\n",
      "▪Minimum node: minimum number of nodes that the Cluster needs to\n",
      "have.\n",
      "▪Maximum node: maximum number of nodes that the Cluster can scale\n",
      "to.\n",
      "•Edit Upgrade Strategy feature : you can change\n",
      "◦Node Group upgrade strategy: Node Group upgrade strategy. When you set\n",
      "up a Node Group Upgrade Strategy via the Surge upgrade method for a\n",
      "Node Group in VKS, the VKS system will update sequentially to upgrade the\n",
      "nodes, in an unspecified order .\n",
      "▪Max surge: limits the number of nodes that can be upgraded\n",
      "simultaneously (the number of new nodes (surge) that can be created atEdit a Node Group\n",
      "180the same time). Default Max surge = 1 - upgrade only one node at a\n",
      "time. with maxUnavailable\n",
      "▪Max unavailable: limits the number of nodes that cannot be reached\n",
      "during the upgrade (the number of existing nodes that can be offline at\n",
      "the same time). Default Max unavailable = 0 - ensures all nodes are\n",
      "accessible during the upgrade.\n",
      "•Edit Security Group feature : you can change\n",
      "◦Node Group Security Setting: You can choose Security Group and SSH Key\n",
      "for your Node Group.\n",
      "Attention:\n",
      "When you no longer need to use the Node Group, delete them to save\n",
      "costs. When deleting a Node Group, the following resources will be\n",
      "deleted:\n",
      "•All nodes included in the Node Group \u0000VM\u0000\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu.\n",
      "Step 3\u0000 In the successfully created Cluster , select the Node Group you want to\n",
      "delete and select Delete.\n",
      "Step 4\u0000 Select Delete to completely delete your Node Group .\n",
      "Delete a Node Group\n",
      "181Auto Healing\n",
      "On the VKS system, the Auto Healing feature is applied to each Node Group and is\n",
      "always on. Enabling self-healing in Kubernetes provides many important benefits,\n",
      "helping to ensure high availability and reliability for your applications.\n",
      "The Auto Healing feature has the following highlights:\n",
      "•Automatic error detection: Kubernetes can automatically detect failed or\n",
      "problematic nodes through monitoring the status of the nodes. Some signs that\n",
      "a node is failing include: node reporting \"NotReady\" status, node unable to be\n",
      "pinged, node experiencing hardware failure, etc.\n",
      "•Automatically restart the node: When a node detects an error, Kubernetes will\n",
      "automatically restart the node. Restarting the node can help fix temporary\n",
      "errors and return the node to normal operation.\n",
      "•Minimize manual intervention: Auto Healing helps minimize manual\n",
      "intervention by system administrators, saving time and effort.\n",
      "•Improved performance: Auto Healing helps improve system performance by\n",
      "ensuring that nodes always operate normally.\n",
      "•The Node reports the NotReady status on consecutive checks at 10 minute\n",
      "intervals .Overview\n",
      "Mechanism of action\n",
      "Auto Healing mechanism: VKS system activates auto\n",
      "healing when\n",
      "182If the above conditions are met, the system will immediately perform auto healing.\n",
      "This process is performed in 2 steps:\n",
      "•Step 1\u0000 The VKS system drains the node, which means moving all pods running\n",
      "on this NotReady node to other nodes in the node group before removing that\n",
      "node from the node group.\n",
      "•Step 2\u0000 The system will recreate a new node with the configuration set up on\n",
      "the node group and join this node to the cluster. If after rebooting, the node still\n",
      "reports a \"NotReady\" status, the system will continue to reboot the node until\n",
      "the node returns to its normal operating state.\n",
      "Attention:\n",
      "•When the system performs Auto Healing, creating a new node may\n",
      "encounter an error if you do not have enough credits or you have run\n",
      "out of quota to create a VM on the vServer system. At this time, every\n",
      "30 minutes the system will restart the node until the node returns to\n",
      "normal operating status. To avoid the error above, you need to:\n",
      "◦Make sure you have enough credits: If you're a prepaid user, add\n",
      "more credits to your account.\n",
      "◦Request a quota increase: You can request a quota increase for\n",
      "your account here .\n",
      "Currently, the Auto Healing feature is applied to each Node Group and is always on\n",
      ". You do not need to manually enable it when initializing the Cluster or Node Group.\n",
      "Turn on Auto Healing\n",
      "183Auto Scaling\n",
      "Auto Scaling for Cluster is a feature in Kubernetes that allows automatically\n",
      "adjusting the size of the cluster, specifically the number of nodes in the cluster to\n",
      "meet usage needs.\n",
      "The Auto Scaling feature has the following highlights:\n",
      "1.Optimize performance: Auto Scaling allows the cluster to automatically scale\n",
      "resources as needed. When workloads are higher, the cluster automatically\n",
      "creates additional nodes to ensure applications operate at their best\n",
      "performance.\n",
      "2.Cost savings: Auto Scaling allows the cluster to automatically reduce resources\n",
      "when not needed. If workload decreases, the cluster automatically reclaims\n",
      "unused resources to save costs.\n",
      "3.Ensure availability: Auto Scaling helps ensure that the cluster is available to\n",
      "meet demand and avoid resource overload or shortages.\n",
      "4.Auto Scaling: Auto Scaling helps automatically recover from crashes or errors\n",
      "by creating new nodes to replace failed nodes.\n",
      "When deploying applications in a cloud environment, using Auto Scaling helps\n",
      "optimize resource usage, improve application availability and performance, and\n",
      "makes cluster management easy and convenient. more effective.Overview\n",
      "Mechanism of action\n",
      "Scale up mechanism: the Procuracy system performs\n",
      "scale up when\n",
      "184•Pods cannot be scheduled on any existing node due to lack of resources.\n",
      "•Adding 1 more node similar to the current node group configuration is useful\n",
      "and can handle this resource shortage problem.\n",
      "Illustration:\n",
      "If the above two conditions are met, the system will increase the number of nodes\n",
      "(one or more nodes) to accommodate all unscheduling pods. This process will be\n",
      "done immediately in 2 steps:\n",
      "•Step 1\u0000 The VKS system creates a new node according to the current node\n",
      "group configuration.\n",
      "•Step 2\u0000 The VKS system will deploy these unscheduling pods to new nodes.\n",
      "185Attention:\n",
      "•When the system performs Auto Scaling, creating a new node may\n",
      "encounter an error if you do not have enough credits or you have run\n",
      "out of quota to create a VM on the vServer system. To avoid the error\n",
      "above, you need to:\n",
      "◦Make sure you have enough credits: If you're a prepaid user, add\n",
      "more credits to your account.\n",
      "◦Request a quota increase: You can request a quota increase for\n",
      "your account here .\n",
      "•One or more nodes have continuously low load over a period of time.\n",
      "Specifically, the node has low utilization (availability) including CPU and\n",
      "memory requests of the pod at < 50%.\n",
      "•All existing pods of that node, can be moved to another node without any\n",
      "problem.\n",
      "If the above two conditions are met, by default within about 10 minutes, that node\n",
      "will be deleted from the Cluster. This deletion process will include 3 steps:\n",
      "•Step 1\u0000 The VKS system will mark that node as unschedulable.\n",
      "•Step 2\u0000 The system moves the entire pod to another node.\n",
      "•Step 3\u0000 After successfully moving all pods to another node, the VKS system will\n",
      "delete the marked node.\n",
      "On the VKS system, you can turn on Auto Scaling when:\n",
      "Scale down mechanism: the Procuracy system\n",
      "performs scale down when\n",
      "Turn on Auto Scaling\n",
      "186•Initialize a Cluster\n",
      "•Create a Node Group\n",
      "•Edit a Node Group\n",
      "To enable Auto Scaling for your Kubernetes Cluster please enable the Enable Auto\n",
      "Scaling option. When enabling this option you need to enter:.\n",
      "•Minimum node : minimum number of nodes that the Cluster must have.\n",
      "•Maximum node : maximum number of nodes that the Cluster can scale to.\n",
      "•Node Group upgrade strategy : Node Group upgrade strategy. When you set\n",
      "up a Node Group Upgrade Strategy via the Surge upgrade method for a Node\n",
      "Group in VKS, the VKS system will update sequentially to upgrade the nodes, in\n",
      "an unspecified order .\n",
      "◦Max surge: limits the number of nodes that can be upgraded simultaneously\n",
      "(the number of new nodes (surge) that can be created at the same time).\n",
      "Default Max surge = 1 - upgrade only one node at a time.\n",
      "◦Max unavailable : limits the number of nodes that cannot be reached during\n",
      "the upgrade (the number of existing nodes that can be offline at the same\n",
      "time). Default Max unavailable = 0 - ensures all nodes are accessible during\n",
      "the upgrade.\n",
      "For example, as shown below: I have initialized a Node Group with:\n",
      "•Number of nodes: 3 nodes\n",
      "•Minimum nodes: 1 nodes\n",
      "•Maximum nodes: 5 nodes\n",
      "At this time:\n",
      "•If one or more pods cannot be scheduled on any node on the cluster due to lack\n",
      "of resources and adding a node similar to this node group configuration can\n",
      "solve the problem, the system will perform scale up. With this setup, the system\n",
      "can scale up to a maximum of 5 nodes .\n",
      "•If there is a node with low utilization (availability) at < 50% and all pods of that\n",
      "node can be scheduled on another node, the system will perform scale up. With\n",
      "this setup, the system can scale down to a minimum of 1 node.\n",
      "187Upgrading Node Group Version\n",
      "Currently, our VKS system has supported you to upgrade Node Group Version, you\n",
      "can upgrade Node Group Version to:\n",
      "•Control Plane Version \u0000For example upgrade from 1.24 (current Node Group\n",
      "version) to 1.25 (current Control Plane Version), but cannot upgrade to other\n",
      "versions.\n",
      "To perform a Node Group Version upgrade, you can follow these instructions:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the Overview screen , select the Kubernetes Cluster menu. Select a \n",
      "Cluster where you want to upgrade Node Group Version .\n",
      "Step 3\u0000 Select iconand select Upgrade Node Group version to upgrade the node\n",
      "group version.\n",
      "Step 4\u0000 You can select the new version for all Node Groups. The new version needs\n",
      "to be valid and compatible with the current version of the cluster. Specifically: you\n",
      "can choose:\n",
      "•Upgrade Node Group to the same version as Control Plane Version (for\n",
      "example: 1.24 to 1.25\u0000\n",
      "Step 5\u0000 The VKS system will upgrade all Node Groups to the Control Plane version.\n",
      "After the upgrade is complete, the Node Group status returns to ACTIVE .\n",
      "Attention:\n",
      "•Upgrading Node Group Version is optional and independent of\n",
      "upgrading Control Plane Version. However, all Node Groups in a\n",
      "Cluster will be upgraded at the same time, as well as Control Plane\n",
      "Version and Node Group Version in the same Cluster cannot differ by\n",
      "more than 1 minor version. Besides, the VKS system automatically\n",
      "188upgrades the Node Group Version when the current K8S Version being\n",
      "used for your Cluster exceeds the supplier's support period.\n",
      "•During the Node Group Version upgrade, you cannot perform other\n",
      "actions on your Node Group.\n",
      "•Below are a few notes before, during and after the upgrade process,\n",
      "please refer to:\n",
      "Before getting into work:\n",
      "•Check the current version: Visit Releases for a list of supported\n",
      "versions. Select a new version that is valid and compatible with the\n",
      "current version of the cluster.\n",
      "•Ensure Node Group availability: Node Group must be in active state\n",
      "\u0000ACTIVE) and all nodes must be HEALTHY.\n",
      "•Stop running tasks: Stop running tasks on the cluster to avoid affecting\n",
      "the upgrade process.\n",
      "While doing:\n",
      "•Monitor Node Group status: Monitor Node Group status during the\n",
      "upgrade process. The Node Group status will change to UPDATING\n",
      "and after completion will return to ACTIVE.\n",
      "•Check system log: Check the system log for any errors or warnings\n",
      "during the upgrade process.\n",
      "After implementation:\n",
      "•Check Node Group Availability: Confirm that the Node Group has been\n",
      "upgraded successfully and all nodes are operating normally.\n",
      "•Test applications: Test applications running on the cluster to ensure\n",
      "they work properly after the upgrade.\n",
      "Note:\n",
      "•Upgrading Node Group Version may take some time depending on the\n",
      "size and complexity of the Node Group.\n",
      "•In some rare cases, upgrading Node Group Version may fail. If this\n",
      "happens, the VKS system will automatically rollback the cluster to the\n",
      "current version.\n",
      "189Lable and Taint\n",
      "Labels are an important feature in Kubernetes, used to organize and manage\n",
      "objects effectively. You can assign key-value pairs to Kubernetes objects such as\n",
      "Pod, Node, Service, Deployment, etc. Specifically:\n",
      "•Each Lable is a key-value pair: Key is a string of characters used to identify the\n",
      "name of the label. Value is an optional character string that provides detailed\n",
      "information about the label.\n",
      "•Keys and values   must follow the naming rules: Keys and values   must not\n",
      "contain spaces or special characters other than (-, _,.).\n",
      "•Lable can be used for a variety of purposes, including:\n",
      "◦Classify objects based on criteria such as environment, version, status, etc\n",
      "◦Monitor and manage objects in a Kubernetes cluster.\n",
      "For example:\n",
      "•app: nginx- This label indicates the object is related to the Nginx application.\n",
      "•environment: production- This label indicates that the object belongs to the\n",
      "production environment.\n",
      "•version: 1.7.2- This label indicates the object is related to version 1.7.2.\n",
      "To create a Lable for a Node Group, follow these instructions:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the previously created Cluster, select Create a Node group.Lable\n",
      "Create Label\n",
      "190Step 3\u0000 At the Node Group initialization screen, we have set up information for your\n",
      "Node Group. You can keep these default values   or adjust the desired parameters\n",
      "for your Node Group. In the Node Group Metadata Setting section, you need:\n",
      "•Enter the key for your label. The key must begin and end with letters or numbers\n",
      "and include the characters az, AZ, 0\u00009, -, _, . Maximum 253 characters.\n",
      "Alternatively, you can enter the key as a DNS subdomain, for example: \n",
      "example.com/my-app\n",
      "•Enter the value for this corresponding key.\n",
      "Step 5\u0000 Select Create Node Group. Please wait a few minutes for us to initialize\n",
      "your Node Group. The status of the Node Group is currently Creating .\n",
      "Step 6\u0000 When the Node Group status is Active , you can view Node Group\n",
      "information by selecting Node Group Name on the main screen.\n",
      "Or you can create Lable through kubectl with the command:\n",
      "You can check the newly created label again with the command:\n",
      "For example the result for this command would be as follows:\n",
      "nodeSelector is a parameter used in PodSpec to specify that Pods should only be\n",
      "scheduled on Nodes with a specific label. This is useful when you want to run Pods\n",
      "on Nodes with specific resources or properties.kubectl label nodes my-node1 disktype=ssd\n",
      "kubectl get nodes --show-labels\n",
      "NAME      STATUS    ROLES    AGE     VERSION        LABELS\n",
      "worker0   Ready     <none>   1d      v1.13.0        ...,disktype=ssd,kube\n",
      "worker1   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hos\n",
      "worker2   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hos\n",
      "Use Lable with nodeSelector\n",
      "191•Create a my-pod.yaml file containing the following content:\n",
      "In this example, the Pod my-podis scheduled only on Nodes with label \n",
      "disktype: ssdand region: hcm03.\n",
      "•Deploy Pod on your Cluster:\n",
      "Taint is an important feature in Kubernetes, serving as a mechanism to tag Nodes\n",
      "and control Pod scheduling on those Nodes. Different from regular Label, Taint is\n",
      "used to specify special properties of Node and execute specific actions when Pod\n",
      "does not meet the conditions defined by Taint. Specifically:\n",
      "Specifically:\n",
      "•Each Taint includes:\n",
      "◦Key is a string of characters used to identify the name of the taint.\n",
      "◦Value is an optional character string that provides detailed information about\n",
      "the taint.\n",
      "◦Effect:\n",
      "▪NoSchedule: Prevent Pods from having a corresponding Toleration\n",
      "scheduled on the Node.apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: my-pod\n",
      "spec:\n",
      "  nodeSelector:\n",
      "    disktype: ssd\n",
      "    region: hcm03\n",
      "kubectl -f apply my-pod.yaml\n",
      "Taint\n",
      "192▪NoExecute: Allows the Pod to be scheduled on the Node but the Pod\n",
      "will not be executed.\n",
      "▪PreferNoSchedule: Kubernetes will try to prioritize not scheduling the\n",
      "Pod to the Node with this Taint.\n",
      "•Keys and values   must follow the naming rules: Keys and values   must not\n",
      "contain spaces or special characters other than (-, _,.).\n",
      "•Toleration: In order for a Pod to be scheduled and run on a Node with Taint, the\n",
      "Pod needs to have a corresponding Toleration. Toleration is declared in\n",
      "PodSpec using tolerationsfield. For example:\n",
      "•Relationship between Taint and Toleration: When Kubernetes schedules a\n",
      "Pod, Kubernetes matches the Node's Taints with the Pod's Tolerations. Pods\n",
      "are only scheduled on a Node if there is Toleration for all Taints of that Node.\n",
      "For example:\n",
      "•node.role.kubernetes.io/master:NoSchedule - prevents regular Pods from being\n",
      "run on this Node.\n",
      "To create a Taint for a Node Group, follow these instructions:\n",
      "Step 1\u0000 Visit https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 At the previously created Cluster, select Create a Node group.\n",
      "Step 3\u0000 At the Node Group initialization screen, we have set up information for your\n",
      "Node Group. You can keep these default values   or adjust the desired parameters\n",
      "for your Node Group. In the Node Group Metadata Setting section, you need:\n",
      "•Enter the key for your taint. The key must begin and end with letters or numbers\n",
      "and include the characters az, AZ, 0\u00009, -, _, . Maximum 253 characters.tolerations: - key: node.role.kubernetes.io/master effect: NoSchedule\n",
      "Create Taints\n",
      "193Alternatively, you can enter the key as a DNS subdomain, for example: \n",
      "example.com/my-app\n",
      "•Enter the value for this corresponding key.\n",
      "•Choose 1 of 3 effect types: NoSchedule, NoExecute, PreferNoSchedule.\n",
      "Step 5\u0000 Select Create Node Group. Please wait a few minutes for us to initialize\n",
      "your Node Group. The status of the Node Group is currently Creating .\n",
      "Step 6\u0000 When the Node Group status is Active , you can view Node Group\n",
      "information by selecting Node Group Name on the main screen.\n",
      "Or you can create Taint through kubectl with the command:\n",
      "Taint usage example:\n",
      "Suppose you have a Node masterused for management purposes and you want to\n",
      "prevent regular Pods from being run on this Node. You can use Taint as follows:\n",
      "In order for Pod to run on Node master, the Pod needs to have the corresponding\n",
      "Toleration:kubectl taint node my-node node.role.kubernetes.io/master:NoSchedule.\n",
      "kubectl taint node my-master node.role.kubernetes.io/master:NoSchedule\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: my-pod\n",
      "spec:\n",
      "  tolerations:\n",
      "  - key: node.role.kubernetes.io/master\n",
      "    effect: NoSchedule\n",
      "194Network\n",
      "195Working with Application\n",
      "Load Balancer (ALB)\n",
      "•Application Load Balancer \u0000ALB\u0000 is a tool in network and server infrastructure\n",
      "used to distribute network traffic to multiple servers or virtual machines to\n",
      "improve the performance and availability of applications. ALB operates at the\n",
      "application layer, allowing traffic distribution based on many factors such as\n",
      "request type, server state, and load distribution algorithm. ALB provides\n",
      "advanced routing capabilities, allowing traffic to be directed based on Host or\n",
      "Path Header. It also supports session persistence, which helps maintain user\n",
      "sessions to the same server. This is useful for applications that require\n",
      "consistency in user interactions. For more information about ALB, please refer\n",
      "to \u0000How it works \u0000ALB\u0000\u0000\n",
      "Model:Overview\n",
      "What is ALB?\n",
      "196In addition to the basic components of a K8S cluster and an ALB that you already\n",
      "know, in this model we use: \n",
      "•Ingress: is a resource in Kubernetes that is configured to make Services\n",
      "accessible from outside the k8s cluster via URL, and can also load balance\n",
      "traffic, support SSL/TLS connections and provide virtual hosting based on\n",
      "names. An Ingress does not arbitrarily expose protocols other than HTTP and\n",
      "HTTPS. Ingress acts as a single entry point for HTTP and HTTPS requests from\n",
      "outside the cluster to internal services. Traffic routing is controlled by rules\n",
      "defined in the Ingress resource \u0000Ingress Yaml File). An Ingress is managed by\n",
      "VNGCloud Ingress Controller: is an application that runs in the cluster and\n",
      "manages Ingress resources based on the Ingress Yaml File defined by the\n",
      "customer\n",
      "197Ingress for an Application\n",
      "Load Balancer\n",
      "In order for the Ingress resource \u0000Ingress Yaml file) to work, the cluster must have a\n",
      "running VNGCloud Ingress Controller. Unlike other Controller types that run as part\n",
      "of kube-controller-manager . VNGCloud Ingress Controller is not automatically\n",
      "started with the cluster. Please follow the instructions below to install VNGCloud\n",
      "Ingress Controller as well as work with Ingress Yaml files.\n",
      "•Create a Kubernetes cluster on VNGCloud, or use an existing cluster. Note:\n",
      "make sure you have downloaded the cluster configuration file once the cluster\n",
      "has been successfully initialized and accessed your cluster.\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vLBFullAccess , vServerFullAccess . To create a service account, go here and\n",
      "follow these steps:\n",
      "◦Select \" Create a Service Account \", enter a name for the Service Account\n",
      "and click Next Step to assign permissions to the Service Account\n",
      "◦Find and select Policy: vLBFullAccess and Policy: vServerFullAccess ,\n",
      "then click \" Create a Service Account \" to create Service Account, Policy:\n",
      "vLBFullAccess and Policy: vServerFullAccess created by VNG Cloud, you\n",
      "cannot delete these policies.\n",
      "◦After successful creation, you need to save the Client_ID and Secret_Key of\n",
      "the Service Account to perform the next step.\n",
      "•Change the Security Group information to allow ALBs to connect to Nodes in\n",
      "your Node Group. You need to change them on vServer Portal when:\n",
      "◦The Security Group attached to your Cluster/Node Group is different from\n",
      "the default parameters we created.\n",
      "◦You need to change the security level for your Cluster or you need to open\n",
      "more ports for specific services to operate on the Cluster. Details\n",
      "information here .Prepare\n",
      "198\n",
      "Attention:\n",
      "When you initialize the Cluster according to the instructions above, if you\n",
      "have not enabled the Enable vLB Native Integration Driver option , by\n",
      "default we will not pre-install this plugin into your Cluster. You need to\n",
      "manually create Service Account and install VNGCloud Ingress Controller\n",
      "according to the instructions below. If you have enabled the Enable vLB\n",
      "Native Integration Driver option , then we have pre-installed this plugin\n",
      "into your Cluster, skip the Service Account Initialization step, install\n",
      "VNGCloud Ingress Controller and continue following the instructions from\n",
      "Deploy once. Workload.\n",
      "Initialize Service Account\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vLBFullAccess , vServerFullAccess . To create a service account,\n",
      "go here and follow these steps:\n",
      "◦Select \" Create a Service Account \", enter a name for the\n",
      "Service Account and click Next Step to assign permissions to the\n",
      "Service Account\n",
      "◦Find and select Policy: vLBFullAccess and Policy:\n",
      "vServerFullAccess , then click \" Create a Service Account \" to\n",
      "create Service Account, Policy: vLBFullAccess and Policy:\n",
      "vServerFullAccess created by VNG Cloud, you cannot delete\n",
      "these policies.\n",
      "◦After successful creation, you need to save the Client_ID and \n",
      "Secret_Key of the Service Account to perform the next step.\n",
      "Install VNGCloud Ingress Controller\n",
      "Create Service Account and install VNGCloud Ingress Controller\n",
      "Create Service Account and install\n",
      "VNGCloud Ingress Controller\n",
      "199\n",
      "•Install Helm version 3.0 or higher. Refer to \n",
      "https://helm.sh/docs/intro/install/ for instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Replace your K8S cluster's ClientID, Client Secret, and ClusterID\n",
      "information and continue running:\n",
      "•After the installation is complete, check the status of vngcloud-\n",
      "ingress-controller pods:\n",
      "For example, in the image below you have successfully installed\n",
      "vngcloud-controller-manager:\n",
      "The following is a guide for you to deploy the nginx service on Kubernetes.\n",
      "•Create nginx-service-lb7.yaml file with the following content:helm repo add vks-helm-charts https://vngcloud.github.io/vks-hel\n",
      "helm repo update\n",
      "helm install vngcloud-ingress-controller vks-helm-charts/vngclou\n",
      "  --namespace kube-system \\\n",
      "  --set cloudConfig.global.clientID= <L ấ y ClientID c ủ a Service A\n",
      "  --set cloudConfig.global.clientSecret= <L ấ y ClientSecret c ủ a S\n",
      "  --set cluster.clusterID= <L ấ y Cluster ID c ủ a cluster mà b ạ n đ ã\n",
      "kubectl get pods -n kube-system | grep vngcloud-ingress-controll\n",
      "NAME                                      READY   STATUS    REST\n",
      "vngcloud-ingress-controller-0             1/1     Running   0   \n",
      "Deploy a Workload\n",
      "Step 1 : Create Deployment for Nginx app.\n",
      "200•Deploy This deployment equals:\n",
      "•Run the following command to test DeploymentapiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx \n",
      "  type: NodePort \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80\n",
      "kubectl apply -f nginx-service-lb7.yaml\n",
      "Step 2: Check the Deployment and Service information\n",
      "just deployed\n",
      "201•If the results are returned as below, it means you have deployed Deployment\n",
      "successfully.\n",
      "1.If you do not have an Application Load Balancer previously created on the vLB\n",
      "system.\n",
      "Now, when creating an Ingress, leave the Load Balancer ID information blank at the \n",
      "vks.vngcloud.vn/load-balancer-id annotation .\n",
      "•For example, suppose you have deployed a service named nginx-service. At\n",
      "this point, you can create the nginx-ingress.yaml file as follows:kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)  \n",
      "service/kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP  \n",
      "service/nginx-service   NodePort    10.96.25.133   <none>        80:32572\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTA\n",
      "deployment.apps/nginx-app   1/1     1            1           2m50s   ngin\n",
      "NAME                             READY   STATUS    RESTARTS   AGE     IP  \n",
      "pod/nginx-app-7f45b65946-6wlgw   1/1     Running   0          2m49s   172\n",
      "Step 3: Create Ingress Resource\n",
      "202•Run the following command to deploy Ingress\n",
      "Once you have deployed Ingress, we will automatically create an ALB on your\n",
      "cluster. This ALB will be displayed on vLB Portal, details can be accessed here .\n",
      "This ALB will have default information:apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: nginx-ingress\n",
      "spec:\n",
      "  ingressClassName: \"vngcloud\"\n",
      "  defaultBackend:\n",
      "    service:\n",
      "      name: nginx-service\n",
      "      port:\n",
      "        number: 80\n",
      "  rules:\n",
      "    - http:\n",
      "        paths:\n",
      "          - path: /4\n",
      "            pathType: Exact\n",
      "            backend:\n",
      "              service:\n",
      "                name: nginx-service\n",
      "                port:\n",
      "                  number: 80               \n",
      "kubectl apply -f nginx-ingress.yaml\n",
      "Ingredient Quantity Properties\n",
      "ALB Package first VNG ALB_Small\n",
      "Listener 2 •1 listener with HTTP\n",
      "protocol and port 80\n",
      "•1 listener with HTTPS\n",
      "protocol and port 443\n",
      "Pool first •1 pool default HTTP\n",
      "protocol and ROUND\n",
      "ROBIN algorithm\n",
      "203For example:\n",
      "Attention:\n",
      "•Currently Ingress only supports TLS port 443 and is the termination\n",
      "point for TLS \u0000TLS termination). TLS Secret must contain fields with\n",
      "key names tls.crt and tls.key, which are the certificate and private key\n",
      "to use for TLS. If you want to use a Certificate for a host, please\n",
      "upload the Certificate according to the instructions at \u0000Upload a\n",
      "certificate] and use them as an annotation. For example:Health Check first •Use TCP to check\n",
      "health of members.\n",
      "2042.If you already have a previously initialized Application Load Balancer on the\n",
      "vLB system and you want to reuse the ALB for your cluster.\n",
      "Now, when creating an Ingress, enter the Load Balancer ID information into the \n",
      "vks.vngcloud.vn/load-balancer-id annotation. For example, in this case I reused\n",
      "the ALB with ID = lb-2b9d8974\u00003760\u00004d60\u00008203\u00009671f229fb96\u0000apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: example-ingress\n",
      "  annotations:\n",
      "    # kubernetes.io/ingress.class: \"vngcloud\" # this annotation is deprec\n",
      "    vks.vngcloud.vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484\n",
      "spec:\n",
      "  ingressClassName: \"vngcloud\"\n",
      "  defaultBackend:\n",
      "    service:\n",
      "      name: apache-service\n",
      "      port:\n",
      "        number: 80\n",
      "  tls:\n",
      "    - hosts:\n",
      "        - host.example.com\n",
      "  rules:\n",
      "    - host: host.example.com\n",
      "      http:\n",
      "        paths:\n",
      "          - path: /4\n",
      "            pathType: Exact\n",
      "            backend:\n",
      "              service:\n",
      "                name: nginx-service\n",
      "                port:\n",
      "                  number: 80\n",
      "205•After you have created ingress according to the instructions at Ingress for an\n",
      "Application Load Balancer . If:\n",
      "◦Your ALB currently has 2 listeners in it:\n",
      "▪1 listener has HTTP protocol configuration and port 80\n",
      "▪If a listener has HTTPS protocol configuration and port 443, we will use\n",
      "these 2 listeners.\n",
      "◦Your ALB does not have either or both listeners with the above\n",
      "configuration, we will automatically create them.\n",
      "Attention:\n",
      "If your ALB has:apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: example-ingress\n",
      "  annotations:\n",
      "    # kubernetes.io/ingress.class: \"vngcloud\" # this annotation is deprec\n",
      "    vks.vngcloud.vn/load-balancer-id: \"lb-2b9d8974-3760-4d60-8203-9671f22\n",
      "    vks.vngcloud.vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484\n",
      "spec:\n",
      "  ingressClassName: \"vngcloud\"\n",
      "  defaultBackend:\n",
      "    service:\n",
      "      name: apache-service\n",
      "      port:\n",
      "        number: 80\n",
      "  tls:\n",
      "    - hosts:\n",
      "        - host.example.com\n",
      "  rules:\n",
      "    - host: host.example.com\n",
      "      http:\n",
      "        paths:\n",
      "          - path: /4\n",
      "            pathType: Exact\n",
      "            backend:\n",
      "              service:\n",
      "                name: nginx-service\n",
      "                port:\n",
      "                  number: 80\n",
      "206•1 listener has HTTP protocol configuration and port 443\n",
      "•Or a listener configured with HTTPS protocol and portal 80\n",
      "then when creating Ingress an error will occur. At this point, you need to\n",
      "edit valid listener information on the vLB system and recreate ingress.\n",
      "3. After successfully creating ingress with an ALB , you are good to go\n",
      "•Edit your ingress configuration according to the specific instructions at \n",
      "Configure for an Application Load Balancer .\n",
      "•Or you can add/edit/delete policies in your ALB by editing the following\n",
      "parameters in the ingress resource \u0000Ingress Yaml file). For example, below, I\n",
      "have set up 2 rules as follows:\n",
      "207•Like other Kubernetes resources, Ingress has a structure including the following\n",
      "information fields:\n",
      "◦apiVersion: API version for Ingress.\n",
      "◦kind: Resource type, in this case \"Ingress\".\n",
      "◦ingressClassName : you need to specify this field value as \"vngcloud\" to\n",
      "use vngcloud-ingress-controller.apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: nginx-ingress\n",
      "  annotations:\n",
      "    vks.vngcloud.vn/certificate-ids: \"secret-58542bfb-f410-4095-9e1c-34cd\n",
      "spec:\n",
      "  ingressClassName: \"vngcloud\"\n",
      "  defaultBackend:\n",
      "    service:\n",
      "      name: nginx-service\n",
      "      port:\n",
      "        number: 80\n",
      "  tls:\n",
      "    - hosts:\n",
      "       - '*.example.com'\n",
      "       - 'example.com'\n",
      "  rules:\n",
      "    - host: example.com\n",
      "      http:\n",
      "        paths:\n",
      "          - path: /1\n",
      "            pathType: Exact\n",
      "            backend:\n",
      "              service:\n",
      "                name: nginx-service1\n",
      "                port:\n",
      "                  number: 80\n",
      "    - http:\n",
      "        paths:\n",
      "          - path: /2\n",
      "            pathType: Exact\n",
      "            backend:\n",
      "              service:\n",
      "                name: nginx-service2\n",
      "                port:\n",
      "                  number: 80\n",
      "208◦metadata: Information describing Ingress, including name, annotations.\n",
      "◦spec: Ingress configuration, including traffic route rules according to the\n",
      "conditions of incoming requests. Ingress resources only support rules to\n",
      "direct HTTP traffic.\n",
      "For general information about working with Ingress resources \u0000Ingress Yaml files),\n",
      "see \u0000Configure for an Application Load Balancer]).\n",
      "•After successfully creating ingress, you can view the ingress list via command\n",
      "For example, below we have successfully created nginx-ingress:\n",
      "•Or view details of an ingress by\n",
      "For example, below are the details of nginx-ingress that I created:kubectl get ingress\n",
      "NAME            CLASS      HOSTS   ADDRESS          PORTS   AGE\n",
      "nginx-ingress   vngcloud   *       180.93.181.129   80      103m\n",
      "kubectl describe ingress nginx-ingressStep 4: Check and edit the created Ingress resource\n",
      "209•To update an existing nginx-ingress, we can do so by updating the Ingress Yaml\n",
      "file as follows:\n",
      "Copy\n",
      "You can get Load Balancer Public Endpoint information at the vLB interface.\n",
      "Specifically, access at\n",
      "For example, below I have successfully accessed the nginx app with the address: \n",
      "http://180.93.181.129/Name:             nginx-ingress\n",
      "Labels:           <none>\n",
      "Namespace:        default\n",
      "Address:          180.93.181.129\n",
      "Ingress Class:    vngcloud\n",
      "Default backend:  nginx-service:80 (172.16.24.202:80)\n",
      "Rules:\n",
      "  Host        Path  Backends\n",
      "  ----        ----  --------\n",
      "  *\n",
      "              /path1   nginx-service:80 (172.16.24.202:80)\n",
      "Annotations:  vks.vngcloud.vn/load-balancer-id: lb-6cdea8fd-4589-410e-933\n",
      "Events:       <none>\n",
      "kubectl edit ingress nginx-ingress\n",
      "http://Endpoint/Step 5: To access the nginx app, you can use the Load\n",
      "Balancer Endpoint that the system has created.\n",
      "210\n",
      "211Configure for an\n",
      "Application Load Balancer\n",
      "On the \u0000Ingress for an Application Load Balancer] page, we have shown you how to\n",
      "install the Ingress Controller and create ingress via the Ingress Yaml file. The\n",
      "following are detailed meanings of the information you can set for an Ingress\n",
      "Use the annotations below when creating ingress to customize the Load Balancer\n",
      "to suit your needs:\n",
      "Annotations Required/Not required Meaning\n",
      "vks.vngcloud.vn/load-\n",
      "balancer-idOptional •If you do not\n",
      "already have\n",
      "a previously\n",
      "initialized\n",
      "Application\n",
      "Load\n",
      "Balancer on\n",
      "the vLB\n",
      "system. Now,\n",
      "when\n",
      "creating an\n",
      "Ingress, leave\n",
      "this\n",
      "information\n",
      "blank. After\n",
      "you have\n",
      "implemented\n",
      "Ingress\n",
      "deployment\n",
      "following the\n",
      "instructions\n",
      "at Ingress for\n",
      "an\n",
      "Application\n",
      "LoadAnnotations\n",
      "212Balancer . We\n",
      "will\n",
      "automatically\n",
      "create an ALB\n",
      "on your\n",
      "cluster. This\n",
      "ALB will be\n",
      "displayed on\n",
      "vLB Portal,\n",
      "details can be\n",
      "accessed \n",
      "here\n",
      "•If you already\n",
      "have a\n",
      "previously\n",
      "initialized\n",
      "Application\n",
      "Load\n",
      "Balancer on\n",
      "the vLB\n",
      "system and\n",
      "you want to\n",
      "reuse the ALB\n",
      "for your\n",
      "cluster. Now,\n",
      "when\n",
      "creating an\n",
      "Ingress, enter\n",
      "the Load\n",
      "Balancer ID\n",
      "information\n",
      "into this\n",
      "annotation.\n",
      "After you\n",
      "have created\n",
      "Ingress\n",
      "according to\n",
      "the\n",
      "instructions\n",
      "at Ingress for\n",
      "an\n",
      "Application\n",
      "Load\n",
      "Balancer . If:\n",
      "213◦Your ALB\n",
      "currently\n",
      "has 2\n",
      "listeners\n",
      "in it:\n",
      "▪1\n",
      "listen\n",
      "er\n",
      "has\n",
      "HTT\n",
      "P\n",
      "proto\n",
      "col\n",
      "confi\n",
      "gurat\n",
      "ion\n",
      "and\n",
      "port\n",
      "80\n",
      "▪If a\n",
      "listen\n",
      "er\n",
      "has\n",
      "HTT\n",
      "PS\n",
      "proto\n",
      "col\n",
      "confi\n",
      "gurat\n",
      "ion\n",
      "and\n",
      "port\n",
      "443,\n",
      "we\n",
      "will\n",
      "use\n",
      "these\n",
      "2\n",
      "listen\n",
      "ers.\n",
      "◦Your ALB\n",
      "does not\n",
      "have\n",
      "either or\n",
      "214both\n",
      "listeners\n",
      "with the\n",
      "above\n",
      "configura\n",
      "tion, we\n",
      "will\n",
      "automati\n",
      "cally\n",
      "create\n",
      "them.\n",
      "Attention:\n",
      "If your ALB has:\n",
      "•1 listener has\n",
      "HTTP\n",
      "protocol\n",
      "configuration\n",
      "and port 443\n",
      "•Or a listener\n",
      "configured\n",
      "with HTTPS\n",
      "protocol and\n",
      "portal 80\n",
      "then when\n",
      "creating Ingress\n",
      "an error will\n",
      "occur. At this\n",
      "point, you need to\n",
      "edit valid listener\n",
      "information on the\n",
      "vLB system and\n",
      "recreate ingress.\n",
      "vks.vngcloud.vn/load-\n",
      "balancer-nameOptional •Annotation \n",
      "vks.vngclou\n",
      "d.vn/load-\n",
      "balancer-\n",
      "name\n",
      "will be used if\n",
      "you do not\n",
      "use\n",
      "annotation \n",
      "215load-\n",
      "balancer-id\n",
      ".\n",
      "•Annotation \n",
      "vks.vngclou\n",
      "d.vn/load-\n",
      "balancer-\n",
      "name\n",
      "only makes\n",
      "sense when\n",
      "you create a\n",
      "new Ingress\n",
      "resource.\n",
      "After the\n",
      "Ingress\n",
      "resource is\n",
      "successfully\n",
      "created, this\n",
      "annotation \n",
      "will be\n",
      "automatically\n",
      "deleted .\n",
      "Using this\n",
      "annotation\n",
      "after the\n",
      "Ingress\n",
      "resource is\n",
      "created will \n",
      "have no\n",
      "effect .\n",
      "•When you\n",
      "use this\n",
      "annotation, if\n",
      "you do not\n",
      "already have\n",
      "a previously\n",
      "initialized\n",
      "Application\n",
      "Load\n",
      "Balancer on\n",
      "the vLB\n",
      "system. We\n",
      "will\n",
      "automatically\n",
      "create an ALB\n",
      "216on your\n",
      "cluster. This\n",
      "ALB will be\n",
      "displayed on\n",
      "vLB Portal,\n",
      "details can be\n",
      "accessed \n",
      "here\n",
      "•If you already\n",
      "have a\n",
      "previously\n",
      "initialized\n",
      "Application\n",
      "Load\n",
      "Balancer on\n",
      "the vLB\n",
      "system and\n",
      "you want to\n",
      "reuse the ALB\n",
      "for your\n",
      "cluster. Now,\n",
      "please enter\n",
      "the Load\n",
      "Balancer\n",
      "Name\n",
      "information\n",
      "into this\n",
      "annotation.\n",
      "vks.vngcloud.vn/package-idOptional •If you do not\n",
      "enter this\n",
      "information,\n",
      "we will use\n",
      "the ALB\n",
      "Small\n",
      "configuration\n",
      "by default.\n",
      "•If you already\n",
      "have an\n",
      "ACTIVE vLB\n",
      "host and you\n",
      "want to\n",
      "integrate this\n",
      "host into your\n",
      "K8S cluster,\n",
      "217please skip\n",
      "this\n",
      "information\n",
      "field.\n",
      "vks.vngcloud.vn/tags Optional •The tag is\n",
      "added to your\n",
      "ALB.\n",
      "vks.vngcloud.vn/scheme Optional •Default is \n",
      "internet-\n",
      "facing , you\n",
      "can change it\n",
      "to internal\n",
      "depending on\n",
      "your needs.\n",
      "vks.vngcloud.vn/security-\n",
      "groupsOptional •By default, a \n",
      "default\n",
      "security\n",
      "group will be\n",
      "created\n",
      "according to\n",
      "your Cluster.\n",
      "vks.vngcloud.vn/inbound-\n",
      "cidrsOptional •Default All\n",
      "CIRD\u0000 \n",
      "0.0.0.0/0\n",
      "vks.vngcloud.vn/healthy-\n",
      "threshold-countOptional •Default 3\n",
      "vks.vngcloud.vn/unhealthy-\n",
      "threshold-countOptional •Default 3\n",
      "vks.vngcloud.vn/healthchec\n",
      "k-interval-secondsOptional •Default 30\n",
      "vks.vngcloud.vn/healthchec\n",
      "k-timeout-secondsOptional •Default 5\n",
      "vks.vngcloud.vn/healthchec\n",
      "k-protocolOptional •Default TCP .\n",
      "The user can\n",
      "select one of\n",
      "the\n",
      "TCP/HTTP\n",
      "values\n",
      "218vks.vngcloud.vn/healthchec\n",
      "k-http-methodOptional •Default GET .\n",
      "User can\n",
      "choose one\n",
      "of GET /\n",
      "POST / PUT\n",
      "values\n",
      "vks.vngcloud.vn/healthchec\n",
      "k-pathOptional •Default /\n",
      "vks.vngcloud.vn/healthchec\n",
      "k-http-versionOptional •Default 1.0 .\n",
      "Users can\n",
      "choose one\n",
      "of the values   \n",
      "1.0, 1.1\n",
      "vks.vngcloud.vn/healthchec\n",
      "k-http-domain-nameOptional •Default is\n",
      "empty\n",
      "vks.vngcloud.vn/healthchec\n",
      "k-portOptional •Default traffic\n",
      "port\n",
      "vks.vngcloud.vn/success-\n",
      "codesOptional •Default 200\n",
      "vks.vngcloud.vn/idle-\n",
      "timeout-clientOptional •Default 50\n",
      "vks.vngcloud.vn/idle-\n",
      "timeout-memberOptional •Default 50\n",
      "vks.vngcloud.vn/idle-\n",
      "timeout-connectionOptional •Default 5\n",
      "vks.vngcloud.vn/pool-\n",
      "algorithmOptional •Default \n",
      "ROUND_ROBI\n",
      "N . The user\n",
      "can select\n",
      "one of the\n",
      "values   \n",
      "ROUND_ROBI\n",
      "N /\n",
      "LEAST_CONN\n",
      "ECTIONS /\n",
      "SOURCE_IP\n",
      "219The Ingress installed by the VNGCloud Ingress Controller will have the information\n",
      "IngressClassName = \"vngcloud\". You may not change this information.\n",
      "•An Ingress without any rules will send all traffic to a single default service\n",
      "default backend, or if no host and path match the HTTP request in the Ingress\n",
      "Yaml file, traffic will be routed to the service default backend. For example\n",
      "below, we are configuring the default if the request does not satisfy any rule in\n",
      "the Ingress yaml file, it will go to service name: example-svc-1 with port number\n",
      "8080vks.vngcloud.vn/enable-\n",
      "sticky-sessionOptional •Default false .\n",
      "vks.vngcloud.vn/enable-tls-\n",
      "encryptionOptional •Default false\n",
      "vks.vngcloud.vn/target-\n",
      "node-labelsOptional •Default is\n",
      "empty\n",
      "vks.vngcloud.vn/certificate-\n",
      "idsOptional •Default is\n",
      "empty\n",
      "defaultBackend:\n",
      "    service:\n",
      "      name: example-svc-1\n",
      "      port:\n",
      "        number: 8080IngressClassName\n",
      "DefaultBackend\n",
      "220You can secure Ingress by specifying a Secret that contains the TLS key and\n",
      "certificate. Currently Ingress only supports TLS port 443 and is the termination\n",
      "point for TLS \u0000TLS termination). TLS Secret must contain fields with key names\n",
      "tls.crt and tls.key, which are the certificate and private key to use for TLS.\n",
      "Specifically, you need to specify:\n",
      "•Host: the specified hosts will use the cert.\n",
      "•SecretName: secret name containing cert.\n",
      "Each path in Ingress has a corresponding pathType. There are three supported\n",
      "pathTypes:\n",
      "•Exact: Matches the URL path with absolute precision and is case sensitive.\n",
      "•Prefix: Matches based on the URL path prefix separated by /. Matching is case-\n",
      "sensitive and is performed on each element of the URL path. A component of\n",
      "the main URL path is a label separated by a / in the URL path \u0000This means that\n",
      "the URL path can consist of multiple levels separated by /, each string is\n",
      "between two main / marks). is a label, each label is a component of the URL\n",
      "path). A URL request is considered to match a path field (configured in the\n",
      "Ingress specification) when the entire value of the path (which can include\n",
      "multiple components separated by /) matches the first labels (adjectives). left of\n",
      "the URL\u0000. For example /example1/path1 matches /example1/path1/path2, but not\n",
      "/example1/path1path2\n",
      "Specific examples:\n",
      "Path type Path(s) Request path(s)Is there a match or\n",
      "not?\n",
      "Exact /example1 /example1 HaveTLS\n",
      "Path types\n",
      "221•In some cases, multiple paths within Ingress will match the path of the request\n",
      "URL. In those cases, priority will be given to the longest matching path first. If\n",
      "the two paths still have the same length, the priority will be in the order of the\n",
      "rule created on the Ingress Yaml file.Exact /example1 /host1 Are not\n",
      "Exact /example1 /example1/ Are not\n",
      "Exact /example1/ /example1 Are not\n",
      "Prefix / (all paths) Have\n",
      "Prefix /example1 /example1,\n",
      "/example1/Have\n",
      "Prefix /example1/ /example1,\n",
      "/example1/Have\n",
      "Prefix /example1/host1\n",
      "1/example1/host1Are not\n",
      "Prefix /example1/host1 /example1/host1Have\n",
      "Prefix /example1/host1\n",
      "//example1/host1Have\n",
      "Prefix /example1/host1 /example1/host1\n",
      "/Have\n",
      "Prefix /example1/host1 /example1/host1\n",
      "/cccHave\n",
      "Prefix /example1/host1 /example1/host1\n",
      "xyzAre not\n",
      "Prefix /,/example1 /example1/cccHave\n",
      "Prefix /, /example1,\n",
      "/example1/host1/example1/host1Have\n",
      "Prefix /, /example1,\n",
      "/example1/host1/ccc Have\n",
      "Prefix /example1 /ccc Are not\n",
      "222Each HTTP rule contains the following information:\n",
      "•1 optional host . If no host (we can understand it as a domain name) is\n",
      "specified, the rule will be applied to all HTTP traffic inbound to the specified IP\n",
      "address. If a host is specified (for example example1.com), the rule only applies\n",
      "to that host.\n",
      "•A list of paths ( for example /example1/host1), each path has a backend\n",
      "service associated with it defined by Service Name and Port Number. Both host\n",
      "and path must match the content of the incoming request before the load\n",
      "balancer directs traffic to the desired Services.\n",
      "•A backend is a combination of the Service name and Port Number. HTTP and\n",
      "HTTPS requests going to Ingress and whose URL matches the host and path of\n",
      "the rule will be sent to the list of backends.\n",
      "For example, does Host match the Host header according to the table:\n",
      "Host Host header Is there a match or not?\n",
      "*.example1.com example2.example1.comHave\n",
      "*.example1.com baz.example2.example1.\n",
      "comAre not\n",
      "*.example1.com example1.com Are notIngress rule\n",
      "223ALB Limitation\n",
      "A few notes about the limitations of ingressing an ALB into a cluster:\n",
      "•A cluster can have multiple Ingresses, each Ingress containing resource\n",
      "information for a single ALB.\n",
      "•One ALB can be used for many Ingress. From there, one ALB can be used for\n",
      "many clusters but must ensure these clusters have the same Subnet .\n",
      "•An ALB can include many listeners, many pools, and many policies. For limits on\n",
      "the number of listeners, number of pools, number of policies, please refer to\n",
      "\u0000Resource limit]\n",
      "•Ingress controller manager needs to configure annotations to specify ALB\n",
      "properties, such as protocol, port,... These annotations may vary depending on\n",
      "the cloud service provider, currently with vngcloud ingress controller is being\n",
      "used. provides a reference annotation list at Configuration for an Application\n",
      "Load Balancer . We will further upgrade this part in the next release versions.\n",
      "•Currently, vLB does not support the strip path feature in Load Balancer Layer 7,\n",
      "we will soon integrate this feature in the future.\n",
      "•Changing the name or size \u0000Rename, Resize) of the Load Balancer resource on\n",
      "vServer Portal can cause incompatibility with resources on the Kubernetes\n",
      "Cluster. This can lead to resources becoming inactive on the Cluster, or\n",
      "resources being resynchronized, or resource information between vServer\n",
      "Portal and the Cluster not matching. To prevent this problem, use kubectl\n",
      "Cluster resource management.Note\n",
      "Limit\n",
      "224Working with Network\n",
      "load balancing (NLB)\n",
      "•Network Load Balancer \u0000NLB\u0000 is a load balancer provided by VNGCloud that\n",
      "helps distribute network traffic to multiple back-end servers in a computer\n",
      "group (instance group). NLB operates at layer 4 of the OSI model, providing\n",
      "load balancing based on IP addresses and TCP/UDP ports. For more detailed\n",
      "information about NLB, please refer to \u0000How it works \u0000NLB\u0000\u0000\n",
      "Model deployment\n",
      "•vngcloud-controller-manager : VNG Cloud Controller Manager is a controller\n",
      "that runs on Kubernetes clusters deployed on VNG Cloud. It is responsible for\n",
      "managing VNG Cloud resources for Kubernetes clusters, including:\n",
      "◦Create and manage Network Load Balancer \u0000NLB\u0000 for Kubernetes Services\n",
      "with service type = Load Balancer.\n",
      "What is NLB?\n",
      "225Integrate with Network Load Balancer\n",
      "To integrate a Network Load Balancer with a Kubernetes cluster, you can use a\n",
      "Service with type LoadBalancer . When you create such a Service, VNGCloud\n",
      "Controller Manager will automatically create an NLB to forward traffic to pods on\n",
      "your node . You can also use annotations to customize Network Load Balancer\n",
      "properties, such as port, protocol,...\n",
      "•Create a Kubernetes cluster on VNGCloud, or use an existing cluster. Note:\n",
      "make sure you have downloaded the cluster configuration file once the cluster\n",
      "has been successfully initialized and accessed your cluster.\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vLBFullAccess , vServerFullAccess . To create a service account, go here and\n",
      "follow these steps:\n",
      "◦Select \" Create a Service Account \", enter a name for the Service Account\n",
      "and click Next Step to assign permissions to the Service Account\n",
      "◦Find and select Policy: vLBFullAccess and Policy: vServerFullAccess ,\n",
      "then click \" Create a Service Account \" to create Service Account, Policy:\n",
      "vLBFullAccess and Policy: vServerFullAccess created by VNG Cloud, you\n",
      "cannot delete these policies.\n",
      "◦After successful creation, you need to save the Client_ID and Secret_Key of\n",
      "the Service Account to perform the next step.\n",
      "Attention:\n",
      "When you initialize the Cluster according to the instructions above, if you\n",
      "have not enabled the Enable vLB Native Integration Driver option , by\n",
      "Prepare\n",
      "Create Service Account and install\n",
      "VNGCloud Controller Manager\n",
      "226\n",
      "default we will not pre-install this plugin into your Cluster. You need to\n",
      "manually create Service Account and install VNGCloud Controller Manager\n",
      "according to the instructions below. If you have enabled the Enable vLB\n",
      "Native Integration Driver option , then we have pre-installed this plugin\n",
      "into your Cluster, skip the Service Account Initialization step, install\n",
      "VNGCloud Controller Manager and continue following the instructions\n",
      "from Deploy once. Workload.\n",
      "Initialize Service Account\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vLBFullAccess , vServerFullAccess . To create a service account,\n",
      "go here and follow these steps:\n",
      "◦Select \" Create a Service Account \", enter a name for the\n",
      "Service Account and click Next Step to assign permissions to the\n",
      "Service Account\n",
      "◦Find and select Policy: vLBFullAccess and Policy:\n",
      "vServerFullAccess , then click \" Create a Service Account \" to\n",
      "create Service Account, Policy: vLBFullAccess and Policy:\n",
      "vServerFullAccess created by VNG Cloud, you cannot delete\n",
      "these policies.\n",
      "◦After successful creation, you need to save the Client_ID and \n",
      "Secret_Key of the Service Account to perform the next step.\n",
      "•Uninstall cloud-controller-manager\n",
      "•Besides, you can delete the Service Account being used for the\n",
      "cloud-controller-manager you just removedInstructions for creating Service Account and installing VNGCloud\n",
      "Controller Manager\n",
      "kubectl get daemonset -n kube-system | grep -i \"cloud-controller\n",
      "# if your output is similar to the following, you MUST delete th\n",
      "kubectl delete daemonset cloud-controller-manager -n kube-system\n",
      "227\n",
      "Install VNGCloud Controller Manager\n",
      "•Install Helm version 3.0 or higher. Refer to \n",
      "https://helm.sh/docs/intro/install/ for instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Replace your K8S cluster's ClientID, Client Secret, and ClusterID\n",
      "information and continue running:\n",
      "•After the installation is complete, check the status of vngcloud-\n",
      "Integrate-controller pods:\n",
      "For example, in the image below you have successfully installed\n",
      "vngcloud-controller-manager:kubectl get sa -n kube-system | grep -i \"cloud-controller-manage\n",
      "# if your output is similar to the above, you MUST delete this s\n",
      "kubectl delete sa cloud-controller-manager -n kube-system --forc\n",
      "helm repo add vks-helm-charts https://vngcloud.github.io/vks-hel\n",
      "helm repo update\n",
      "helm install  vngcloud-controller-manager vks-helm-charts/vngclo\n",
      "  --namespace kube-system \\\n",
      "  --set cloudConfig.global.clientID= <L ấ y ClientID c ủ a Service A\n",
      "  --set cloudConfig.global.clientSecret= <L ấ y ClientSecret c ủ a S\n",
      "  --set cluster.clusterID= <L ấ y Cluster ID c ủ a cluster mà b ạ n đ ã\n",
      "kubectl get pods -n kube-system | grep vngcloud-controller-manag\n",
      "NAME                                          READY   STATUS    \n",
      "vngcloud-controller-manager-8864c754c-bqhvz   1/1     Running   \n",
      "Deploy a Workload\n",
      "2281.If you do not have a previously initialized Network Load Balancer available on\n",
      "the vLB system.\n",
      "At this point, you need to do:\n",
      "•Create nginx-service-lb4.yaml file with the following content:\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx\n",
      "  type: LoadBalancer \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80Step 1 : Create Deployment, Service for Nginx app.\n",
      "229•Or use the following script file to deploy HTTP Apache Service with Internal\n",
      "LoadBalancer allowing internal access on port 8080\u0000\n",
      "•Or sample YAML file to create Deployment and Service for a UDP server\n",
      "application in a Kubernetes cluster:apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: internal-http-apache2-deployment\n",
      "spec:\n",
      "  replicas: 2\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: apache2\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: apache2\n",
      "    spec:\n",
      "      containers:\n",
      "        - name: apache2\n",
      "          image: httpd\n",
      "          ports:\n",
      "            - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: internal-http-apache2-service\n",
      "  annotations:\n",
      "    vks.vngcloud.vn/scheme: \"internal\"              # MUST set like this \n",
      "spec:\n",
      "  selector:\n",
      "    app: apache2\n",
      "  type: LoadBalancer                                # MUST set like this \n",
      "  ports:\n",
      "    - name: http\n",
      "      protocol: TCP\n",
      "      port: 8080                                    # CAN be accessed via \n",
      "      targetPort: 80\n",
      "2302.If you already have a previously initialized Network Load Balancer on the vLB\n",
      "system and you want to reuse the NLB for your cluster.\n",
      "At this point, please enter the Load Balancer ID information into the \n",
      "vks.vngcloud.vn/load-balancer-id annotation. The example below is a sampleapiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: udp-server-deployment\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      name: udp-server\n",
      "  replicas: 5\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        name: udp-server\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: udp-server\n",
      "        image: vcr.vngcloud.vn/udp-server\n",
      "        imagePullPolicy: Always\n",
      "        ports:\n",
      "        - containerPort: 10001\n",
      "          protocol: UDP\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: udp-server-service\n",
      "  annotations:\n",
      "    vks.vngcloud.vn/pool-algorithm: \"source-ip\"\n",
      "  labels:\n",
      "    app: udp-server\n",
      "spec:\n",
      "  type: LoadBalancer\n",
      "  sessionAffinity: ClientIP\n",
      "  ports:\n",
      "  - port: 10001\n",
      "    protocol: UDP\n",
      "  selector:\n",
      "    name: udp-server\n",
      "231YAML file to deploy Nginx with External LoadBalancer using vngcloud-controller-\n",
      "manager to automatically expose the service to the internet using an L4 load\n",
      "balancer using an available NLB with ID = lb-2b9d8974\u0000 3760\u00004d60\u00008203\u0000\n",
      "9671f229fb96\n",
      "3.Once a new NLB has been automatically created by us , you can now proceedapiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: external-http-nginx-deployment\n",
      "spec:\n",
      "  replicas: 2\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "kind: Service\n",
      "apiVersion: v1\n",
      "metadata:\n",
      "  name: external-http-nginx-service\n",
      "  annotations:\n",
      "    vks.vngcloud.vn/package-id: \"lbp-ddbf9313-3f4c-471b-afd5-f6a3305159fc\n",
      "    vks.vngcloud.vn/load-balancer-id: \"lb-2b9d8974-3760-4d60-8203-9671f22\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx\n",
      "  type: LoadBalancer\n",
      "  ports:\n",
      "  - name: http\n",
      "    port: 80\n",
      "    targetPort: 80\n",
      "232•Edit your NLB configuration according to the specific instructions at Configure\n",
      "for a Network Load Balancer . For example below, I have edited the protocol and\n",
      "port as follows:\n",
      "•Like other Kubernetes resources, vngcloud-controller-manager has a structure\n",
      "including the following information fields:\n",
      "◦apiVersion: API version for Ingress.\n",
      "◦kind: Resource type, in this case \"Service\".apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: http-apache2-deployment\n",
      "spec:\n",
      "  replicas: 2\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: apache2\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: apache2\n",
      "    spec:\n",
      "      containers:\n",
      "        - name: apache2\n",
      "          image: httpd\n",
      "          ports:\n",
      "            - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: http-apache2-service\n",
      "  annotations:\n",
      "    vks.vngcloud.vn/load-balancer-id: \"lb-f8c0d85b-cb0c-4c77-b382-37982c4\n",
      "spec:\n",
      "  selector:\n",
      "    app: apache2\n",
      "  type: LoadBalancer\n",
      "  ports:\n",
      "    - name: http\n",
      "      protocol: TCP\n",
      "      port: 8000\n",
      "      targetPort: 80\n",
      "233◦metadata: Information describing Ingress, including name, annotations.\n",
      "◦spec: Configure the conditions of incoming requests.\n",
      "For general information about working with vngcloud-controller-manager, see\n",
      "\u0000Configure for a Network Load Balancer]\n",
      "•Deploy this Service using:\n",
      "•Run the following command to test Deployment\n",
      "•If the results are returned as below, it means you have deployed Deployment\n",
      "successfully.\n",
      "At this point, the vLB system will automatically create a corresponding LB for the\n",
      "deployed nginx app, for example:kubectl apply -f nginx-service-lb4.yaml\n",
      "kubectl get svc,deploy,pod -owide\n",
      "NAME                    TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(\n",
      "service/kubernetes      ClusterIP      10.96.0.1      <none>        443/T\n",
      "service/nginx-service   LoadBalancer   10.96.74.154   <pending>     80:31\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAI\n",
      "deployment.apps/nginx-app   0/1     1            0           2s    nginx  \n",
      "NAME                             READY   STATUS              RESTARTS   A\n",
      "pod/nginx-app-7f45b65946-bmrcf   0/1     ContainerCreating   0          2Step 2: Check Deployment and Service information just\n",
      "deployed\n",
      "234You can get Load Balancer Public Endpoint information at the vLB interface.\n",
      "Specifically, access at https://hcm-3.console.vngcloud.vn/vserver/load-\n",
      "balancer/vlb/\n",
      "For example, below I have successfully accessed the nginx app with the address: \n",
      "http://180.93.181.20/\n",
      "http://Endpoint/Step 3: To access the just exported nginx app, you can\n",
      "use the URL with the format:\n",
      "235\n",
      "236Configure for a\n",
      "Network Load Balancer\n",
      "On the \u0000Integrate with Network Load Balancer] page, we have shown you how to\n",
      "install VNGCloud Controller Manager, create and apply yaml file. The following are\n",
      "detailed meanings of the information you can set in the yaml file:\n",
      "Use the annotations below to customize the Load Balancer to suit your needs:\n",
      "Annotations Required/Not required Meaning\n",
      "vks.vngcloud.vn/load-\n",
      "balancer-idOptional •If you do not\n",
      "already have a\n",
      "previously initialized\n",
      "Network Load\n",
      "Balancer on the vLB\n",
      "system. We will\n",
      "automatically create\n",
      "1 NLB on your\n",
      "cluster. This NLB\n",
      "will be displayed on\n",
      "vLB Portal, details\n",
      "can be accessed \n",
      "here\n",
      "•If you already have\n",
      "a previously\n",
      "initialized Network\n",
      "Load Balancer on\n",
      "the vLB system and\n",
      "you want to reuse\n",
      "the NLB for your\n",
      "cluster. Now, please\n",
      "enter the Load\n",
      "Balancer ID\n",
      "information into this\n",
      "annotation.Annotations\n",
      "237vks.vngcloud.vn/load-\n",
      "balancer-nameOptional •Annotation \n",
      "vks.vngcloud.vn/\n",
      "load-balancer-\n",
      "name\n",
      "will be used if you \n",
      "do not use\n",
      "annotation \n",
      "load-balancer-id.\n",
      "•Annotation \n",
      "vks.vngcloud.vn/\n",
      "load-balancer-\n",
      "name\n",
      "only makes sense\n",
      "when you create a\n",
      "new load balancer.\n",
      "After the load\n",
      "balancer is\n",
      "successfully\n",
      "created, this\n",
      "annotation will be\n",
      "automatically\n",
      "deleted . Using this\n",
      "annotation after the\n",
      "load balancer has\n",
      "been created will \n",
      "have no effect .\n",
      "•When you use this\n",
      "annotation, if you\n",
      "do not already have\n",
      "a previously\n",
      "initialized Network\n",
      "Load Balancer on\n",
      "the vLB system. We\n",
      "will automatically\n",
      "create 1 NLB on\n",
      "your cluster. This\n",
      "ALB will be\n",
      "displayed on vLB\n",
      "Portal, details can\n",
      "be accessed here\n",
      "•If you already have\n",
      "a previously\n",
      "initialized Network\n",
      "Load Balancer on\n",
      "238the vLB system and\n",
      "you want to reuse\n",
      "the NLB for your\n",
      "cluster. Now, please\n",
      "enter the Load\n",
      "Balancer Name\n",
      "information into this\n",
      "annotation.\n",
      "vks.vngcloud.vn/package\n",
      "-idOptional •If you do not enter\n",
      "this information, we\n",
      "will use the NLB\n",
      "Small configuration\n",
      "by default.\n",
      "•If you already have\n",
      "an ACTIVE vLB host\n",
      "and you want to\n",
      "integrate this host\n",
      "into your K8S\n",
      "cluster, please skip\n",
      "this information\n",
      "field.\n",
      "vks.vngcloud.vn/tags Optional •The tag is added to\n",
      "your NLB.\n",
      "vks.vngcloud.vn/schemeOptional •Default is internet-\n",
      "facing , you can\n",
      "change it to internal\n",
      "depending on your\n",
      "needs.\n",
      "vks.vngcloud.vn/security-\n",
      "groupsOptional •By default, a default\n",
      "security group will\n",
      "be created\n",
      "according to your\n",
      "Cluster.\n",
      "vks.vngcloud.vn/inbound-\n",
      "cidrsOptional •Default All CIRD\u0000 \n",
      "0.0.0.0/0\n",
      "vks.vngcloud.vn/healthy-\n",
      "threshold-countOptional •Default 3\n",
      "vks.vngcloud.vn/unhealth\n",
      "y-threshold-countOptional •Default 3\n",
      "239vks.vngcloud.vn/healthch\n",
      "eck-interval-secondsOptional •Default 30\n",
      "vks.vngcloud.vn/healthch\n",
      "eck-timeout-secondsOptional •Default 5\n",
      "vks.vngcloud.vn/healthch\n",
      "eck-protocolOptional •Default TCP . Users\n",
      "can select one of\n",
      "the values   TCP/\n",
      "HTTP/ HTTPS/\n",
      "PING\u0000UDP\n",
      "vks.vngcloud.vn/healthch\n",
      "eck-http-methodOptional •Default GET . User\n",
      "can choose one of\n",
      "GET / POST / PUT\n",
      "values\n",
      "vks.vngcloud.vn/healthch\n",
      "eck-pathOptional •Default /\n",
      "vks.vngcloud.vn/healthch\n",
      "eck-http-versionOptional •Default 1.0 . Users\n",
      "can choose one of\n",
      "the values   1.0, 1.1\n",
      "vks.vngcloud.vn/healthch\n",
      "eck-http-domain-nameOptional •Default is empty\n",
      "vks.vngcloud.vn/healthch\n",
      "eck-portOptional •Default traffic port\n",
      "vks.vngcloud.vn/success-\n",
      "codesOptional •Default 200\n",
      "vks.vngcloud.vn/idle-\n",
      "timeout-clientOptional •Default 50\n",
      "vks.vngcloud.vn/idle-\n",
      "timeout-memberOptional •Default 50\n",
      "vks.vngcloud.vn/idle-\n",
      "timeout-connectionOptional •Default 5\n",
      "vks.vngcloud.vn/pool-\n",
      "algorithmOptional •Default \n",
      "ROUND_ROBIN .\n",
      "The user can select\n",
      "one of the values   \n",
      "ROUND_ROBIN /\n",
      "240LEAST_CONNECTIO\n",
      "NS / SOURCE_IP\n",
      "vks.vngcloud.vn/target-\n",
      "node-labelsOptional •Default is empty\n",
      "vks.vngcloud.vn/enable-\n",
      "proxy-protocolOptional •Default is empty.\n",
      "The user specifies a\n",
      "list of service\n",
      "names in the Load\n",
      "Balancer to which\n",
      "the Proxy Protocol\n",
      "will be applied.\n",
      "241NLB Limitation\n",
      "A few notes about the limitations of integrating an NLB into a cluster:\n",
      "•One NLB can be used for many clusters but must ensure these clusters have\n",
      "the same Subnet .\n",
      "•An NLB can include many listeners, many pools, and many policies. For limits on\n",
      "the number of listeners, number of pools, number of policies, please refer to \n",
      "Resource Limits .\n",
      "•Changing the name or size \u0000Rename, Resize) of the Load Balancer resource on\n",
      "vServer Portal can cause incompatibility with resources on the Kubernetes\n",
      "Cluster. This can lead to resources becoming inactive on the Cluster, or\n",
      "resources being resynchronized, or resource information between vServer\n",
      "Portal and the Cluster not matching. To prevent this problem, use kubectl\n",
      "Cluster resource management.Note\n",
      "Limit\n",
      "242CNI\n",
      "CNI \u0000Container Network Interface) is a standard set of tools that provides\n",
      "networking capabilities to containers in a Kubernetes cluster. Simply put, CNI is an\n",
      "abstraction layer that helps Kubernetes manage and configure networking for pods\n",
      "(a collection of containers sharing the same network) in a flexible and efficient way.\n",
      "When you create a new pod, Kubernetes calls CNI to create a network interface for\n",
      "that pod. The CNI plugin performs the following tasks:\n",
      "•Assign IP address: Assign a unique IP address to the pod.\n",
      "•Routing configuration: Set up routing rules that allow communication between\n",
      "pods,...\n",
      "Additionally, the connections work as follows:\n",
      "•Connecting within the same VPC : Nodes within the same VPC will connect\n",
      "directly to each other.\n",
      "•Connecting between different VPCs : Use VPC Peering to connect nodes\n",
      "between different VPCs.\n",
      "•Connect to external infrastructure: Use networking solutions such as site-to-\n",
      "site VPN or Direct Connect to connect from nodes in VPC to external\n",
      "infrastructures \u0000On Cloud, On-premise).\n",
      "This helps maintain a continuous, flexible, and secure network infrastructure in a\n",
      "multi-cloud or hybrid-cloud environment.Overview\n",
      "How does CNI work?\n",
      "Comparison between CNI plugins\n",
      "243Currently, VKS is providing 3 popular CNI plugins: Calico Overlay, Cilium Overlay,\n",
      "Cilium VPC Native Routing. In which:\n",
      "•Calico Overlay : Uses overlay model through tunneling ( IP-in-IP ). Compatible\n",
      "with many infrastructures but performance can be affected by tunnel overhead\n",
      ".\n",
      "•Cilium Overlay : Also uses the overlay model but has strong integration with \n",
      "eBPF , which improves performance, security, and scalability.\n",
      "•Cilium VPC Native Routing : Uses eBPF and no overlay required , leveraging\n",
      "the routing capabilities of the VPC infrastructure, providing the best\n",
      "performance and scalability.\n",
      "When to use Calico Overlay : simple to use, does not require too high\n",
      "performance.\n",
      "When to use Cilium Overlay : simple to use, does not require too high performance\n",
      "but requires intensive monitoring \u0000Hubble\u0000.\n",
      "When to use Cilium VPC Native Routing : high performance requirements, easy \n",
      "connectivity to external systems , and in-depth monitoring needs \u0000Hubble\u0000.\n",
      "244Using CNI Calico Overlay\n",
      "Overview\n",
      "The CNI Calico Overlay in VKS is a type of overlay network that uses IP-in-IP\n",
      "encapsulation to create an overlay network. This allows pods to communicate with\n",
      "each other without changing the underlying physical network configuration. Pods\n",
      "will receive IP addresses from the IP address range configured for Calico, which is\n",
      "usually different from the IP address of your VPC or subnet.\n",
      "On VKS, Calico Overlay works according to the following model:\n",
      "In there:\n",
      "•Pods on each node communicate with each other via the cali interface and \n",
      "bridge cni0 .\n",
      "Model\n",
      "245•When pods need to communicate with pods on other nodes, packets are\n",
      "encapsulated into overlay packets and sent over the physical network \u0000VPC\n",
      "Network) .\n",
      "•Calico on each node is responsible for performing encapsulation and\n",
      "decapsulation so that pods can communicate across different nodes.\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have any\n",
      "VPC, Subnet, please initialize VPC, Subnet according to the instructions here .\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please initialize SSH key following the instructions here .\n",
      "•kubectl installed and configured on your device. please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use an outdated\n",
      "version of kubectl, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Access https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 On the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully initialize your VKS account. After successfully\n",
      "Activating, select Create a Cluster.\n",
      "Step 4\u0000 At the Cluster initialization screen, we have set up the information for the\n",
      "Cluster and a Default Node Group for you. To use Calico Overlay for your Cluster ,Necessary conditions\n",
      "Create a Cluster using Calico Overlay\n",
      "246please select:\n",
      "•Network type : Calico Overlay\n",
      "Field Meaning Illustrative example\n",
      "VPC The IP address range that\n",
      "the Cluster nodes will use\n",
      "to communicate.In the picture, we choose\n",
      "VPC with IP range \n",
      "10.111.0.0/16 ,\n",
      "corresponding to 65536\n",
      "IPs\n",
      "Subnet A smaller IP address range\n",
      "belonging to the VPC. Each\n",
      "node in the Cluster will be\n",
      "assigned an IP from this\n",
      "Subnet. The Subnet must\n",
      "be within the IP range of\n",
      "the selected VPC.In the picture, we choose\n",
      "Subnet with Primary IP\n",
      "range of 10.111.0.0/24 ,\n",
      "corresponding to 256 IPs\n",
      "IP\u0000IP encapsulation\n",
      "modeIP\u0000IP encapsulation mode\n",
      "in VKS is AlwaysIn the figure, we select\n",
      "Always mode to always\n",
      "encapsulate packets.\n",
      "CIDR The virtual network range\n",
      "that the pods will useIn the picture, we choose\n",
      "the virtual network range\n",
      "as 172.16.0.0/16. The\n",
      "pods will get IP from this IP\n",
      "range.\n",
      "247Attention:\n",
      "•Only one networktype: In a cluster, you can use only one of three\n",
      "networktypes: Calico Overlay, Cilium Overlay, or Cilium VPC Native\n",
      "Routing\n",
      "•Multiple subnets for a cluster: VKS supports the use of multiple\n",
      "subnets for a cluster. This allows you to configure each node group in\n",
      "the cluster to be located on different subnets within the same VPC,\n",
      "helping to optimize resource allocation and network management.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the status of the Cluster is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "Below are instructions for deploying an nginx deployment and testing IP assignment\n",
      "for the pods deployed in your cluster.\n",
      "Deploy a Workload\n",
      "248Step 1\u0000 Access https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the Download icon and select \n",
      "Download Config File to download the kubeconfig file. This file will give you full\n",
      "access to your Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config folder\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to check the node\n",
      "•If the result is as below, it means your Cluster is successfully initialized with 5\n",
      "nodes:\n",
      "•Continue running the following command to check the pods deployed on your\n",
      "kube-system namespace:\n",
      "•If the result is as below, it means that the pods supporting Calico Overlay have\n",
      "been successfully run:kubectl get nodes\n",
      "NAME                                  STATUS   ROLES    AGE   VERSION\n",
      "vks-cluster01-nodegroup-536d9-452f1   Ready    <none>   15h   v1.28.8\n",
      "vks-cluster01-nodegroup-998b1-14f64   Ready    <none>   16h   v1.28.8\n",
      "vks-cluster01-nodegroup01-22e98       Ready    <none>   19h   v1.28.8\n",
      "vks-cluster01-nodegroup01-36911       Ready    <none>   19h   v1.28.8\n",
      "vks-cluster01-nodegroup01-9102e       Ready    <none>   19h   v1.28.8\n",
      "k get pods -A\n",
      "249Step 2\u0000 Deploy nginx on the newly created cluster:\n",
      "•Initialize the nginx-deployment.yaml file with the following content:NAMESPACE     NAME                                           READY   STAT\n",
      "kube-system   calico-kube-controllers-868b574465-wbxlx       1/1     Runn\n",
      "kube-system   calico-node-65ql2                              1/1     Runn\n",
      "kube-system   calico-node-d9hc7                              1/1     Runn\n",
      "kube-system   calico-node-gp2s7                              1/1     Runn\n",
      "kube-system   calico-node-hgk86                              1/1     Runn\n",
      "kube-system   calico-node-vj9ts                              1/1     Runn\n",
      "kube-system   calico-typha-74d79bf5f6-zzdn9                  1/1     Runn\n",
      "kube-system   coredns-1727334072-86776977c9-l9tcp            1/1     Runn\n",
      "kube-system   coredns-1727334072-86776977c9-xqcn9            1/1     Runn\n",
      "kube-system   konnectivity-agent-bj7wc                       1/1     Runn\n",
      "kube-system   konnectivity-agent-fnm7j                       1/1     Runn\n",
      "kube-system   konnectivity-agent-gvnbl                       1/1     Runn\n",
      "kube-system   konnectivity-agent-jj764                       1/1     Runn\n",
      "kube-system   konnectivity-agent-vgmwf                       1/1     Runn\n",
      "kube-system   kube-proxy-8r85m                               1/1     Runn\n",
      "kube-system   kube-proxy-bddf5                               1/1     Runn\n",
      "kube-system   kube-proxy-kwskl                               1/1     Runn\n",
      "kube-system   kube-proxy-zv6m4                               1/1     Runn\n",
      "kube-system   kube-proxy-zw65v                               1/1     Runn\n",
      "kube-system   vngcloud-controller-manager-67cf7f868c-jc66k   1/1     Runn\n",
      "kube-system   vngcloud-csi-controller-746b67bcb8-dn5d7       7/7     Runn\n",
      "kube-system   vngcloud-csi-controller-746b67bcb8-hqm24       7/7     Runn\n",
      "kube-system   vngcloud-csi-node-24nlb                        3/3     Runn\n",
      "kube-system   vngcloud-csi-node-fgxpg                        3/3     Runn\n",
      "kube-system   vngcloud-csi-node-q9npf                        3/3     Runn\n",
      "kube-system   vngcloud-csi-node-tw5sv                        3/3     Runn\n",
      "kube-system   vngcloud-csi-node-z2sk9                        3/3     Runn\n",
      "kube-system   vngcloud-ingress-controller-0                  1/1     Runn\n",
      "250•Perform this deployment via command:\n",
      "Step 3\u0000 Check the deployed nginx pods and the IP address assigned to each pod\n",
      "•Perform a check of the pods via the command:\n",
      "•You can observe below, the nginx pods are assigned IPs 172.16.xx which satisfy\n",
      "the Calico CIDR condition 172.16.0.0/16 that we specified above:apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 20\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:latest\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "kubectl apply -f nginx-deployment.yaml\n",
      "kubectl get pods -o wide\n",
      "251•You can also perform a detailed description of each pod to check this pod\n",
      "information via the command:NAME                         READY   STATUS    RESTARTS   AGE   IP        \n",
      "nginx-app-7c79c4bf97-2xbwd   1/1     Running   0          49s   172.16.19\n",
      "nginx-app-7c79c4bf97-5hcds   1/1     Running   0          49s   172.16.19\n",
      "nginx-app-7c79c4bf97-5hgwp   1/1     Running   0          49s   172.16.19\n",
      "nginx-app-7c79c4bf97-5l79h   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-5q2f4   1/1     Running   0          49s   172.16.22\n",
      "nginx-app-7c79c4bf97-5szc6   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-9272q   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-cgwrj   1/1     Running   0          49s   172.16.67\n",
      "nginx-app-7c79c4bf97-fhlg4   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-fj865   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-gh6hj   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-hx2rn   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-jv26j   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-km7p4   1/1     Running   0          49s   172.16.22\n",
      "nginx-app-7c79c4bf97-lrh2r   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-lvj6g   1/1     Running   0          49s   172.16.67\n",
      "nginx-app-7c79c4bf97-nhhdk   1/1     Running   0          49s   172.16.22\n",
      "nginx-app-7c79c4bf97-qr2lm   1/1     Running   0          49s   172.16.67\n",
      "nginx-app-7c79c4bf97-x4ztb   1/1     Running   0          49s   172.16.22\n",
      "nginx-app-7c79c4bf97-xrqwx   1/1     Running   0          49s   172.16.67\n",
      "kubectl describe pod nginx-app-7c79c4bf97-2xbwd\n",
      "252Using CNI Cilium Overlay\n",
      "Overview\n",
      "CNI Cilium Overlay in VKS is a type of overlay network that uses eBPF (extended\n",
      "Berkeley Packet Filter) to enhance network performance and security. Compared\n",
      "to solutions like Calico Overlay , Cilium offers higher performance thanks to its\n",
      "ability to process traffic directly in the kernel using eBPF. In addition, Calico often\n",
      "uses iptables to manage traffic, while Cilium with eBPF can handle network policies\n",
      "and application-specific behaviors \u0000Layer 7\u0000.\n",
      "On VKS, Cilium Overlay works according to the following model:\n",
      "In there:\n",
      "•Pod (eth0) \u0000 lxc01/lxc02 : Pods communicate over a virtual network created\n",
      "by Cilium.\n",
      "Model\n",
      "253•lxc01/lxc02 \u0000 cilium_host : Packets from Pods are forwarded to cilium_host,\n",
      "which is the intermediate layer between the Pod's network and the physical\n",
      "network.\n",
      "•cilium_host \u0000 ens3 : After being processed by Cilium (and eBPF\u0000, packets are\n",
      "sent to the physical network via ens3.\n",
      "•ens3 \u0000 VPC Network : Finally, packets are transmitted over the physical\n",
      "network to other nodes or out of the cluster.\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have any\n",
      "VPC, Subnet, please initialize VPC, Subnet according to the instructions here .\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please initialize SSH key following the instructions here .\n",
      "•kubectl installed and configured on your device. please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use an outdated\n",
      "version of kubectl, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Access https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 On the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully initialize your VKS account. After successfully\n",
      "Activating, select Create a Cluster.Necessary conditions\n",
      "Initialize a Cluster using Cilium Overlay\n",
      "254Step 4\u0000 At the Cluster initialization screen, we have set up the information for the\n",
      "Cluster and a Default Node Group for you. To use Cilium Overlay for your Cluster ,\n",
      "please select:\n",
      "•Network type : Cilium Overlay\n",
      "Field Meaning Illustrative example\n",
      "VPC The IP address range that\n",
      "the Cluster nodes will use\n",
      "to communicate.In the picture, we choose\n",
      "VPC with IP range \n",
      "10.111.0.0/16 ,\n",
      "corresponding to 65536\n",
      "IPs\n",
      "Subnet A smaller IP address range\n",
      "belonging to the VPC.\n",
      "Each node in the Cluster\n",
      "will be assigned an IP from\n",
      "this Subnet. The Subnet\n",
      "must be within the IP\n",
      "range of the selected VPC.In the picture, we choose\n",
      "Subnet with Primary IP\n",
      "range of 10.111.0.0/24 ,\n",
      "corresponding to 256 IPs\n",
      "IP\u0000IP encapsulation modeIP\u0000IP encapsulation mode\n",
      "in VKS is AlwaysIn the figure, we select\n",
      "Always mode to always\n",
      "encapsulate packets.\n",
      "CIDR The virtual network range\n",
      "that the pods will useIn the picture, we choose\n",
      "the virtual network range\n",
      "as 172.16.0.0/16. The\n",
      "pods will get IP from this\n",
      "IP range.\n",
      "255Attention:\n",
      "•Only one networktype: In a cluster, you can use only one of three\n",
      "networktypes: Calico Overlay, Cilium Overlay, or Cilium VPC Native\n",
      "Routing\n",
      "•Multiple subnets for a cluster: VKS supports the use of multiple\n",
      "subnets for a cluster. This allows you to configure each node group in\n",
      "the cluster to be located on different subnets within the same VPC,\n",
      "helping to optimize resource allocation and network management.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the status of the Cluster is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "Below are instructions for deploying an nginx deployment and testing IP assignment\n",
      "for the pods deployed in your cluster.\n",
      "Deploy a Workload\n",
      "256Step 1\u0000 Access https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the Download icon and select \n",
      "Download Config File to download the kubeconfig file. This file will give you full\n",
      "access to your Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config folder\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to check the node\n",
      "•If the result is as below, it means your Cluster is successfully initialized with 3\n",
      "nodes:\n",
      "•Continue running the following command to check the pods deployed on your\n",
      "kube-system namespace:\n",
      "•If the result is as below, it means that the pods supporting Cilium Overlay have\n",
      "been successfully run:kubectl get nodes\n",
      "NAME                                   STATUS   ROLES    AGE     VERSION\n",
      "vks-cluster-02-nodegroup-7fb09-3a594   Ready    <none>   5m48s   v1.29.1\n",
      "vks-cluster-02-nodegroup-7fb09-3cb67   Ready    <none>   5m34s   v1.29.1\n",
      "vks-cluster-02-nodegroup-7fb09-430aa   Ready    <none>   5m52s   v1.29.1\n",
      "k get pods -A\n",
      "257Step 2\u0000 Deploy nginx on the newly created cluster:\n",
      "•Initialize the nginx-deployment.yaml file with the following content:NAMESPACE     NAME                                           READY   STAT\n",
      "kube-system   cilium-8xtwz                                   1/1     Runn\n",
      "kube-system   cilium-cpxvv                                   1/1     Runn\n",
      "kube-system   cilium-envoy-b95pg                             1/1     Runn\n",
      "kube-system   cilium-envoy-dx8qg                             1/1     Runn\n",
      "kube-system   cilium-envoy-sqdn8                             1/1     Runn\n",
      "kube-system   cilium-operator-75b8c6f6d4-7x4f6               1/1     Runn\n",
      "kube-system   cilium-operator-75b8c6f6d4-k7j45               1/1     Runn\n",
      "kube-system   cilium-zs2cm                                   1/1     Runn\n",
      "kube-system   coredns-1727408780-5fcf89468-7hmvp             1/1     Runn\n",
      "kube-system   coredns-1727408780-5fcf89468-v9nbd             1/1     Runn\n",
      "kube-system   hubble-relay-8899f8cdc-976zf                   1/1     Runn\n",
      "kube-system   hubble-ui-574c5bb99b-gg7jx                     2/2     Runn\n",
      "kube-system   konnectivity-agent-46nvd                       1/1     Runn\n",
      "kube-system   konnectivity-agent-qhq4m                       1/1     Runn\n",
      "kube-system   konnectivity-agent-xs7bq                       1/1     Runn\n",
      "kube-system   vngcloud-controller-manager-7c47d64584-z8827   1/1     Runn\n",
      "kube-system   vngcloud-csi-controller-848f68f46-2hkxl        7/7     Runn\n",
      "kube-system   vngcloud-csi-controller-848f68f46-bkvkg        7/7     Runn\n",
      "kube-system   vngcloud-csi-node-8rxbx                        3/3     Runn\n",
      "kube-system   vngcloud-csi-node-mxknq                        3/3     Runn\n",
      "kube-system   vngcloud-csi-node-tfrsp                        3/3     Runn\n",
      "kube-system   vngcloud-ingress-controller-0                  1/1     Runn\n",
      "258•Perform this deployment via command:\n",
      "Step 3\u0000 Check the deployed nginx pods and the IP address assigned to each pod\n",
      "•Perform a check of the pods via the command:\n",
      "•You can observe below, the nginx pods are assigned IPs 172.16.xx which satisfy\n",
      "the Cilium CIDR condition 172.16.0.0/16 that we specified above:apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 20\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:latest\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "kubectl apply -f nginx-deployment.yaml\n",
      "kubectl get pods -o wide\n",
      "259•You can also perform a detailed description of each pod to check this pod\n",
      "information via the command:NAME                         READY   STATUS    RESTARTS   AGE   IP        \n",
      "nginx-app-7c79c4bf97-4lcbn   1/1     Running   0          83s   172.16.0.\n",
      "nginx-app-7c79c4bf97-669z9   1/1     Running   0          83s   172.16.1.\n",
      "nginx-app-7c79c4bf97-7hqp5   1/1     Running   0          83s   172.16.1.\n",
      "nginx-app-7c79c4bf97-8fjhm   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-8xmfm   1/1     Running   0          83s   172.16.0.\n",
      "nginx-app-7c79c4bf97-9b4px   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-b7vlg   1/1     Running   0          83s   172.16.1.\n",
      "nginx-app-7c79c4bf97-bc6r4   1/1     Running   0          83s   172.16.0.\n",
      "nginx-app-7c79c4bf97-flkz5   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-k55j6   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-l9p8p   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-llnfq   1/1     Running   0          83s   172.16.1.\n",
      "nginx-app-7c79c4bf97-mg9t8   1/1     Running   0          83s   172.16.0.\n",
      "nginx-app-7c79c4bf97-mlh7g   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-n946h   1/1     Running   0          83s   172.16.1.\n",
      "nginx-app-7c79c4bf97-p9k42   1/1     Running   0          83s   172.16.0.\n",
      "nginx-app-7c79c4bf97-sl4b8   1/1     Running   0          83s   172.16.1.\n",
      "nginx-app-7c79c4bf97-tdtjc   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-zwxps   1/1     Running   0          83s   172.16.2.\n",
      "nginx-app-7c79c4bf97-zxx87   1/1     Running   0          83s   172.16.0.\n",
      "kubectl describe pod nginx-app-7c79c4bf97-4lcbn\n",
      "260Using CNI Cilium VPC Native Routing\n",
      "CNI \u0000Container Network Interface) Cilium VPC Native Routing is a mechanism that\n",
      "helps Kubernetes manage networks without using overlay networks. Instead of\n",
      "using virtual network layers, CNI Cilium VPC Native Routing leverages the direct\n",
      "routing capabilities of cloud service providers' VPCs \u0000Virtual Private Clouds) to\n",
      "optimize data transfer between nodes and pods in the Kubernetes cluster.\n",
      "On VKS, CNI \u0000Container Network Interface) Cilium VPC Native Routing operates\n",
      "according to the following model:\n",
      "In there:\n",
      "Overview\n",
      "Model\n",
      "261•Each Node has a private IP address range for pods \u0000Pod CIDR\u0000. Pods in each\n",
      "node use addresses from this CIDR and communicate over the virtual network.\n",
      "•Cilium and eBPF perform network management for all pods on each node,\n",
      "including handling traffic going from pod to pod, or from node to node. When\n",
      "necessary, eBPF performs masquerading to hide the internal IP address of the\n",
      "pod when communicating with the external network.\n",
      "•Cilium ensures that pods can communicate with each other both within the\n",
      "same node and between different nodes.\n",
      "To be able to initialize a Cluster and Deploy a Workload , you need:\n",
      "•There is at least 1 VPC and 1 Subnet in ACTIVE state . If you do not have any\n",
      "VPC, Subnet, please initialize VPC, Subnet according to the instructions below:\n",
      "◦Step 1: Access the vServer homepage at the link https://hcm-\n",
      "3.console.vngcloud.vn/vserver\n",
      "◦Step 2: Select the VPCs menu in the left menu of the screen.\n",
      "◦Step 3: Here, if you don't have any VPC yet, please select Create VPC by\n",
      "entering the VPC name and defining the desired CIDR/16 range.\n",
      "◦Step 4: After having at least 1 VPC, to create a subnet, you need to select \n",
      "View Detail to expand the control panel at the bottom, including the Subnet\n",
      "section.\n",
      "◦Step 5: In the Subnet section, select Add Subnet. Now, you need to enter:\n",
      "▪Subnet name: the subnet's mnemonic name\n",
      "▪Primary CIDR : \u0000This is the primary IP address range of the subnet. All\n",
      "internal IP addresses of virtual machines \u0000VMs\u0000 in this subnet will be\n",
      "taken from this address range. For example, if you set Primary CIDR to\n",
      "10.1.0.0/24, the IP addresses of the VMs will be in the range of 10.1.0.1 to\n",
      "10.1.0.254.\n",
      "▪Secondary CIDR : This is a secondary IP address range, used to provide\n",
      "additional IP addresses or to separate different services within the same\n",
      "subnet. Each Node has a private IP address range for its pods \u0000PodNecessary conditions\n",
      "262CIDR\u0000. The pods in each node use addresses from this CIDR and\n",
      "communicate over the virtual network.\n",
      "Attention:\n",
      "•The IP address ranges of Primary CIDR and Secondary CIDR cannot\n",
      "overlap. This means that the address range of Secondary CIDR must\n",
      "be outside the range of Primary CIDR and vice versa. For example, if\n",
      "Primary CIDR is 10.1.0.0/24, then Secondary CIDR cannot be 10.1.0.0/20\n",
      "263because it is within the range of Primary CIDR. Instead, you can use a\n",
      "different address range like 10.1.16.0/20.\n",
      "•There is at least 1 SSH key in ACTIVE state . If you do not have any SSH key,\n",
      "please initialize SSH key following the instructions here .\n",
      "•kubectl installed and configured on your device. please refer here if you are not\n",
      "sure how to install and use kuberctl. In addition, you should not use an outdated\n",
      "version of kubectl, we recommend that you use a kubectl version that is no\n",
      "more than one version different from the cluster version.\n",
      "Attention:\n",
      "•When using Cilium's native routing mode, it is crucial to configure \n",
      "Security Groups correctly to allow necessary connections. For\n",
      "example, when running an NGINX pod on a node, you must permit\n",
      "traffic on port 80 to ensure requests from other nodes can connect.\n",
      "This configuration is not required when using the network overlay\n",
      "mode.\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Access https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 On the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully initialize your VKS account. After successfully\n",
      "Activating, select Create a Cluster.\n",
      "Step 4\u0000 At the Cluster initialization screen, we have set up the information for the\n",
      "Cluster and a Default Node Group for you. To use CNI Cilium VPC Native Routing\n",
      "for your Cluster , please select:\n",
      "Create a Cluster using CNI Cilium VPC\n",
      "Native Routing\n",
      "264•Network type : Cilium VPC Native Routing and other parameters as follows:\n",
      "Field Meaning Illustrative example\n",
      "VPC The IP address range that\n",
      "the Cluster nodes will use\n",
      "to communicate.In the picture, we choose\n",
      "VPC with IP range \n",
      "10.111.0.0/16 ,\n",
      "corresponding to 65536\n",
      "IPs\n",
      "Subnet A smaller IP address range\n",
      "belonging to the VPC.\n",
      "Each node in the Cluster\n",
      "will be assigned an IP from\n",
      "this Subnet. The Subnet\n",
      "must be within the IP\n",
      "range of the selected VPC.In the picture, we choose\n",
      "Subnet with Primary IP\n",
      "range of 10.111.0.0/24 ,\n",
      "corresponding to 256 IPs\n",
      "Default Pod IP range This is the secondary IP\n",
      "address range used for\n",
      "pods. It is called \n",
      "Secondary IP range\n",
      "because it does not match\n",
      "the primary IP range of the\n",
      "node. Pods in the Cluster\n",
      "will be assigned IPs from\n",
      "this range.In the picture, we choose \n",
      "Secondary IP range as \n",
      "10.111.160.0/20 -\n",
      "Corresponding to 4096\n",
      "IPs for pods\n",
      "Node CIDR mask size CIDR size for nodes. This\n",
      "parameter indicates how\n",
      "many IP addresses each\n",
      "node will be assigned from\n",
      "the pod IP range. This size\n",
      "should be chosen to\n",
      "ensure that there are\n",
      "enough IP addresses for\n",
      "all pods on each node. You\n",
      "can refer to the table\n",
      "below to understand how\n",
      "to calculate the number of\n",
      "IP addresses that can be\n",
      "used to allocate to nodes\n",
      "and pods in your cluster.In the picture, we choose \n",
      "Node CIDR mask size as \n",
      "/25 - Each node will have \n",
      "128 IP addresses , suitable\n",
      "for the number of pods\n",
      "you want to run on a node.\n",
      "265Suppose, when initializing the cluster, I choose:\n",
      "•VPC : 10.111.0.0/16\n",
      "•Subnet:\n",
      "◦Primary IP Range: 10.111.0.0/24\n",
      "◦Secondary IP Range: 10.111.160.0/20\n",
      "•Node CIDR mask size: Selectable values   range from /24 to /26 .\n",
      "Node CIDR\n",
      "mask sizeNumber of\n",
      "IPs per nodeNumber of\n",
      "nodes that\n",
      "can be\n",
      "created in the\n",
      "/20 range\n",
      "\u00004096 IPs)Number of\n",
      "IPs allocated\n",
      "to pods on\n",
      "each nodeActual\n",
      "number of\n",
      "pods that can\n",
      "be created\n",
      "/24 256 16 256 128\n",
      "/25 128 32 128 64\n",
      "/26 64 64 64 32Calculating the number of IPs for pods and nodes:\n",
      "266\n",
      "267Attention:\n",
      "•Only one networktype: In a cluster, you can use only one of three\n",
      "networktypes: Calico Overlay, Cilium Overlay, or Cilium VPC Native\n",
      "Routing\n",
      "•Multiple subnets for a cluster: VKS supports the use of multiple\n",
      "subnets for a cluster. This allows you to configure each node group in\n",
      "the cluster to be located on different subnets within the same VPC,\n",
      "helping to optimize resource allocation and network management.\n",
      "•Cilium VPC Native Routing and Secondary IP Range : When using\n",
      "Cilium VPC Native Routing for a cluster, you can use multiple\n",
      "Secondary IP Ranges. However, each Secondary IP Range can only be\n",
      "used by a single cluster. This helps avoid IP address conflicts and\n",
      "ensures consistency in network management.\n",
      "•When there are not enough IP addresses in the Node CIDR range or \n",
      "Secondary IP range to create more nodes, specifically:\n",
      "◦If you cannot use the new Node because of running out of IP\n",
      "addresses in the Secondary IP range . At this time, new nodes will\n",
      "still be created and joined to the cluster but you cannot use them.\n",
      "The pods that are required to launch on this new node will be stuck\n",
      "in the \" ContainerCreating \" state because no suitable node can\n",
      "be found to deploy. At this time, you need to create a new node\n",
      "group with a secondary IP range that is not used on any cluster.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the status of the Cluster is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "Below are instructions for deploying an nginx deployment and testing IP assignment\n",
      "for the pods deployed in your cluster.\n",
      "Deploy a Workload\n",
      "268Step 1\u0000 Access https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the icon Download and select \n",
      "Download Config File to download the kubeconfig file. This file will give you full\n",
      "access to your Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config folder\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to check the node\n",
      "•If the result is as below, it means your Cluster is successfully initialized with 3\n",
      "nodes:\n",
      "•Continue by running the following command to check the pods deployed on\n",
      "your kube-system namespace:\n",
      "•If the result is as below, it means that the pods supporting Cilium VPC Native\n",
      "have been running:kubectl get nodes\n",
      "NAME                                           STATUS   ROLES    AGE     \n",
      "vks-cluster-democilium-nodegroup-558f4-39206   Ready    <none>   5m35s   \n",
      "vks-cluster-democilium-nodegroup-558f4-63344   Ready    <none>   5m45s   \n",
      "vks-cluster-democilium-nodegroup-558f4-e6e4d   Ready    <none>   6m24s   \n",
      "k get pods -A\n",
      "269Step 2\u0000 Deploy nginx on the newly created cluster:\n",
      "•Initialize the nginx-deployment.yaml file with the following content:NAMESPACE     NAME                                          READY   STATU\n",
      "kube-system   cilium-envoy-2g22l                            1/1     Runni\n",
      "kube-system   cilium-envoy-h9mjb                            1/1     Runni\n",
      "kube-system   cilium-envoy-ngz89                            1/1     Runni\n",
      "kube-system   cilium-ft98g                                  1/1     Runni\n",
      "kube-system   cilium-operator-5fc5c56c4c-66l6d              1/1     Runni\n",
      "kube-system   cilium-operator-5fc5c56c4c-qfnw2              1/1     Runni\n",
      "kube-system   cilium-rfrr7                                  1/1     Runni\n",
      "kube-system   cilium-xmlq5                                  1/1     Runni\n",
      "kube-system   coredns-1727334052-85db76748b-fpmfr           1/1     Runni\n",
      "kube-system   coredns-1727334052-85db76748b-jqv79           1/1     Runni\n",
      "kube-system   hubble-relay-8578649fdb-bgzzz                 1/1     Runni\n",
      "kube-system   hubble-ui-574c5bb99b-g7l6c                    2/2     Runni\n",
      "kube-system   konnectivity-agent-hmf2x                      1/1     Runni\n",
      "kube-system   konnectivity-agent-q69n2                      1/1     Runni\n",
      "kube-system   konnectivity-agent-wgqbw                      1/1     Runni\n",
      "kube-system   vngcloud-controller-manager-d4d4f7b84-m65nb   1/1     Runni\n",
      "kube-system   vngcloud-csi-controller-565c55dbcc-88pt4      7/7     Runni\n",
      "kube-system   vngcloud-csi-controller-565c55dbcc-v22q4      7/7     Runni\n",
      "kube-system   vngcloud-csi-node-665r2                       3/3     Runni\n",
      "kube-system   vngcloud-csi-node-8x542                       3/3     Runni\n",
      "kube-system   vngcloud-csi-node-gx7zd                       3/3     Runni\n",
      "kube-system   vngcloud-ingress-controller-0                 1/1     Runni\n",
      "270•Perform this deployment via command:\n",
      "Step 3\u0000 Check the deployed nginx pods and the IP address assigned to each pod\n",
      "•Perform a check of the pods via the command:\n",
      "•You can observe below, the nginx pods are assigned IPs 10.111.16x.x which\n",
      "satisfy the Secondary IP range and Node CIDR mask size conditions that we\n",
      "specified above:apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 20\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:latest\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "kubectl apply -f nginx-deployment.yaml\n",
      "kubectl get pods -o wide\n",
      "271•You can also perform a detailed description of each pod to check this pod\n",
      "information via the command:\n",
      "Step 4\u0000 There are a few steps you can take to thoroughly test the performance of\n",
      "Cilium. Specifically:\n",
      "•First, you need to install Cilium CLI following the instructions here .\n",
      "•After installing Cilium CLS, check the status of Cilium in your cluster via the\n",
      "command:\n",
      "•If the result is displayed as below, it means that Cilium is working properly and\n",
      "fully :NAME                         READY   STATUS    RESTARTS   AGE   IP        \n",
      "nginx-app-7c79c4bf97-6v88s   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-754m7   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-9tjw7   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-c6vx7   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-c7nch   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-cggfq   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-cz4xc   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-d84rb   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-dbmt7   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-gtx8b   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-km7tx   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-lmk7c   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-mc24h   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-n4zvf   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-n84tc   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-qtjjx   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-rp4bt   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-sk7tf   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-x8jxm   1/1     Running   0          31s   10.111.16\n",
      "nginx-app-7c79c4bf97-zlstg   1/1     Running   0          31s   10.111.16\n",
      "kubectl describe pod nginx-app-7c79c4bf97-6v88s\n",
      "cilium status wait\n",
      "272Step 5\u0000 You can perform a healthy check to check Cilium in your cluster\n",
      "•Run the following command to perform a healthy check\n",
      "•Reference results    /¯¯\\\n",
      " /¯¯\\__/¯¯\\    Cilium:             OK\n",
      " \\__/¯¯\\__/    Operator:           OK\n",
      " /¯¯\\__/¯¯\\    Envoy DaemonSet:    OK\n",
      " \\__/¯¯\\__/    Hubble Relay:       OK\n",
      "    \\__/       ClusterMesh:        disabled\n",
      "DaemonSet              cilium-envoy       Desired: 3, Ready: 3/3, Availab\n",
      "Deployment             hubble-relay       Desired: 1, Ready: 1/1, Availab\n",
      "Deployment             hubble-ui          Desired: 1, Ready: 1/1, Availab\n",
      "Deployment             cilium-operator    Desired: 2, Ready: 2/2, Availab\n",
      "DaemonSet              cilium             Desired: 3, Ready: 3/3, Availab\n",
      "Containers:            hubble-ui          Running: 1\n",
      "                       cilium-operator    Running: 2\n",
      "                       cilium             Running: 3\n",
      "                       cilium-envoy       Running: 3\n",
      "                       hubble-relay       Running: 1\n",
      "Cluster Pods:          32/32 managed by Cilium\n",
      "Helm chart version:\n",
      "Image versions         cilium             vcr.vngcloud.vn/81-vks-public/c\n",
      "                       cilium-envoy       vcr.vngcloud.vn/81-vks-public/c\n",
      "                       hubble-relay       vcr.vngcloud.vn/81-vks-public/c\n",
      "                       hubble-ui          vcr.vngcloud.vn/81-vks-public/c\n",
      "                       hubble-ui          vcr.vngcloud.vn/81-vks-public/c\n",
      "                       cilium-operator    vcr.vngcloud.vn/81-vks-public/c\n",
      "kubectl -n kube-system exec ds/cilium -- cilium-health status --probe\n",
      "273Additionally, you can also perform additional End-to-End connectivity tests or\n",
      "Network performance tests following the instructions at End-To-End Connectivity\n",
      "Testing or Network Performance Test .\n",
      "Step 6\u0000 Check the connection between Pods\n",
      "•Perform a connectivity test between pods, ensuring that the pods can\n",
      "communicate via the VPC IP address without going through overlay networks\n",
      ". For example, below I perform a ping from the pod nginx-app-7c79c4bf97\u0000\n",
      "6v88s with IP address: 10.111.161.53 to a server in the same VPC with IP address:\n",
      "10.111.0.10\u0000\n",
      "•If the result is as follows, the connection is successful:Probe time:   2024-09-26T07:11:57Z\n",
      "Nodes:\n",
      "  vks-cluster-democilium-nodegroup-558f4-e6e4d (localhost):\n",
      "    Host connectivity to 10.111.0.8:\n",
      "      ICMP to stack:   OK, RTT=306.523µs\n",
      "      HTTP to agent:   OK, RTT=206.191µs\n",
      "    Endpoint connectivity to 10.111.160.91:\n",
      "      ICMP to stack:   OK, RTT=307.205µs\n",
      "      HTTP to agent:   OK, RTT=365.113µs\n",
      "  vks-cluster-democilium-nodegroup-558f4-39206:\n",
      "    Host connectivity to 10.111.0.14:\n",
      "      ICMP to stack:   OK, RTT=1.90859ms\n",
      "      HTTP to agent:   OK, RTT=344.725µs\n",
      "    Endpoint connectivity to 10.111.161.9:\n",
      "      ICMP to stack:   OK, RTT=1.889682ms\n",
      "      HTTP to agent:   OK, RTT=549.887µs\n",
      "  vks-cluster-democilium-nodegroup-558f4-63344:\n",
      "    Host connectivity to 10.111.0.9:\n",
      "      ICMP to stack:   OK, RTT=1.920985ms\n",
      "      HTTP to agent:   OK, RTT=706.376µs\n",
      "    Endpoint connectivity to 10.111.160.223:\n",
      "      ICMP to stack:   OK, RTT=1.919709ms\n",
      "      HTTP to agent:   OK, RTT=1.090877ms\n",
      "kubectl exec -it nginx-app-7c79c4bf97-6v88s -- ping 10.111.0.10\n",
      "274PING 10.111.0.10 (10.111.0.10): 56 data bytes\n",
      "64 bytes from 10.111.0.10: seq=0 ttl=62 time=3.327 ms\n",
      "64 bytes from 10.111.0.10: seq=1 ttl=62 time=0.541 ms\n",
      "64 bytes from 10.111.0.10: seq=2 ttl=62 time=0.472 ms\n",
      "64 bytes from 10.111.0.10: seq=3 ttl=62 time=0.463 ms\n",
      "--- 10.111.0.10 ping statistics ---\n",
      "4 packets transmitted, 4 packets received, 0% packet loss\n",
      "round-trip min/avg/max = 0.463/1.200/3.327 ms\n",
      "275Storage\n",
      "276Working with Container\n",
      "Storage Interface (CSI)\n",
      "•Container Storage Interface \u0000CSI) is a standard interface that allows containers\n",
      "to interact with different storage systems. CSI provides a set of common APIs\n",
      "that containers can use to access and manage data, regardless of the\n",
      "underlying storage system.What is CSI?\n",
      "277Integrate with Container\n",
      "Storage Interface (CSI)\n",
      "To integrate CSI with Kubernetes cluster, follow these steps:\n",
      "•Create a Kubernetes cluster on VNGCloud, or use an existing cluster. Note:\n",
      "make sure you have downloaded the cluster configuration file once the cluster\n",
      "has been successfully initialized and accessed your cluster.\n",
      "Attention:\n",
      "•When you initialize the Cluster according to the instructions above, if\n",
      "you have not enabled the Enable BlockStore Persistent Disk CSI\n",
      "Driver option , by default we will not pre-install this plugin into your\n",
      "Cluster. You need to manually create Service Account and install\n",
      "VNGCloud BlockStorage CSI Driver according to the instructions\n",
      "below. If you have enabled the Enable BlockStore Persistent Disk CSI\n",
      "Driver option , we have pre-installed this plugin into your Cluster, skip\n",
      "the Service Account Initialization step, install VNGCloud BlockStorage\n",
      "CSI Driver and continue following the instructions from now on. Deploy\n",
      "a Workload.\n",
      "•VNGCloud BlockStorage CSI Driveronly supports attaching volumes\n",
      "to a single node \u0000VM) throughout the life of that volume. If you have a\n",
      "need for ReadWriteMany, you may consider using the NFS CSI Driver,\n",
      "as it allows multiple nodes to Read and Write on the same volume at\n",
      "the same time. This is very useful for applications that need to share\n",
      "data between multiple pods or services in Kubernetes.\n",
      "Prepare\n",
      "Create Service Account and install\n",
      "VNGCloud BlockStorage CSI Driver\n",
      "278\n",
      "Initialize Service Account\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vServerFullAccess . To create a service account, go here and follow\n",
      "these steps:\n",
      "◦Select \" Create a Service Account \", enter a name for the\n",
      "Service Account and click Next Step to assign permissions to the\n",
      "Service Account\n",
      "◦Find and select Policy: vServerFullAccess , then click \" Create a\n",
      "Service Account \" to create a Service Account, Policy:\n",
      "vLBFullAccess and Policy: vServerFullAccess are created by VNG\n",
      "Cloud, you cannot delete these policies.\n",
      "◦After successful creation, you need to save the Client_ID and \n",
      "Secret_Key of the Service Account to perform the next step.\n",
      "Install VNGCloud BlockStorage CSI Driver\n",
      "•Install Helm version 3.0 or higher. Refer to \n",
      "https://helm.sh/docs/intro/install/ for instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Replace your K8S cluster's ClientID, Client Secret, and ClusterID\n",
      "information and continue running:\n",
      "Copy\n",
      "•After the installation is complete, check the status of vngcloud-\n",
      "blockstorage-csi-driver pods:Create Service Account and install VNGCloud BlockStorage CSI Driver\n",
      "helm repo add vks-helm-charts https://vngcloud.github.io/vks\n",
      "helm repo update\n",
      "helm install vngcloud-blockstorage-csi-driver vks-helm-chart\n",
      "  --replace --namespace kube-system \\\n",
      "  --set vngcloudAccessSecret.keyId=${VNGCLOUD_CLIENT_ID} \\\n",
      "  --set vngcloudAccessSecret.accessKey=${VNGCLOUD_CLIENT_SEC\n",
      "  --set vngcloudAccessSecret.vksClusterId=${VNGCLOUD_VKS_CLU\n",
      "kubectl get pods -n kube-system | grep vngcloud-csi-\n",
      "279\n",
      "•For example, in the image below you have successfully installed\n",
      "vngcloud-blockstorage-csi-driver:\n",
      "The following is a guide for you to deploy the nginx service on Kubernetes.\n",
      "•Create nginx-service.yaml file with the following content:NAME                                           READY   STATU\n",
      "vngcloud-csi-controller-56bd7b85f-ctpns        7/7     Runni\n",
      "vngcloud-csi-controller-56bd7b85f-npp9n        7/7     Runni\n",
      "vngcloud-csi-node-c8r2w                        3/3     Runni\n",
      "Deploy a Workload\n",
      "Step 1 : Create Deployment for Nginx app.\n",
      "280•Deploy This deployment equals:\n",
      "•Run the following command to test DeploymentapiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-app\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx:1.19.1\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "spec:\n",
      "  selector:\n",
      "    app: nginx \n",
      "  ports:\n",
      "    - protocol: TCP\n",
      "      port: 80\n",
      "      targetPort: 80\n",
      "kubectl apply -f nginx-service.yaml\n",
      "kubectl get svc,deploy,pod -owideStep 2: Check the Deployment and Service information\n",
      "just deployed\n",
      "281•If the results are returned as below, it means you have deployed Deployment\n",
      "successfully.\n",
      "•Create a persistent-volume.yaml file with the following content:\n",
      "CopyNAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   P\n",
      "service/kubernetes      ClusterIP      10.96.0.1       <none>        4\n",
      "service/nginx-app       NodePort       10.96.215.192   <none>        3\n",
      "service/nginx-service   LoadBalancer   10.96.179.221   <pending>     8\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CON\n",
      "deployment.apps/nginx-app   1/1     1            1           16s   ngi\n",
      "NAME                             READY   STATUS    RESTARTS   AGE   IP\n",
      "pod/nginx-app-7f45b65946-t7d7k   1/1     Running   0          16s   17\n",
      "Create Persistent Volume\n",
      "282•Run the following command to deploy a Pod using a Persistent Volume\n",
      "CopyapiVersion: storage.k8s.io/v1\n",
      "kind: StorageClass\n",
      "metadata:\n",
      "  name: my-expansion-storage-class                    # [1] The Storag\n",
      "provisioner: bs.csi.vngcloud.vn                       # The VNG-CLOUD \n",
      "parameters:\n",
      "  type: vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018    # The volume typ\n",
      "allowVolumeExpansion: true                            # MUST set this \n",
      "---\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: my-expansion-pvc                           # [2] The PVC name,\n",
      "spec:\n",
      "  accessModes:\n",
      "    - ReadWriteOnce\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 20Gi                                # [3] The PVC size,\n",
      "  storageClassName: my-expansion-storage-class     # [4] The StorageCl\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: nginx                                      # [5] The Pod name,\n",
      "spec:\n",
      "  containers:\n",
      "    - image: nginx\n",
      "      imagePullPolicy: IfNotPresent\n",
      "      name: nginx\n",
      "      ports:\n",
      "        - containerPort: 80\n",
      "          protocol: TCP\n",
      "      volumeMounts:\n",
      "        - mountPath: /var/lib/www/html\n",
      "          name: my-volume-name                     # MUST be the same \n",
      "  volumes:\n",
      "    - name: my-volume-name                         # [6] The volume na\n",
      "      persistentVolumeClaim:\n",
      "        claimName: my-expansion-pvc                # MUST be the same \n",
      "        readOnly: false\n",
      "283At this time, the vServer system will automatically create a Volume corresponding\n",
      "to the yaml file above, for example:\n",
      "Snapshot is a low-cost, convenient and effective data backup method and can be\n",
      "used to create images, restore data and distribute copies of data. If you are a new\n",
      "user who has never used the Snapshot service, you will need to Activate Snapshot\n",
      "Service before you can create a Snapshot for your Persistent Volume.\n",
      "To be able to create Snapshots, you need to perform Activate Snapshot Service.\n",
      "You will not be charged for activating the snapshot service. After you create\n",
      "snapshots, costs will be calculated based on the storage capacity and storage time\n",
      "of these snapshots. Follow these steps to enable the Snapshot service:\n",
      "Step 1\u0000 Visit https://hcm-3.console.vngcloud.vn/vserver/block-\n",
      "store/snapshot/overviewkubectl apply -f persistent-volume.yaml\n",
      "Create Snapshots\n",
      "Activate Snapshot Service\n",
      "284Step 2\u0000 Select Activate Snapshot Service .\n",
      "For example:\n",
      "•Install Helm version 3.0 or higher. Refer to https://helm.sh/docs/intro/install/ for\n",
      "instructions on how to install.\n",
      "•Add this repo to your cluster via the command:\n",
      "•Continue running:\n",
      "•After the installation is complete, check the status of vngcloud-blockstorage-\n",
      "csi-driver pods:\n",
      "For example, in the image below you have successfully installed vngcloud-\n",
      "snapshot-controller:\n",
      "helm repo add vks-helm-charts https://vngcloud.github.io/vks-helm-char\n",
      "helm repo update\n",
      "helm install vngcloud-snapshot-controller vks-helm-charts/vngcloud-sna\n",
      "  --replace --namespace kube-system\n",
      "kubectl get pods -n kube-system | grep snapshot-controllerInstall VNGCloud Snapshot Controller\n",
      "285•Run the following command to deploy Volume Snapshot\n",
      "•After applying the file successfully, you can check the service and pvc list via:NAME                                           READY   STATUS             \n",
      "snapshot-controller-7fdd984f89-745tg           0/1     ContainerCreating  \n",
      "snapshot-controller-7fdd984f89-k94wq           0/1     ContainerCreating  \n",
      "apiVersion: snapshot.storage.k8s.io/v1\n",
      "kind: VolumeSnapshotClass\n",
      "metadata:\n",
      "  name: my-snapshot-storage-class  # [2] The name of the volume snapshot \n",
      "driver: bs.csi.vngcloud.vn\n",
      "deletionPolicy: Delete\n",
      "parameters:\n",
      "  force-create: \"false\"\n",
      "---\n",
      "apiVersion: snapshot.storage.k8s.io/v1\n",
      "kind: VolumeSnapshot\n",
      "metadata:\n",
      "  name: my-snapshot-pvc  # [4] The name of the snapshot, CAN be changed\n",
      "spec:\n",
      "  volumeSnapshotClassName: my-snapshot-storage-class  # MUST match with [\n",
      "  source:\n",
      "    persistentVolumeClaimName: my-expansion-pvc  # MUST match with [3]\n",
      "kubectl apply -f snapshot.yaml\n",
      "kubectl get sc,pvc,pod -owideCreate a snapshot.yaml file with the following content\n",
      "Check the newly created PVC and Snapshot\n",
      "286To change the IOPS parameters of the newly created Persistent Volume, follow\n",
      "these steps:\n",
      "Step 1\u0000 Run the command below to list the PVCs in your Cluster\n",
      "Step 2\u0000 Edit the PVC YAML file according to the command\n",
      "•If you have not edited the IOPS of the Persistent Volume before, when you run\n",
      "the above command, add an annotation bs.csi.vngcloud.vn/volume-type:\n",
      "\"volume-type-id\" . For example, below I am changing the Persistent Volume\n",
      "IOPS from 200 \u0000Volume type id = vtype-61c3fc5b-f4e9\u000045b4\u00008957\u0000\n",
      "8aa7b6029018) to 1000 \u0000Volume type id = vtype-85b39362-a360\u00004bbb-9afa-\n",
      "a36a40cea748 )\n",
      "CopyNAME                                                       PROVISIONER\n",
      "storageclass.storage.k8s.io/my-expansion-storage-class     bs.csi.vngc\n",
      "storageclass.storage.k8s.io/sc-iops-200-retain (default)   bs.csi.vngc\n",
      "NAME                                     STATUS   VOLUME              \n",
      "persistentvolumeclaim/my-expansion-pvc   Bound    pvc-14456f4a-ee9e-43\n",
      "NAME                             READY   STATUS    RESTARTS   AGE   IP\n",
      "pod/nginx                        1/1     Running   0          10m   17\n",
      "pod/nginx-app-7f45b65946-t7d7k   1/1     Running   0          94m   17\n",
      "kubectl get persistentvolumes\n",
      "kubectl edit pvc my-expansion-pvcChange the IOPS parameters of the newly created\n",
      "Persistent Volume\n",
      "287•If you have edited the IOPS of the Persistent Volume before, when you run the\n",
      "above command, your yaml file will already have the annotation\n",
      "bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Now, edit this annotation to\n",
      "the Volume type id with the IOPS you desire.\n",
      "To change the Disk Volume of the newly created Persistent Volume, run the\n",
      "following command:apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  annotations:\n",
      "    bs.csi.vngcloud.vn/volume-type: \"vtype-85b39362-a360-4bbb-9afa-a36\n",
      "    kubectl.kubernetes.io/last-applied-configuration: |\n",
      "      {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"a\n",
      "    pv.kubernetes.io/bind-completed: \"yes\"\n",
      "    pv.kubernetes.io/bound-by-controller: \"yes\"\n",
      "    volume.beta.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn\n",
      "    volume.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn\n",
      "  creationTimestamp: \"2024-04-21T14:16:53Z\"\n",
      "  finalizers:\n",
      "  - kubernetes.io/pvc-protection\n",
      "  name: my-expansion-pvc\n",
      "  namespace: default\n",
      "  resourceVersion: \"11041591\"\n",
      "  uid: 14456f4a-ee9e-435d-a94f-5a2e820954e9\n",
      "spec:\n",
      "  accessModes:\n",
      "  - ReadWriteOnce\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 20Gi\n",
      "  storageClassName: my-expansion-storage-class\n",
      "  volumeMode: Filesystem\n",
      "  volumeName: pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9\n",
      "status:\n",
      "  accessModes:\n",
      "  - ReadWriteOnce\n",
      "  capacity:\n",
      "    storage: 20Gi\n",
      "  phase: Bound\n",
      "Change the Disk Volume of the newly created Persistent\n",
      "Volume\n",
      "288For example, initially the PVC created was 20 Gi in size, now I will increase it to 30\n",
      "Gi\n",
      "Attention:\n",
      "•You can only increase Disk Volume but cannot reduce Disk Volume\n",
      "size.\n",
      "To restore Persistent Volume from Snapshot, follow these steps:\n",
      "•Create file restore-volume.yaml with the following content:\n",
      "Copykubectl patch pvc my-expansion-pvc -p '{\"spec\":{\"resources\":{\"requests\":{\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: my-restore-pvc  # The name of the PVC, CAN be changed\n",
      "spec:\n",
      "  storageClassName: my-expansion-storage-class  \n",
      "  dataSource:\n",
      "    name: my-snapshot-pvc # MUST match with [4] from the section 5.2\n",
      "    kind: VolumeSnapshot\n",
      "    apiGroup: snapshot.storage.k8s.io\n",
      "  accessModes:\n",
      "    - ReadWriteOnce\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 20GiRestore Persistent Volume from Snapshot\n",
      "289CSI Limitation\n",
      "290Security Group\n",
      "Security Group acts as a firewall to help you control traffic going in and out of the\n",
      "server \u0000VM\u0000. On the VKS system, to ensure the cluster operates safely and\n",
      "effectively, default Security Groups are set up to allow necessary access for the\n",
      "cluster's internal operations. Automatically creating a Security Group simplifies the\n",
      "cluster deployment process and ensures that the cluster is protected from the start.\n",
      "Specifically, when you initialize a Cluser, we will automatically create several\n",
      "Security Groups with the following parameters:\n",
      "For each Cluster created in the VKS system, we will automatically create a Security\n",
      "Group. This security group will include:\n",
      "•Inbound:\n",
      "Protocol Ether type Port range Source Meaning\n",
      "TCP IPv4 30000\u000032767CIDR of the\n",
      "VPC you use\n",
      "for the\n",
      "Cluster.Security\n",
      "group rule\n",
      "used for TCP\n",
      "Node Port\n",
      "Services\n",
      "UDP IPv4 30000\u000032767CIDR of the\n",
      "VPC you use\n",
      "for the\n",
      "Cluster.Security\n",
      "group rule\n",
      "used for UDP\n",
      "Node Port\n",
      "Services\n",
      "TCP IPv4 10250 External IP of\n",
      "Load Balancer\n",
      "used for\n",
      "Cluster.Security\n",
      "group rule\n",
      "used for\n",
      "Kubelet API\n",
      "control-plane\n",
      "TCP IPv4 10250 CIDR of the\n",
      "VPC you use\n",
      "for the\n",
      "Cluster.Security\n",
      "group rule\n",
      "used forThe default security group is automatically created for all Clusters\n",
      "291•Outbound\n",
      "When you use VNGCloud Controller Manager to integrate Network Load Balancer\n",
      "with Cluster on VKS system, we will automatically create a Security Group. This\n",
      "security group will include:\n",
      "•Inbound:Kubelet API\n",
      "control-plane\n",
      "TCP IPv4 179 CIDR of the\n",
      "VPC you use\n",
      "for the\n",
      "Cluster.Security\n",
      "group rule\n",
      "used for\n",
      "Kubelet API\n",
      "control-plane\n",
      "4 IPv4 1\u000065535 CIDR of the\n",
      "VPC you use\n",
      "for the\n",
      "Cluster.Security\n",
      "group rule\n",
      "used for\n",
      "Calico IP-in-IP\n",
      "TCP IPv4 5473 CIDR of the\n",
      "VPC you use\n",
      "for the\n",
      "Cluster.Security\n",
      "group rule\n",
      "used for\n",
      "Calico Typha\n",
      "Protocol Ether type Port range DestinationMeaning\n",
      "ANY IPv4 0\u000065535 0.0.0.0/0 Default rule of\n",
      "all Security\n",
      "groups\n",
      "ANY IPv6 0\u000065535 ::/0 Default rule of\n",
      "all Security\n",
      "groups\n",
      "Protocol Ether type Port range Source\n",
      "TCP, UDP or ICMPIPv4 Port of Service Subnet Mask of\n",
      "the Subnet youSecurity group is automatically created by VNGCLOUD Controller Manager\n",
      "292•Outbound:\n",
      "When you use VNGCloud Ingress Controller to integrate Application Load Balancer\n",
      "with Cluster on VKS system, we will automatically create a Security Group. This\n",
      "security group will include:\n",
      "•Inbound:\n",
      "•Outbound:use for the Cluster.\n",
      "Protocol Ether type Port range DestinationMeaning\n",
      "ANY IPv4 0\u000065535 0.0.0.0/0 Default rule of\n",
      "all Security\n",
      "groups\n",
      "ANY IPv6 0\u000065535 ::/0 Default rule of\n",
      "all Security\n",
      "groups\n",
      "Protocol Ether type Port range Source\n",
      "TCP IPv4 Port of Service Subnet Mask of\n",
      "the Subnet you\n",
      "use for the Cluster.\n",
      "Protocol Ether type Port range DestinationMeaning\n",
      "ANY IPv4 0\u000065535 0.0.0.0/0 Default rule of\n",
      "all Security\n",
      "groups\n",
      "ANY IPv6 0\u000065535 ::/0 Default rule of\n",
      "all Security\n",
      "groupsSecurity group is automatically created by VNGCLOUD Ingress Controller\n",
      "293Attention:\n",
      "•Default Security Groups are set up to meet the basic security needs of\n",
      "the cluster. If you edit or delete the Security Groups created for the\n",
      "cluster, it may result in connectivity and access issues between nodes\n",
      "in the cluster or the cluster may not function correctly or may not even\n",
      "start. To ensure the stability and security of the cluster, the system will\n",
      "automatically reset Security Groups to default settings after every\n",
      "fixed period of time.\n",
      "294Migration\n",
      "Migration from one cluster to another is the process of moving data, applications,\n",
      "and services from one set of servers to another. The purpose of this is often to\n",
      "upgrade the system, increase availability and fault tolerance, or to scale the\n",
      "system.\n",
      "Overview\n",
      "Model\n",
      "Implementation steps\n",
      "295Specifically:\n",
      "•Step 1\u0000 Prepare target cluster \u0000Prepard target resource): on the VKS system, you\n",
      "need to initialize a Cluster according to the instructions here . Make sure that\n",
      "the destination cluster's configuration is the same as the source cluster's\n",
      "configuration.\n",
      "•Step 2 \u0000Optional] : If your cluster has private resources such as images,\n",
      "databases, storage... Now, before starting to migrate, you need to proactively\n",
      "migrate these resources yourself.\n",
      "•Step 3 : Install Velero on both source and destination clusters \u0000Install Velero\n",
      "tool): after you have migrated private resources outside the cluster, you can use\n",
      "the migration tool to backup and restore. Restore the application on the source\n",
      "cluster and target cluster.\n",
      "•Step 4 : Backup: To back up resources, use the Velero tool to create a backup\n",
      "object in the source cluster. Velero will perform queries, package the data, and\n",
      "upload them to an S3 Compatible Object Storage.\n",
      "•Step 5 : Restore: During the restore process at the destination cluster, Velero\n",
      "will download backup data to the new cluster and redeploy resources based on\n",
      "the JSON file.\n",
      "•Step 6 \u0000Optional] : Update resource config: After the target cluster's resources\n",
      "are deployed properly, you can switch traffic for your service. After confirming\n",
      "that all services are running properly, you can delete the source cluster.\n",
      "296Below are detailed instructions for common cases when you migrate workloads\n",
      "from one Cluster to another. You can refer to and follow the instructions at:\n",
      "•Migrate Cluster from VKS to VKS\n",
      "•Migrate Cluster from vContainer to VKS\n",
      "•Migrate Cluster from another platform to VKS\n",
      "297Migrate Cluster from VKS to VKS\n",
      "To migrate a Cluster from the VKS system to the VKS system, follow the steps in\n",
      "this document.\n",
      "•Perform download helper bash script and grand execute permission for this\n",
      "file ( velero_helper.sh )\n",
      "•\u0000Optional) Deploy some services to check the correctness of the migration.\n",
      "Suppose, at the source Cluster, I have deployed an nginx service as follows:\n",
      "◦Deployment files:Prerequisites\n",
      "298apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx\n",
      "  namespace: mynamespace\n",
      "  labels:\n",
      "    app: nginx\n",
      "spec:\n",
      "  ports:\n",
      "  - port: 80\n",
      "    name: web\n",
      "  selector:\n",
      "    app: nginx\n",
      "  type: NodePort\n",
      "---\n",
      "apiVersion: apps/v1\n",
      "kind: StatefulSet\n",
      "metadata:\n",
      "  name: web\n",
      "  namespace: mynamespace\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  serviceName: \"nginx\"\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "          name: web\n",
      "        volumeMounts:\n",
      "        - name: disk-ssd\n",
      "          mountPath: /usr/share/nginx/html\n",
      "  volumeClaimTemplates:\n",
      "  - metadata:\n",
      "      name: disk-ssd\n",
      "      namespace: mynamespace\n",
      "    spec:\n",
      "      accessModes: [ \"ReadWriteOnce\" ]\n",
      "      storageClassName: ssd-3000\n",
      " resources:\n",
      "299Copy\n",
      "•Now, when you access Node's Public IP, you will see \"Hello, MyVNGCloud\".\n",
      "On the VKS system, you need to initialize a Cluster according to the instructions \n",
      "here . Make sure that the destination cluster's configuration is the same as the\n",
      "source cluster's configuration.\n",
      "Attention:\n",
      "For the migration to be successful, on the target Cluster, you need to\n",
      "ensure the following requirements:\n",
      "•The amount of resources needed such as number of nodes, node\n",
      "instance configuration,...\n",
      "•Node labels and node taints are the same as the old cluster.\n",
      "•Corresponding or alternative Storage Class.\n",
      "Migrating private resources outside cluster (moving private resources outside the\n",
      "cluster) is the process of moving private resources outside the source Cluster to a\n",
      "place that the destination Cluster can use. For example, you may have private\n",
      "resources such as images, databases, etc. Now, before starting to migrate, you\n",
      "need to migrate these resources yourself. For example, if you need:      resources:\n",
      "        requests:\n",
      "          storage: 40Gi\n",
      "kubectl exec -n mynamespace -it web-0 bash\n",
      "cd /usr/share/nginx/html\n",
      "echo -e \"<html>\\n<head>\\n  <title>MyVNGCloud</title>\\n</head>\\n<bod\n",
      "Prepare target cluster (Prepare target resource)\n",
      "[Optional] Migrate private resources outside cluster\n",
      "300•Migrate Container Images: you can migrate images to VNGCloud Container\n",
      "Registry through instructions here .\n",
      "•Migrate Databases: you can use Relational Database Service \u0000RDS\u0000 and Object\n",
      "Storage Service \u0000OBS\u0000 depending on your needs. After the migration is\n",
      "complete, remember to reconfigure the database for your applications on VKS\n",
      "Cluster.\n",
      "•Migrate Storage: you can use vServer's NFS Server .\n",
      "Attention:\n",
      "•After you migrate resources outside the Cluster, you need to ensure\n",
      "the target Cluster can connect to these migrated resources.\n",
      "After you have migrated private resources outside the cluster, you can use the\n",
      "migration tool to backup and restore the application on the source cluster and\n",
      "target cluster.\n",
      "•Create a vStorage Project, Container to receive the cluster's backup data\n",
      "according to instructions here .\n",
      "•Create an S3 key corresponding to this vStorage Project according to the\n",
      "instructions here .\n",
      "For example, I have initialized a vStorage Project, Container with the following\n",
      "information: Region: HCM03, Container: mycontainer, Endpoint: \n",
      "https://hcm03.vstorage.vngcloud.vn .\n",
      "•Create file credentials-velero with the following content:\n",
      "Install Velero on both source and destination clusters\n",
      "(Install Velero tool)\n",
      "On both Clusters (source and target)\n",
      "301•Install Velero CLI\u0000\n",
      "•Install Velero on your 2 clusters with the command:\n",
      "Attention:\n",
      "•When you perform a cluster migration from VKS to VKS, we\n",
      "recommend that you use Snapshot to migrate your Volume from the\n",
      "source cluster to the destination cluster.\n",
      "•Install the VNGCloud Snapshot Controller plugin on 2 clusters with the\n",
      "command:\n",
      "Copy[default]\n",
      "aws_access_key_id=________________________\n",
      "aws_secret_access_key=________________________\n",
      "curl -OL https://github.com/vmware-tanzu/velero/releases/download/v1.1\n",
      "tar -xvf velero-v1.13.2-linux-amd64.tar.gz\n",
      "cp velero-v1.13.2-linux-amd64/velero /usr/local/bin\n",
      "velero install --provider aws \\\n",
      "  --plugins velero/velero-plugin-for-aws:v1.9.0,velero/velero-plugin-f\n",
      "  --secret-file ./credentials-velero \\\n",
      "  --bucket ________________________ \\\n",
      "  --backup-location-config region=hcm03,s3ForcePathStyle=\"true\",s3Url=\n",
      "  --use-node-agent \\\n",
      "  --features=EnableCSI\n",
      "velero client config set features=EnableCSI\n",
      "helm repo add vks-helm-charts https://vngcloud.github.io/vks-helm-char\n",
      "helm repo update\n",
      "helm install vngcloud-snapshot-controller vks-helm-charts/vngcloud-sna\n",
      "  --replace --namespace kube-system\n",
      "302•Annotate the Persistent Volumes that need to be backed up. By default, Velero\n",
      "will not backup volume. You can run the command below to annotate backup of\n",
      "all volumes.\n",
      "•Additionally, you can mark not to backup system resources with the following\n",
      "command:\n",
      "•Apply the file below to create the default VolumeSnapshotClass:\n",
      "•Perform backup according to the syntax:./velero_helper.sh mark_volume -c\n",
      "./velero_helper.sh mark_exclude -c\n",
      "apiVersion: snapshot.storage.k8s.io/v1\n",
      "kind: VolumeSnapshotClass\n",
      "metadata:\n",
      "  name: vngcloud-vsclass\n",
      "  labels:\n",
      "    velero.io/csi-volumesnapshot-class: \"true\"\n",
      "driver: bs.csi.vngcloud.vn\n",
      "deletionPolicy: Delete\n",
      "# user can choose the VolumeSnapshotClass by setting annotation velero\n",
      "# user can choose the VolumeSnapshotClass by setting annotation velero\n",
      "velero backup create vks-full-backup \\\n",
      "  --exclude-namespaces velero \\\n",
      "  --include-cluster-resources=true \\\n",
      "  --wait\n",
      "velero backup describe vks-full-backup --detailsAt the source Cluster\n",
      "303•Perform restore according to the command:\n",
      "velero restore create --from-backup vks-full-backup \\\n",
      "    --exclude-resources=\"MutatingWebhookConfiguration,ValidatingWebhoo\n",
      "velero restore create --from-backup vks-full-backupAt the destination Cluster\n",
      "304Migration Cluster from\n",
      "vContainer to VKS\n",
      "Tools used: Velero + vStorage (s3)\n",
      "Process of migrating from vContainer to vKS \u0000Customer + VNG Cloud)\n",
      "Step 1\u0000 Evaluate the current version of the vContainer cluster and the\n",
      "corresponding vKS cluster to migrate. At this point, there will be 2 cases (step 2\n",
      "and step 3\u0000\n",
      "Step 2\u0000 If the vContainer version is lower than the versions currently supported by\n",
      "vKS, then the Custom Resources \u0000CRDs\u0000 need to be reviewed for compatibility with\n",
      "the new kubernetes versions.\n",
      "•If compatible with the new vKS version, continue with step 3.\n",
      "1. Process\n",
      "305•If not compatible, need to manually update and reconfigure CRD as well as\n",
      "related Applications.\n",
      "Step 3\u0000 If the vContainer version is supported by vKS \u00001.27, 1.28 and 1.29\u0000, then you\n",
      "need to check the resources on vContainer before backing up. You need to pay\n",
      "attention to the following types of resources:\n",
      "•PV/PVC\u0000 Velero does not support backup with hostPath type, only Local type is\n",
      "supported.\n",
      "•Ingress resources : Ingress resources managed by container-ingress-nginx-\n",
      "controller will not work after migration.\n",
      "•Label and Taint nodes : Velero does not re-attach Labels and Taints to nodes in\n",
      "the vKS\n",
      "Step 4 : Backup resources on vContainer cluster\n",
      "Step 5 : Restore resources on vKS cluster\n",
      "Step 6 : Perform testing and adjustments\n",
      "Check the existing StorageClasses on vContainer and create corresponding\n",
      "StorageClasses on vKS. Then perform 1\u00001 mapping of StorageClasses between\n",
      "vContainer and vKS cluster.\n",
      "Example ConfigMap file in Step 3.\n",
      "Velero does not support hostPath volume backup, only local.2. Important Note:\n",
      "1. Mapping StorageClass on vKS cluster\n",
      "2. Use PersistentVolume as hostPath\n",
      "306For clusters using PV type hostPath, it is necessary to convert to local type as\n",
      "follows:\n",
      "Note: Need to delete and redeploy the Application using PV type hostPath\n",
      "Case 1\u0000 Type Local supports hostPath path\n",
      "For example, PV hostPath is configured to mount at the /opt/data folder, convert to\n",
      "Local type and mount back to Pods, according to the following form:\n",
      "307apiVersion: storage.k8s.io/v1 \n",
      "kind: StorageClass \n",
      "metadata: \n",
      "  name: manual \n",
      "provisioner: kubernetes.io/no-provisioner \n",
      "volumeBindingMode: WaitForFirstConsumer \n",
      "--- \n",
      "apiVersion: v1 \n",
      "kind: PersistentVolume \n",
      "metadata: \n",
      "  name: pv-volume \n",
      "  labels: \n",
      "    type: local \n",
      "spec: \n",
      "  storageClassName: manual \n",
      "  capacity: \n",
      "    storage: 1Gi \n",
      "  accessModes: \n",
      "    - ReadWriteOnce \n",
      "  local: \n",
      "    path: \"/opt/data\" \n",
      "  nodeAffinity: \n",
      "    required: \n",
      "      nodeSelectorTerms: \n",
      "308Case 2\u0000 Type Local does not support the original hostPath path\n",
      "When creating PV/PVC according to Case 1, Pod cannot mount PVC and an error\n",
      "appears\n",
      "MountVolume.NewMounter initialization failed for volume \"pvc\" : path\n",
      "\"/mnt/data\" does not exist\n",
      "Copy the data to a new folder (eg /var, /opt, /tmp, ...) and repeat Case 1, then mount\n",
      "the PVC into the Pod as usual:      - matchExpressions: \n",
      "        - key: kubernetes.io/hostname \n",
      "          operator: Exists \n",
      "--- \n",
      "apiVersion: v1 \n",
      "kind: PersistentVolumeClaim \n",
      "metadata: \n",
      "  name: pv-claim \n",
      "spec: \n",
      "  storageClassName: manual \n",
      "  accessModes: \n",
      "    - ReadWriteOnce \n",
      "  resources: \n",
      "    requests: \n",
      "      storage: 1Gi \n",
      "cp -R /mnt/data /var \n",
      "309Create a vStorage project, Container and corresponding S3 key to store backup\n",
      "data\n",
      "On both clusters:\n",
      "•Create a credentials-velero file with the following content:\n",
      "•Install Velero CLI\n",
      "•Install Velero on 2 kubernetes clusters[default] \n",
      "aws_access_key_id=________________________ # <= Adjust here \n",
      "aws_secret_access_key=________________________ # <= Adjust here \n",
      "curl -OL  https://github.com/vmware-tanzu/velero/releases/download/v1.14.\n",
      "tar -xvf velero-v1.14.1-linux-amd64.tar.gz \n",
      "cp velero-v1.14.1 -linux-amd64/velero /usr/local/bin 3. Detailed implementation steps\n",
      "Step 1: Install Velero on both clusters (vContainer and\n",
      "vKS)\n",
      "310•Annotate the Persistent Volumes to be backed up\n",
      "•Annotate non-backup resources of kube-system\n",
      "•Annotate other resources (not marked in the velero_helper.sh file), such as CSI,\n",
      "Ingress Controller, or other resources that you do not want to migrate (note that\n",
      "you need to label all resources of objects that do not need to be backed up).\n",
      "◦For example, an application includes DaemonSet, Deployment, Pod, ... then\n",
      "it is necessary to mark labels for all those resources.velero install \\ \n",
      "    --provider aws \\ \n",
      "    --plugins velero/velero-plugin-for-aws:v1.9.0 \\ \n",
      "    --use-node-agent \\ \n",
      "    --use-volume-snapshots=false \\ \n",
      "    --secret-file ./credentials-velero \\ \n",
      "    --bucket __________\\                    # <= Adjust here \n",
      "    --backup-location-config region=hcm03,s3ForcePathStyle=\"true\",s3Url=h\n",
      "./velero_helper.sh mark_volume --confirm \n",
      "./velero_helper.sh mark_exclude --confirm Step 2: Perform backup on vContainer Cluster\n",
      "311•Perform check and attach Labels and Taints on vContainer nodes to vKS nodes\n",
      "before restore\n",
      "•Create 2 backups for Cluster resources and Namespace resources according to\n",
      "the syntax# Thêm label velero.io/exclude-from-backup=true cho t ừ ng resources \n",
      " \n",
      "## V ớ i Cinder CSI \n",
      "kubectl -n kube-system label StatefulSet/csi-cinder-controllerplugin vele\n",
      "kubec kubectl -n kube-system label DaemonSet/csi-cinder-nodeplugin velero\n",
      " \n",
      "## V ớ i vcontainer-ingress-nginx-controller \n",
      "kubectl -n kube-system label Deployment/ vcontainer-ingress-nginx-control\n",
      "kubectl -n kube-system label Deployment/ vcontainer-ingress-nginx-default\n",
      "# Ki ể m tra các Label nodes \n",
      "./velero_helper.sh check_node_label \n",
      "# Ki ể m tra các Taint nodes \n",
      "./velero_helper.sh check_node_taint \n",
      "# Tạ o cluster resource backup \n",
      "velero backup create vcontainer-cluster --include-namespaces \"\"  --includ\n",
      " \n",
      "# Tạ o cluster namespace backup \n",
      "velero backup create vcontainer-namespace --exclude-namespaces velero --w\n",
      " \n",
      "# Xóa các b ả n backup (n ế u c ầ n) \n",
      "velero backup delete vcontainer-namespace vcontainer-cluster --confirm \n",
      "312•View created backups and details of backed up resources (note the STATUS of\n",
      "the backup)\n",
      "•If in the vContainer cluster using CSI is cinder.csi.openstack.org , need to\n",
      "perform StorageClass mapping between 2 clusters vContainer and vKS\n",
      "◦Mapping csi-sc-cinderplugin-nvme-5000 (vContainer) and vngcloud-\n",
      "nvme-5000-delete (vKS), similarly for other StorageClasses\n",
      "◦In vKS, it is necessary to create corresponding StorageClasses.\n",
      "Create sc-mapping.yaml file and apply on VKS cluster# List các b ả n backup đư ợ c t ạ o \n",
      "velero get backup \n",
      "# Chi ti ế t c ủ a m ộ t b ả n backup \n",
      "velero backup describe <bk-name> --details \n",
      "# Xem logs quá trình backup \n",
      "velero backup logs <bk-name> \n",
      "Step 3: Perform Restore on VKS Cluster\n",
      "313•Add permissions for Velero to restore data PersistentVolume\n",
      "Create add-permission.yaml file and apply on VKS clusterapiVersion: v1 \n",
      "kind: ConfigMap \n",
      "metadata: \n",
      "  name: change-storage-class-config \n",
      "  namespace: velero \n",
      "  labels: \n",
      "    velero.io/plugin-config: \"\" \n",
      "    velero.io/change-storage-class: RestoreItemAction \n",
      "data: \n",
      "  csi-sc-cinderplugin-nvme-5000: vngcloud-nvme-5000-delete \n",
      "  #_______old_storage_class_______: _______new_storage_class_______ # <= A\n",
      "314•Perform restore in order\n",
      "◦Note, for each restore, you need to check whether the restore process was\n",
      "successful or not before continuing to execute other commands.apiVersion: v1 \n",
      "kind: ConfigMap \n",
      "metadata: \n",
      "  name: fs-restore-action-config \n",
      "  namespace: velero \n",
      "  labels: \n",
      "    velero.io/plugin-config: \"\" \n",
      "    velero.io/pod-volume-restore: RestoreItemAction \n",
      "data: \n",
      "  secCtx: | \n",
      "    capabilities: \n",
      "      drop: [] \n",
      "      add: [] \n",
      "    allowPrivilegeEscalation: false \n",
      "    readOnlyRootFilesystem: true \n",
      "    runAsUser: 0 \n",
      "    runAsGroup: 0 \n",
      "# Ki ể m tra restore \n",
      "velero get restore \n",
      "velero describe restore <restore-name> --details \n",
      "315•In case of migrating to VKS and still using vcontainer-nginx-ingress-controller, it\n",
      "is necessary to change the Service type to LoadBalancervelero restore create --item-operation-timeout 1m --from-backup vcontaine\n",
      "velero restore create --item-operation-timeout 1m --from-backup vcontaine\n",
      "velero restore create --item-operation-timeout 1m --from-backup vcontaine\n",
      "kubectl patch service -n kube-system vcontainer-ingress-nginx-controller \n",
      "316Migrate Cluster from\n",
      "another platform to VKS\n",
      "To migrate a Cluster from the Cloud Provider or On-premise system to the VKS\n",
      "system, follow the steps in this document.\n",
      "•Perform download helper bash script and grand execute permission for this\n",
      "file ( velero_helper.sh )\n",
      "•\u0000Optional) Deploy some services to check the correctness of the migration.\n",
      "Suppose, at the source Cluster, I have deployed an nginx service as follows:\n",
      "◦Deployment files:Prerequisites\n",
      "317apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx\n",
      "  namespace: mynamespace\n",
      "  labels:\n",
      "    app: nginx\n",
      "spec:\n",
      "  ports:\n",
      "  - port: 80\n",
      "    name: web\n",
      "  selector:\n",
      "    app: nginx\n",
      "  type: NodePort\n",
      "---\n",
      "apiVersion: apps/v1\n",
      "kind: StatefulSet\n",
      "metadata:\n",
      "  name: web\n",
      "  namespace: mynamespace\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx\n",
      "  serviceName: \"nginx\"\n",
      "  replicas: 1\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx\n",
      "        image: nginx\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "          name: web\n",
      "        volumeMounts:\n",
      "        - name: disk-ssd\n",
      "          mountPath: /usr/share/nginx/html\n",
      "  volumeClaimTemplates:\n",
      "  - metadata:\n",
      "      name: disk-ssd\n",
      "      namespace: mynamespace\n",
      "    spec:\n",
      "      accessModes: [ \"ReadWriteOnce\" ]\n",
      "      storageClassName: standard-rwo\n",
      " resources:\n",
      "318•Now, when you access Node's Public IP, you will see \"Hello, MyVNGCloud\".\n",
      "On the VKS system, you need to initialize a Cluster according to the instructions \n",
      "here . Make sure that the destination cluster's configuration is the same as the\n",
      "source cluster's configuration.\n",
      "Attention:\n",
      "For the migration to be successful, on the target Cluster, you need to\n",
      "ensure the following requirements:\n",
      "•The amount of resources needed such as number of nodes, node\n",
      "instance configuration,...\n",
      "•Node labels and node taints are the same as the old cluster.\n",
      "•Corresponding or alternative Storage Class.\n",
      "Migrating private resources outside cluster (moving private resources outside the\n",
      "cluster) is the process of moving private resources outside the source Cluster to a\n",
      "place that the destination Cluster can use. For example, you may have private\n",
      "resources such as images, databases, etc. Now, before starting to migrate, you\n",
      "need to migrate these resources yourself. For example, if you need:      resources:\n",
      "        requests:\n",
      "          storage: 40Gi\n",
      "kubectl exec -n mynamespace -it web-0 bash\n",
      "cd /usr/share/nginx/html\n",
      "echo -e \"<html>\\n<head>\\n  <title>MyVNGCloud</title>\\n</head>\\n<body>\\\n",
      "Prepare target cluster (Prepare target resource)\n",
      "[Optional] Migrate private resources outside cluster\n",
      "319•Migrate Container Images: you can migrate images to VNGCloud Container\n",
      "Registry through instructions here .\n",
      "•Migrate Databases: you can use Relational Database Service \u0000RDS\u0000 and Object\n",
      "Storage Service \u0000OBS\u0000 depending on your needs. After the migration is\n",
      "complete, remember to reconfigure the database for your applications on VKS\n",
      "Cluster.\n",
      "•Migrate Storage: you can use vServer's NFS Server .\n",
      "Attention:\n",
      "•After you migrate resources outside the Cluster, you need to ensure\n",
      "the target Cluster can connect to these migrated resources.\n",
      "After you have migrated private resources outside the cluster, you can use the\n",
      "migration tool to backup and restore the application on the source cluster and\n",
      "target cluster.\n",
      "•Create a vStorage Project, Container to receive the cluster's backup data\n",
      "according to instructions here .\n",
      "•Create an S3 key corresponding to this vStorage Project according to the\n",
      "instructions here .\n",
      "For example, I have initialized a vStorage Project, Container with the following\n",
      "information: Region: HCM03, Container: mycontainer, Endpoint: \n",
      "https://hcm03.vstorage.vngcloud.vn .\n",
      "•Create file credentials-velero with the following content:\n",
      "Install Velero on both source and destination clusters\n",
      "(Install Velero tool)\n",
      "On both Clusters (source and target)\n",
      "320•Install Velero CLI\u0000\n",
      "•Install Velero on your 2 clusters with the command:\n",
      "•Annotate the Persistent Volumes that need to be backed up. By default, Velero\n",
      "will not backup volume. You can run the command below to annotate backup of\n",
      "all volumes.\n",
      "•Additionally, you can mark not to backup system resources with the following\n",
      "command:[default]\n",
      "aws_access_key_id=________________________\n",
      "aws_secret_access_key=________________________\n",
      "curl -OL https://github.com/vmware-tanzu/velero/releases/download/v1.1\n",
      "tar -xvf velero-v1.13.2-linux-amd64.tar.gz\n",
      "cp velero-v1.13.2-linux-amd64/velero /usr/local/bin\n",
      "velero install \\\n",
      "    --provider aws \\\n",
      "    --plugins velero/velero-plugin-for-aws:v1.9.0 \\\n",
      "    --use-node-agent \\\n",
      "    --use-volume-snapshots=false \\\n",
      "    --secret-file ./credentials-velero \\\n",
      "    --bucket ______________________________ \\\n",
      "    --backup-location-config region=hcm03,s3ForcePathStyle=\"true\",s3Ur\n",
      "./velero_helper.sh mark_volume -c\n",
      "./velero_helper.sh mark_exclude -cFor Clusters on Amazon Elastic Kubernetes Service\n",
      "(EKS)\n",
      "At the source Cluster\n",
      "321•Perform backup according to the syntax:\n",
      "Attention:\n",
      "•You must create 2 backup versions for Cluster Resource and\n",
      "Namespace Resource.\n",
      "•Create a Storage Class mapping file between source and destination Cluster:\n",
      "•Perform restore according to the command:velero backup create eks-cluster --include-namespaces \"\" \\\n",
      "  --include-cluster-resources=true \\\n",
      "  --wait\n",
      "velero backup create eks-namespace --exclude-namespaces velero \\\n",
      "    --wait\n",
      "apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: change-storage-class-config\n",
      "  namespace: velero\n",
      "  labels:\n",
      "    velero.io/plugin-config: \"\"\n",
      "    velero.io/change-storage-class: RestoreItemAction\n",
      "data:\n",
      "  _______old_storage_class_______: _______new_storage_class_______  # \n",
      "  _______old_storage_class_______: _______new_storage_class_______  # \n",
      "velero restore create --item-operation-timeout 1m --from-backup eks-cl\n",
      "    --exclude-resources=\"MutatingWebhookConfiguration,ValidatingWebhooAt the destination Cluster\n",
      "322•Annotate the Persistent Volumes and label resources that need to be excluded\n",
      "from the backup\n",
      "•Additionally, you can mark not to backup system resources with the following\n",
      "command:\n",
      "•Perform backup according to the syntax:\n",
      "Attention:\n",
      "•You must create 2 backup versions for Cluster Resource and\n",
      "Namespace Resource.velero restore create --item-operation-timeout 1m --from-backup eks-na\n",
      "velero restore create --item-operation-timeout 1m --from-backup eks-cl\n",
      "./velero_helper.sh mark_volume -c\n",
      "./velero_helper.sh mark_exclude -c\n",
      "velero backup create gke-cluster --include-namespaces \"\" \\\n",
      "  --include-cluster-resources=true \\\n",
      "  --wait\n",
      "velero backup create gke-namespace --exclude-namespaces velero \\\n",
      "    --wait\n",
      "For Cluster on Google Kubernetes Engine (GKE)\n",
      "At the source Cluster\n",
      "323•Create a Storage Class mapping file between source and destination Cluster:\n",
      "•Perform restore according to the command:\n",
      "Attention:\n",
      "•Google Kubernetes Engine \u0000GKE) does not allow daemonset\n",
      "deployment on all nodes. However, Velero only needs to deploy the\n",
      "daemonset on the node with the PV mount. The solution to this\n",
      "problem is that you can adjust the taint and toleration of the\n",
      "daemonset to only deploy it on the node with the PV mount.\n",
      "•You can change the default resource request cpu:500mand mem:512M\n",
      "in the installation step or make adjustments when deploying the yaml.apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: change-storage-class-config\n",
      "  namespace: velero\n",
      "  labels:\n",
      "    velero.io/plugin-config: \"\"\n",
      "    velero.io/change-storage-class: RestoreItemAction\n",
      "data:\n",
      "  _______old_storage_class_______: _______new_storage_class_______  # \n",
      "  _______old_storage_class_______: _______new_storage_class_______  # \n",
      "velero restore create --item-operation-timeout 1m --from-backup gke-cl\n",
      "    --exclude-resources=\"MutatingWebhookConfiguration,ValidatingWebhoo\n",
      "velero restore create --item-operation-timeout 1m --from-backup gke-na\n",
      "velero restore create --item-operation-timeout 1m --from-backup gke-cl\n",
      "At the destination Cluster\n",
      "324Migrate Limitation\n",
      "When using Velero to migrate Cluster to Cluster, you can add the following options.\n",
      "To mark the Volumes you want to backup and unnecessary resources, first you\n",
      "need to download the bash helper script we provide and perform grand execute\n",
      "permission. You can see details of the tab file at: velero_helper.sh\n",
      "Since Velero does not support backing up hostPath Volume, you need to convert\n",
      "hostPath Volume to Persistent Volume according to the following instructions:\n",
      "•To list hostPath Volumes in use:\n",
      "All data Persistent Volumes are stored on vStorage. Need to add annotation for all\n",
      "pods using PV with volume name:\n",
      "backup.velero.io/backup-volumes=volume1,volume2\n",
      "•Or you can automatically find volumes by:./helper.sh check_hostPath\n",
      "./helper.sh mark_volumeMark the Volumes you want to backup and\n",
      "unnecessary resources\n",
      "1. Convert hostPath Volume to Persistent Volume to be\n",
      "able to perform backup\n",
      "2. Mark Persistent Volume to include in backup\n",
      "3. Mark resources in exclude in backup\n",
      "325Because VKS operates under the Fully Managed Control Plane mechanism, you do\n",
      "not need to backup resources such as: calico, kube-dns, kube-scheduler, \n",
      "kube-apiserver,... In addition, vContainer resources such as: \n",
      "magnum-auto-healer, cluster-autoscaler, csi-cinder,... will also be ignored.\n",
      "•Identify resources that do not need backup via the command:\n",
      "When performing a migration, it is possible that the resources in the source Cluster\n",
      "are using labels and taints. You need to ensure these important labels and taints\n",
      "exist in the target Cluster.\n",
      "•Check lable and taint via command:\n",
      "•If your Storage Class is different between the source Cluster and the destination\n",
      "Cluster, you need to transfer the Storage Class between the two clusters. For\n",
      "example:\n",
      "◦At the source Cluster, you have the following 2 Storage Classes:\n",
      "•You can create a mapping file with content like the example below to convert 2\n",
      "storage classes from the source Cluster into 2 storage classes at the\n",
      "destination Cluster. This file must be applied at the target Cluster before you run\n",
      "the backup command:./helper.sh mark_exclude\n",
      "./helper.sh check_node_label\n",
      "./helper.sh check_node_taint\n",
      "@ kubectl get sc\n",
      "NAME                            PROVISIONER       RECLAIMPOLICY   V\n",
      "sc-iops-200-retain (default)    csi.vngcloud.vn   Retain          I\n",
      "sc-ssd-10000-delete (default)   csi.vngcloud.vn   Delete          I4. Check label and taint of node\n",
      "5. Mapping Storage Class\n",
      "326apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: change-storage-class-config\n",
      "  namespace: velero\n",
      "  labels:\n",
      "    velero.io/plugin-config: \"\"\n",
      "    velero.io/change-storage-class: RestoreItemAction\n",
      "data:\n",
      "  sc-iops-200-retain: ssd-200\n",
      "  sc-ssd-10000-delete: ssd-10000\n",
      "327Working VKS with Terraform\n",
      "Terraform is an open source infrastructure as code tool that allows users to manage\n",
      "their infrastructure easily and efficiently across different cloud platforms, such as\n",
      "VNG Cloud, AWS, Google Cloud and Azure. Terraform Server refers to the instance\n",
      "of the Terraform engine running on a specific server or machine. This is where\n",
      "infrastructure code is written and executed, allowing users to create, modify, and\n",
      "destroy resources on the cloud platform.\n",
      "Terraform itself does not have a graphical user interface, instead users interact with\n",
      "it using a command line interface. Terraform requires a cloud provider account and\n",
      "key to be configured along with a Terraform configuration file to execute the\n",
      "infrastructure as code. Additionally, Terraform can operate in clustered\n",
      "environments where multiple users can collaborate on the same infrastructure\n",
      "codebase, making it a powerful and flexible tool for infrastructure management.\n",
      "cloud.\n",
      "To initialize a Kubernetes Cluster using Terraform, you need to perform the\n",
      "following steps:\n",
      "1.Access the IAM Portal here , create a Service Account with VKS Full Access\n",
      "authority . Specifically, at the IAM site, you can:\n",
      "•Select \" Create a Service Account \", enter a name for the Service Account\n",
      "and click Next Step to assign permissions to the Service Account.\n",
      "•Find and select Policy: VKSFullAccess then click \" Create a Service\n",
      "Account \" to create a Service Account, Policy: VKSFullAccess is created\n",
      "by VNG Cloud, you cannot delete these policies.What is Terraform?\n",
      "Implementation steps\n",
      "328•After successful creation, you need to save the Client_ID and Secret_Key of\n",
      "the Service Account to perform the next step.\n",
      "2.Access the VKS Portal here , Activate the VKS service on the Overview tab.\n",
      "Please wait until we successfully create your VKS account.\n",
      "3.Install Terraform:\n",
      "•Download and install Terraform for your operating system from \n",
      "https://developer.hashicorp.com/terraform/install .\n",
      "4.Initialize Terraform configuration:\n",
      "•Create a file variable.tfand declare Service Account information in this\n",
      "file.\n",
      "•Create a file main.tfand define the Kubernetes Cluster resources you\n",
      "want to create.\n",
      "For example:\n",
      "•The file variable.tf:you need to replace the Client ID and Client Secret\n",
      "created in step 1 in this file.\n",
      "•On the main.tf file , you need to be able to add resources to create a Cluster/\n",
      "Node Group:\n",
      "◦Create independent Cluster my-vks-cluster and Node Group my-nodegroup:variable \"client_id\" {\n",
      "  type = string\n",
      "  default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
      "}\n",
      "variable \"client_secret\" {\n",
      "  type = string\n",
      "  default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
      "}\n",
      "329•Create Cluster with Default Node Group\n",
      "Attention:\n",
      "•We recommend that you create and manage Clusters and Node\n",
      "Groups as separate resources, as in the example below. This allows\n",
      "you to add or remove Node Groups without recreating the entire\n",
      "Cluster. If you declare Node Group Default directly in the\n",
      "vngcloud_vks_cluster resource, you cannot delete them without\n",
      "recreating the Cluster itself.\n",
      "•In the main.tf file, to successfully create a cluster with a node group,\n",
      "you must enter information in the following 4 fields:resource \"vngcloud_vks_cluster\" \"primary\" {\n",
      "  name      = \"my-cluster\"\n",
      "  cidr      = \"172.16.0.0/16\"\n",
      "  vpc_id    = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "}\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" {\n",
      "  name= \"my-nodegroup\"\n",
      "  ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  cluster_id= vngcloud_vks_cluster.primary.id\n",
      "}\n",
      "resource \"vngcloud_vks_cluster\" \"primary\" {\n",
      "  name      = \"my-cluster\"\n",
      "  cidr      = \"172.16.0.0/16\"\n",
      "  vpc_id    = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  node_group {\n",
      "    name= \"my-nodegroup\"\n",
      "    ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  }\n",
      "}\n",
      "  vpc_id    = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "330Example 1\u0000\n",
      "Below is the main.tf file I used to initialize the Cluster with the following parameters:\n",
      "•Cluster name: cluster-demo\n",
      "•K8S Version: v1.29.1\n",
      "•Mode: Public Cluster and Public Node Group\n",
      "•Node Group name: nodegroup1\n",
      "•Initial Node: 3\n",
      "•Turn on AutoScaling: scale from 0 to 5 nodes\n",
      "331terraform {\n",
      "  required_providers {\n",
      "    vngcloud = {\n",
      "      source  = \"vngcloud/vngcloud\"\n",
      "      version = \"1.2.2\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "provider \"vngcloud\" {\n",
      "  token_url        = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/to\n",
      "  client_id        = var.client_id\n",
      "  client_secret    = var.client_secret\n",
      "  vserver_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vserver-gatew\n",
      "  vlb_base_url     = \"https://hcm-3.api.vngcloud.vn/vserver/vlb-gateway\"\n",
      "}\n",
      "resource \"vngcloud_vks_cluster\" \"primary\" {\n",
      "  name      = \"cluster-demo\"\n",
      "  description = \"Cluster create via terraform\"\n",
      "  version = \"v1.29.1\"\n",
      "  cidr      = \"172.16.0.0/16\"\n",
      "  enable_private_cluster = false\n",
      "  network_type = \"CALICO\"\n",
      "  vpc_id    = \"net-70ef12d4-d619-43fc-88f0-1c1511683123\"\n",
      "  subnet_id = \"sub-0725ef54-a32e-404c-96f2-34745239c123\"\n",
      "  enabled_load_balancer_plugin = true\n",
      "  enabled_block_store_csi_plugin = true\n",
      "}\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" {\n",
      "  cluster_id = vngcloud_vks_cluster.primary.id\n",
      "  name = \"nodegroup1\"\n",
      "  num_nodes = 3\n",
      "  auto_scale_config {\n",
      "    min_size = 0\n",
      "    max_size = 5\n",
      "  }\n",
      "  upgrade_config {\n",
      "    strategy = \"SURGE\"\n",
      "    max_surge = 1\n",
      "    max_unavailable = 0\n",
      "  }\n",
      "  image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\"\n",
      "  flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6d123\"\n",
      "  disk_size = 50\n",
      "  disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\"\n",
      "enableprivatenodes=false\n",
      "332Example 2\n",
      "Below is the main.tf file I used to initialize the Cluster with the following parameters:\n",
      "•Cluster name: my-cluster\n",
      "•K8S Version: v1.29.1\n",
      "•Mode: Public Cluster and Private Node Group\n",
      "•Node Group name: my-nodegroup\n",
      "•Initial Node: 3 nodes\n",
      "•Turn on AutoScaling: scale from 0 to 5 nodes\n",
      "First, apply the main file according to the following structure:  enable_private_nodes  false\n",
      "  ssh_key_id= \"ssh-f923c53c-cba7-4131-9f86-175d04ae2123\"\n",
      "  security_groups = [\"secg-faf05344-fbd6-4f10-80a2-cda08d15ba5e\"]\n",
      "  labels = {\n",
      "    \"test\" = \"terraform\"\n",
      "  }\n",
      "  taint {\n",
      "    key    = \"key1\"\n",
      "    value  = \"value1\"\n",
      "    effect = \"PreferNoSchedule\"\n",
      "  }\n",
      "}\n",
      "333terraform {\n",
      "  required_providers {\n",
      "    vngcloud = {\n",
      "      source  = \"vngcloud/vngcloud\"\n",
      "      version = \"1.2.2\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "provider \"vngcloud\" {\n",
      "  token_url        = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/to\n",
      "  client_id        = var.client_id\n",
      "  client_secret    = var.client_secret\n",
      "  vserver_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vserver-gatew\n",
      "  vlb_base_url     = \"https://hcm-3.api.vngcloud.vn/vserver/vlb-gateway\"\n",
      "}\n",
      "resource \"vngcloud_vks_cluster\" \"primary\" {\n",
      "  name      = \"my-cluster\"\n",
      "  description = \"VNGCLOUD uses terraform\"\n",
      "  version = \"v1.29.1\"\n",
      "  cidr      = \"172.16.0.0/16\"\n",
      "  enable_private_cluster = false\n",
      "  network_type = \"CALICO\"\n",
      "  vpc_id    = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  enabled_load_balancer_plugin = true\n",
      "  enabled_block_store_csi_plugin = true\n",
      "}\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" {\n",
      "  cluster_id= vngcloud_vks_cluster.primary.id\n",
      "  name= \"my-nodegroup\"\n",
      "  num_nodes = 3\n",
      "  auto_scale_config {\n",
      "    min_size = 0\n",
      "    max_size = 5\n",
      "  }\n",
      "  upgrade_config {\n",
      "    strategy = \"SURGE\"\n",
      "    max_surge = 1\n",
      "max_unavailable = 0\n",
      "  }\n",
      "  image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\"\n",
      "  flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6dc4b\"\n",
      "  disk_size = 20\n",
      "  disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\"\n",
      "enableprivatenodes=true\n",
      "334Then, if you need to add Whitelist IP for Control Plane, add this field to the main.tf\n",
      "file and reapply this file:  enable_private_nodes  true\n",
      "  ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  labels = {\n",
      "    \"mylabel\" = \"vngcloud\"\n",
      "  }\n",
      "  taint {\n",
      "    key    = \"mykey\"\n",
      "    value  = \"myvalue\"\n",
      "    effect = \"PreferNoSchedule\"\n",
      "  }\n",
      "}\n",
      "335terraform {\n",
      "  required_providers {\n",
      "    vngcloud = {\n",
      "      source  = \"vngcloud/vngcloud\"\n",
      "      version = \"1.2.2\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "provider \"vngcloud\" {\n",
      "  token_url        = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/to\n",
      "  client_id        = var.client_id\n",
      "  client_secret    = var.client_secret\n",
      "  vserver_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vserver-gatew\n",
      "  vlb_base_url     = \"https://hcm-3.api.vngcloud.vn/vserver/vlb-gateway\"\n",
      "}\n",
      "resource \"vngcloud_vks_cluster\" \"primary\" {\n",
      "  name      = \"my-cluster\"\n",
      "  description = \"VNGCLOUD uses terraform\"\n",
      "  version = \"v1.29.1\"\n",
      "  cidr      = \"172.16.0.0/16\"\n",
      "  white_list_node_cidr = \"172.25.32.1/16\"\n",
      "  enable_private_cluster = false\n",
      "  network_type = \"CALICO\"\n",
      "  vpc_id    = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  enabled_load_balancer_plugin = true\n",
      "  enabled_block_store_csi_plugin = true\n",
      "}\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" {\n",
      "  cluster_id= vngcloud_vks_cluster.primary.id\n",
      "  name= \"my-nodegroup\"\n",
      "  num_nodes = 3\n",
      "  auto_scale_config {\n",
      "    min_size = 0\n",
      "    max_size = 5\n",
      "  }\n",
      "  upgrade_config {\n",
      "    strategy = \"SURGE\"\n",
      "    max_surge = 1\n",
      "max_unavailable = 0\n",
      "  }\n",
      "  image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\"\n",
      "  flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6dc4b\"\n",
      "  disk_size = 20\n",
      "disktype=\"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\"\n",
      "336Attention:\n",
      "•To get the image_id you want to use, you can access VKS Portal,\n",
      "select System Image menu and get the ID you want or get this\n",
      "information here .\n",
      "•To get the flavor_id you want to use for your Node group, please get\n",
      "the ID here .\n",
      "•After completing the above information, run the command below:\n",
      "•Then, to see the changes that will be applied to the resources that terraform is\n",
      "managing, you can run:\n",
      "•Finally, you choose to run the command line:\n",
      "•Select YES to initiate Cluster and Node Group via Terraform  disk_type  vtype61c3fc5bf4e945b489578aa7b6029018\n",
      "  enable_private_nodes = true\n",
      "  ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  labels = {\n",
      "    \"mylabel\" = \"vngcloud\"\n",
      "  }\n",
      "  taint {\n",
      "    key    = \"mykey\"\n",
      "    value  = \"myvalue\"\n",
      "    effect = \"PreferNoSchedule\"\n",
      "  }\n",
      "}\n",
      "terraform init\n",
      "terraform plan\n",
      "terraform applyLaunch Terraform command\n",
      "337After successfully initializing Terraform, you can go to VKS Portal to view the newly\n",
      "created Cluster information.\n",
      "See more about how to use Terraform to work with VKS here .Check the newly created Cluster on the\n",
      "VNG Cloud Portal interface\n",
      "338Monitoring\n",
      "339Metrics\n",
      "You can install vMonitor Platform Metric Agent into your Kubernetes Cluster to\n",
      "collect and push metrics to the vMonitor Platform site, then use the features at\n",
      "vMonitor Platform to centrally manage resources and monitor unusual activity of\n",
      "your Kubernetes Cluster. .\n",
      "Preparation steps before installation\n",
      "1. Check that you have the Metric Quota and that your quota has not reached the\n",
      "limit. If you do not have it, you need to buy the Metric Quota here .\n",
      "2. Create a Service Account and attach policy: vMonitorMetricPush to have\n",
      "enough rights to push Metric to vMonitor\n",
      "To create a service account, go here , then perform the following steps:\n",
      "•Select \" Create a Service Account \", enter a name for the Service Account and\n",
      "click Next Step to assign permissions to the Service Account\n",
      "•Find and select Policy: vMonitorMetricPush, then click \" Create a Service\n",
      "Account \" to create a Service Account, Policy: vMonitorMetricPush created by\n",
      "VNG Cloud only contains the correct permission to push metrics to the system.\n",
      "•After successful creation, you need to save the Client_ID and Secret_Key to\n",
      "perform the next step.\n",
      "Install helm on Debian/Ubuntu server\n",
      "1. You need to install Helm on a server with kubeconfig containing enough\n",
      "permissions to interact with the Kubernetes Cluster.\n",
      "•Check permissions with kubectl command :\n",
      "Kube checks permissionInstall Metric Agent using Helm\n",
      "340•If the result of the above commands is YES , then you have enough rights.\n",
      "2. Proceed to install Helm\n",
      "•Execute the following commands:\n",
      "•Check that Helm has been installed successfully\n",
      "Install Helm in other operating systems\n",
      "Refer to the installation instructions here: Install Helm Through Package Managers\n",
      "3. Some additional notes about Helm: (see details at: helm docs )\n",
      "•Helm will use kubeconfig to interact with the cluster, by default helm will use\n",
      "config in the path: \"~/.kube/config\"\n",
      "•If we need to change the path to kubeconfig we can use 2 ways:\n",
      "◦For each helm command, add the --kubeconfig option: for example, helm\n",
      "install --kubeconfig <path_to_kubeconfig>\n",
      "◦Declare environment variable: KUBECONFIG# Lệ nh dùng đ ể  ki ể m tra quy ề n t ươ ng tác t ấ t c ả  resource t ạ i namespace def\n",
      "kubectl auth can-i '*' '*'\n",
      "# Lệ nh dùng đ ể  ki ể m tra quy ề n t ươ ng tác t ấ t c ả  resource t ạ i namespace ch ỉ  \n",
      "kubectl auth can-i -n <namespace_ch ỉ _ đ ị nh> '*' '*'\n",
      "# Lệ nh dùng đ ể  ki ể m tra quy ề n t ạ o clusterrole và clusterrolebinding\n",
      "kubectl auth can-i create clusterrole\n",
      "kubectl auth can-i create clusterrolebinding\n",
      "curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /us\n",
      "sudo apt-get install apt-transport-https --yes \n",
      "echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyring\n",
      "sudo apt-get update \n",
      "sudo apt-get install helm \n",
      "helm version\n",
      "# Kế t qu ả  mong mu ố n c ủ a command \n",
      "# version.BuildInfo{Version:\"v3.11.0\", GitCommit:\"472c5736ab01133de504a82\n",
      "341Install Metric Agent\n",
      "By default, when installing vMonitor Platform Metric Agent, there will be 2\n",
      "components:\n",
      "•Deployment Agent kube-state-metrics: collect metrics for each resource of k8s\n",
      "cluster (pod, daemonset, deployment, replicaset, ....)\n",
      "•Daemonset Agent: collect metrics for each k8s cluster node \u0000CPU, Memory\n",
      "usage, ...)\n",
      "1. Add Helm vMonitor Platform Repo\n",
      "2. Install charts\n",
      "•Check and delete related resources before installation to avoid conflicts\n",
      "•Install at default namespace (add flag -n <namespace_specified> to install\n",
      "agent at another namespace)\n",
      "•\u0000YOUR_CLIENT_ID_XXXXXXXXXXXXXXXXXXX\u0000,\n",
      "\u0000YOUR_CLIENT_SECRET_XXXXXXXXXXXXXXX\u0000\u0000 Service account information\n",
      "created in the preparation step\n",
      "•\u0000CLUSTER_NAME\u0000\u0000 This information will be used to filter hosts of k8s cluster in\n",
      "case there are many clusters to monitor\n",
      "•Check that the agent installation was successfulhelm repo add vmonitor-platform https://vngcloud.github.io/helm-charts-vm\n",
      "helm repo update\n",
      "# Get Clusterrole vmonitor metric agent\n",
      "kubectl get clusterrole | grep vmonitor-metric-agent\n",
      "# Th ự c hi ệ n xóa resource n ế u có t ồ n t ạ i\n",
      "kubectl delete clusterrole vmonitor-metric-agent\n",
      "helm install vmonitor-metric-agent vmonitor-platform/vmonitor-metric-agen\n",
      " --set vmonitor.iamClientID=<YOUR_CLIENT_ID_XXXXXXXXXXXXXXXXXXX> \\\n",
      " --set vmonitor.iamClientSecret=<YOUR_CLIENT_SECRET_XXXXXXXXXXXXXXX> \\\n",
      " --set vmonitor.clusterName=<CLUSTER_NAME> \n",
      "342•If the pods agent is not in running state, use the corresponding command to\n",
      "check for errors\n",
      "After the installation is complete, kubernetes tracking metrics have been pushed\n",
      "to the vMonitor Platform site, you can proceed to vMonitor to draw dashboards\n",
      "and widgets.\n",
      "Uninstall Metric Agent\n",
      "Execute the following command to delete the installed related k8s resources:\n",
      "Metric Agent installation does not use kube-state-metrics\n",
      "•Install at default namespace (add flag -n <namespace_specified> to install\n",
      "agent at another namespace)# Ch ạ y command và đ ả m b ả o output là các pods ở  tr ạ ng thái running\n",
      "kubectl get pod | grep \"vmonitor-metric-agent\"\n",
      "# Ch ạ y command n ế u pod ở  tr ạ ng thái Pending\n",
      "kubectl describe pod <vmonitor-metric-agent-node-name>\n",
      "# Ch ạ y command n ế u pod ở  tr ạ ng thái CrashLoopBackOff and Error\n",
      "kubectl logs <vmonitor-metric-agent-node-name>\n",
      "# Sau đ ó ki ể m tra logs xu ấ t hi ệ n t ạ i agent\n",
      "helm uninstall vmonitor-metric-agent\n",
      "helm install vmonitor-metric-agent vmonitor-platform/vmonitor-metric-agen\n",
      " --set vmonitor.iamClientID=<YOUR_CLIENT_ID_XXXXXXXXXXXXXXXXXXX> \\\n",
      " --set vmonitor.iamClientSecret=<YOUR_CLIENT_SECRET_XXXXXXXXXXXXXXX> \\\n",
      " --set vmonitor.clusterName=<CLUSTER_NAME> \\\n",
      " --set vmonitor.kubeStateMetricsEnabled=false \\\n",
      " --set kubeStateMetricsAgent.enabled=false \n",
      "343Charging Fee\n",
      "For the Procuracy, the Managed Control Plane cost is completely free. You only\n",
      "need to pay for other resources that you actually use, including:\n",
      "•All nodes present in the Cluster \u0000VM\u0000. Details on how to calculate the price of\n",
      "vServer can be found here .\n",
      "•Load Balancer is integrated into your Cluster. Details on how to calculate the\n",
      "price of Load Balancer can be found here .\n",
      "•Persistent Volume, Snapshot integrated into your Cluster. Details on how to\n",
      "calculate Volume prices can be found here .\n",
      "Attention:\n",
      "•To ensure your Cluster operates stably, we have automatically set up\n",
      "Auto-renew for all resources on your Cluster. Before the expiration\n",
      "date of the resources, make sure your credit balance is enough for the\n",
      "system to auto-renew successfully.\n",
      "344Reference\n",
      "345Kubernetes versions\n",
      "Currently, the VKS system is providing you with 3 Kubernetes versions including:\n",
      "Version Timeline Attachment\n",
      "Vesion 1.29.1 2025\u000002\u000028 https://github.com/kubernetes/kubernetes/\n",
      "blob/master/CHANGELOG/CHANGELOG\u0000\n",
      "1.29.md#v1291\n",
      "Version 1.28.8 2024\u000010\u000028 https://github.com/kubernetes/kubernetes/\n",
      "blob/master/CHANGELOG/CHANGELOG\u0000\n",
      "1.28.md#v1288\n",
      "Version 1.27.12 2024\u000006\u000028 https://github.com/kubernetes/kubernetes/\n",
      "blob/master/CHANGELOG/CHANGELOG\u0000\n",
      "1.27.md#v12712\n",
      "346Node Flavors\n",
      "Below is a list of Flavors currently supported by VKS\u0000\n",
      "•General Code A\n",
      "Flavor Name CPU Memory Flavor ID\n",
      "a-general-2\u00004 2 4 flav-305a67bf-\n",
      "1825\u00004e3f-bc72\u0000\n",
      "d41afa160d93\n",
      "a-general-4\u00008 4 8 flav-9e88cfb4\u0000\n",
      "ec31\u00004ad4\u00008ba5\u0000\n",
      "243459f6dc4b\n",
      "a-general-8\u000016 8 16 flav-2fec3902\u0000\n",
      "4b71\u0000489c-a294\u0000\n",
      "19bad1df3a70\n",
      "a-general-12\u00002412 24 flav-ea60643c-\n",
      "36f0\u00004fd2-be8d-\n",
      "e42198d6320e\n",
      "a-general-12\u000024\u0000\n",
      "n1012 24 flav-0db2d13d-\n",
      "530b-4312-b2be-\n",
      "cdb3b80c73e4\n",
      "a-general-8\u000016\u0000\n",
      "n108 16 flav-a04a8e9c-\n",
      "8def-4fde-9bfc-\n",
      "2ba738576463\n",
      "a-general-16\u000032\u0000\n",
      "n1016 32 flav-65701e64\u0000\n",
      "471f-4747-a837\u0000\n",
      "500651b4c67d\n",
      "a1-standard-2\u000082 8 flav-edd3a4bd-\n",
      "ce5a-4b72\u00009323\u0000\n",
      "468d58615b68\n",
      "a1-standard-4\u0000164 16 flav-4cb926dc-\n",
      "63be-4edc-8a21\u0000\n",
      "4a66d963b45b\n",
      "347a1-standard-8\u0000328 32 flav-b834f30b-\n",
      "8dad-4d7e-8b71\u0000\n",
      "952824bfef23\n",
      "a1-standard-16\u00006416 64 flav-7f6e15db-\n",
      "191c-4c89\u000080da-\n",
      "70c74ab5f786\n",
      "a1-standard-\n",
      "32\u000012832 128 flav-67617e54\u0000\n",
      "48a5\u00004aed-9213\u0000\n",
      "4560dc0cd9c1\n",
      "a1-standard-\n",
      "48\u000019248 192 flav-778bfd0f-\n",
      "ad5a-4c83-b44a-\n",
      "7a1f2f855240\n",
      "a1-standard-8\u000032\u0000\n",
      "n108 32 flav-f31884f4\u0000\n",
      "e8b9\u00004f9a-9de5\u0000\n",
      "41307afa5960\n",
      "a1-standard-\n",
      "16\u000064-n1016 64 flav-c398e0a9\u0000\n",
      "a153\u00004826\u00008bd5\u0000\n",
      "4d715d3e5032\n",
      "a1-standard-\n",
      "32\u0000128-n1032 128 flav-2ac60541\u0000\n",
      "046e-4761-b40c-\n",
      "990012597e09\n",
      "a1-standard-\n",
      "48\u0000192-n1048 192 flav-bde154c4\u0000\n",
      "52c9\u00004e92\u00009495\u0000\n",
      "7fd7af3080c8\n",
      "a1-highmem-2\u0000162 16 flav-7615377f-\n",
      "909c-4e1d-9512\u0000\n",
      "bc3b639a3777\n",
      "a1-highmem-4\u0000324 32 flav-dc274c6a-\n",
      "ef9a-4aff-9966\u0000\n",
      "adecfac3731f\n",
      "a1-highmem-8\u0000648 64 flav-3e3eb1c2\u0000\n",
      "9f38\u000047f3-b2a1\u0000\n",
      "878ce28c160b\n",
      "a1-highmem-\n",
      "16\u000012816 128 flav-618a1e2c-\n",
      "c3eb-48e8-af13\u0000\n",
      "17661ce8e37c\n",
      "348•General Code Sa1-highmem-\n",
      "32\u000025632 256 flav-9f9421c3\u0000\n",
      "8956\u000043fb-a9c5\u0000\n",
      "2563d5316fb9\n",
      "a1-highmem-\n",
      "8\u000064-n108 64 flav-f51b4591\u0000\n",
      "7e2d-4a2f-93ce-\n",
      "7a0b2a4827db\n",
      "a1-highmem-\n",
      "16\u0000128-n1016 128 flav-f9e6b4a4\u0000\n",
      "a802\u00004f86\u00009adb-\n",
      "fcb746fe62e6\n",
      "a1-highmem-\n",
      "32\u0000256-n1032 256 flav-3f52df69\u0000\n",
      "638b-4363\u0000836c-\n",
      "4b8ff9622080\n",
      "a1-highcpu-4\u00004 4 4 flav-3bb2088e-\n",
      "5c17\u00004780\u000093c8\u0000\n",
      "edea2b5bc1d6\n",
      "a1-highcpu-8\u00008 8 8 flav-4c4cb707\u0000\n",
      "3e85\u00004a17\u00008891\u0000\n",
      "485985b10c0a\n",
      "a1-highcpu-16\u00001616 16 flav-67489c98\u0000\n",
      "a620\u0000466a-9449\u0000\n",
      "5f08dfdebec3\n",
      "a1-highcpu-\n",
      "32\u000032-n1032 32 flav-5f84e226\u0000\n",
      "833b-4ab8-a797\u0000\n",
      "7653dae9a764\n",
      "a1-highcpu-16\u000016\u0000\n",
      "n1016 16 flav-9ef49395\u0000\n",
      "9628\u00004a50\u000094fd-\n",
      "56c3aeef5065\n",
      "a1-highcpu-\n",
      "32\u000064-n1032 64 flav-bf77e8d1\u0000\n",
      "8ec4\u000042e1-aca4\u0000\n",
      "bb4a703b67d8\n",
      "eci.ins.s-general-2\u00004 2\n",
      "349eci.ins.s-general-4\u00008 4\n",
      "eci.ins.s-general-8\u000016 8\n",
      "eci.ins.s-general-16\u000032 16\n",
      "s-general-2\u00004 2\n",
      "s-general-4\u00008 4\n",
      "350s-general-8\u000016 8\n",
      "s-general-16\u000032 16\n",
      "s-general-16\u000032-n10 16\n",
      "s-general-8\u000016-n10 8\n",
      "s1-standard-2\u00008 2\n",
      "s1-standard-4\u000016 4\n",
      "351s1-standard-8\u000032 8\n",
      "s1-standard-12\u000048 12\n",
      "s1-standard-16\u000064 16\n",
      "s1-standard-32\u0000128 32\n",
      "s1-standard-64\u0000256-n10 64\n",
      "s1-standard-48\u0000192-n10 48\n",
      "352s1-standard-32\u0000128-n10 32\n",
      "s1-standard-16\u000064-n10 16\n",
      "s1-standard-8\u000032-n10 8\n",
      "s1-highmem-2\u000016 2\n",
      "s1-highmem-4\u000032 4\n",
      "s1-highmem-8\u000064 8\n",
      "353s1-highmem-16\u0000128 16\n",
      "s1-highmem-32\u0000256 32\n",
      "s1-highmem-32\u0000256-n10 32\n",
      "s1-highmem-16\u0000128-n10 16\n",
      "s1-highmem-8\u000064-n10 8\n",
      "s1-highcpu-4\u00004 4\n",
      "354s1-highcpu-8\u00008 8\n",
      "s1-highcpu-32\u000032 32\n",
      "s1-highcpu-32\u000064-n10 32\n",
      "s1-highcpu-16\u000016-n10 16\n",
      "s1-highcpu-8\u00008-n10 8\n",
      "355eci.ins.s1-standard-2\u00008 2\n",
      "eci.ins.s1-standard-4\u000016 4\n",
      "eci.ins.s1-standard-8\u000032 8\n",
      "eci.ins.s1-standard-16\u000064 16\n",
      "eci.ins.s1-standard-32\u0000128 32\n",
      "s1-highcpu-16\u000016 16\n",
      "356s1-highcpux2\u000032\u000064 32\n",
      "eci.ins.s1-highmem-2\u000016 2\n",
      "eci.ins.s1-highmem-4\u000032 4\n",
      "eci.ins.s1-highmem-8\u000064 8\n",
      "eci.ins.s1-highmem-16\u0000128 16\n",
      "eci.ins.s1-highmem-32\u0000256 32\n",
      "357•GPU Code G\n",
      "eci.ins.s1-highcpu-4\u00004 4\n",
      "eci.ins.s1-highcpu-8\u00008 8\n",
      "eci.ins.s1-highcpu-16\u000016 16\n",
      "Flavor Name CPU Memory\n",
      "g1-standard-4\u000016\u00001rtx2080ti 4 16\n",
      "g1-standard-8\u000032\u00001rtx2080ti 8 32\n",
      "g1-standard-8\u000032\u00002rtx2080ti 8 32\n",
      "g1-standard-16\u000064\u00002rtx2080ti 16 64\n",
      "g1-standard-16\u000064\u00004rtx2080ti 16 64\n",
      "g1-standard-32\u0000128\u00008rtx2080ti 32 128\n",
      "358•GPU Code RTX4090\n",
      "g1-standard-8\u000064\u00001rtx2080ti 8 64\n",
      "g1-standard-8\u000064\u00002rtx2080ti 8 64\n",
      "g2-standard-32\u0000128\u00004rtx4090 32 128 4flav-\n",
      "79d5efb\n",
      "91bd-\n",
      "442b-\n",
      "9919\u0000\n",
      "f7d7dd5\n",
      "761a\n",
      "g2-standard-16\u0000128\u00002rtx4090 16 128 2flav-\n",
      "a9bfd4\n",
      "-0b9f-\n",
      "481a-\n",
      "a6ce-\n",
      "8c924d\n",
      "08ec3\n",
      "g2-standard-16\u000064\u00002rtx4090 16 64 2flav-\n",
      "fc2c0e0\n",
      "\u00008ecb-\n",
      "4039\u0000\n",
      "aa98\u0000\n",
      "11bccc9\n",
      "2d35\n",
      "g2-standard-32\u000064\u00002rtx4090 32 64 2flav-\n",
      "6b11c1c\n",
      "2631\u0000\n",
      "4ce8\u0000\n",
      "8184\u0000\n",
      "1f94859\n",
      "18cb\n",
      "g2-standard-16\u000064\u00001rtx4090 16 64 1flav-\n",
      "2a2286\n",
      "\u0000503d-\n",
      "4e0e-\n",
      "8cdf-\n",
      "359\n",
      "b99447\n",
      "865d3\n",
      "360System Image\n",
      "Below is a list of System Images that the VKS system currently supports:\n",
      "Kubernetes\n",
      "versionImage name Image ID\n",
      "v1.27.12 1_Ubuntu-22.kube_v1\u0000\n",
      "27\u000012img-36ee0a61\u0000863d-4d40-a768\u0000\n",
      "9b41269b8a62\n",
      "v1.28.8 1_Ubuntu-22.kube_v1\u0000\n",
      "28\u00008img-983d55cf-9b5b-44cf-aa72\u0000\n",
      "23f3b25d43ce\n",
      "v1.29.1 1_Ubuntu-22.kube_v1\u0000\n",
      "29\u00001img-108b3a77-ab58\u00004000\u00009b3e-\n",
      "190d0b4b07fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = character_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_documents = [Document(page_content=text, metadata={\"id\": str(i), \"source\": \"dense\"}) for i, text in enumerate(splits)]\n",
    "sparse_documents = [Document(page_content=text, metadata={\"id\": str(i), \"source\": \"sparse\"}) for i, text in enumerate(splits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"llama3.1:8b\"\n",
    "base_url = envs[\"OLLAMA_HOST\"]  # for example, \"http://localhost:11434\"\n",
    "\n",
    "embedding_function = OllamaEmbeddings(base_url=base_url, model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"vks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=dense_documents,\n",
    "    embedding=embedding_function,\n",
    "    collection_name=collection_name,\n",
    "    client=chroma_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dense retriever\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "# Create sparse retriever\n",
    "sparse_retriever = BM25Retriever.from_documents(sparse_documents, k=10)\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[dense_retriever, sparse_retriever], weights=[0.5, 0.5], c=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/kalista/git-cuongpiger/unlocking-data-with-generative-ai-and-rag/.venv/lib/python3.10/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prompt - ignore LangSmith warning, you will not need langsmith for this coding exercise\n",
    "prompt = hub.pull(\"jclemens24/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance check prompt\n",
    "relevance_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert evaluator tasked with determining the relevance of a given context to a specific question.\n",
    "    Question: \"{question}\"\n",
    "    Context: \"{retrieved_context}\"\n",
    "    \n",
    "    Assign a relevance score from 1 to 5 based on the following guidelines:\n",
    "    - 1: Not at all relevant\n",
    "    - 2: Slightly relevant\n",
    "    - 3: Moderately relevant\n",
    "    - 4: Very relevant\n",
    "    - 5: Highly relevant\n",
    "    Provide ONLY the numeric score as your response without any additional text.\n",
    "\n",
    "    Relevance Score:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(llm_output):\n",
    "    try:\n",
    "        score = float(llm_output.strip())\n",
    "        return score\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "# Chain it all together with LangChain\n",
    "def conditional_answer(x):\n",
    "    relevance_score = extract_score(x['relevance_score'])\n",
    "    if relevance_score < 4:\n",
    "        return \"I don't know.\"\n",
    "    else:\n",
    "        return x['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=base_url, \n",
    "                 model=embedding_model,\n",
    "                 temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | RunnableParallel(\n",
    "        {\"relevance_score\": (\n",
    "            RunnablePassthrough()\n",
    "            | (lambda x: relevance_prompt_template.format(question=x['question'], retrieved_context=x['context']))\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ), \"answer\": (\n",
    "            RunnablePassthrough()\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )}\n",
    "    )\n",
    "    | RunnablePassthrough().assign(final_answer=conditional_answer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: What is VKS\n",
      "\n",
      "Relevance Score: 3\n",
      "\n",
      "Final Answer:\n",
      "I don't know.\n",
      "\n",
      "\n",
      "Retrieved Documents:\n",
      "Document 1: Document ID: 26 source: dense\n",
      "Content:\n",
      "Specifically, the Node Name will have additional information: Cluster Name ,\n",
      "Node Group Name .\n",
      "◦Delete User Builder on User's Worker Node.\n",
      "◦Change SSH mechanism from Port 22 to Port 234.\n",
      "If you encounter any problems with this official release, please contact VKS support\n",
      "for assistance.\n",
      "May 03, 2024\n",
      "20The latest update for VKS is available, bringing many new features and\n",
      "improvements to users. Here are the details about the update:\n",
      "New feature:\n",
      "•Supports Whitelist feature: VKS allows creating a Private Node Group with only\n",
      "Private IP and also allows any IP to connect to the Cluster through the Whitelist\n",
      "IP feature. For more details, please refer to Whitelist .\n",
      "Improve:\n",
      "•System optimization: Helps the system operate more smoothly and efficiently.\n",
      "•Bug Fixes: Fixed some minor bugs to provide a better user experience.\n",
      "If you encounter any problems after updating, please contact VKS support for\n",
      "assistance.\n",
      "We're excited to introduce a new update to the VKS service, giving you a more\n",
      "\n",
      "Document 2: Document ID: 228 source: sparse\n",
      "Content:\n",
      "improve the performance and availability of applications. ALB operates at the\n",
      "application layer, allowing traffic distribution based on many factors such as\n",
      "request type, server state, and load distribution algorithm. ALB provides\n",
      "advanced routing capabilities, allowing traffic to be directed based on Host or\n",
      "Path Header. It also supports session persistence, which helps maintain user\n",
      "sessions to the same server. This is useful for applications that require\n",
      "consistency in user interactions. For more information about ALB, please refer\n",
      "to \u0000How it works \u0000ALB\u0000\u0000\n",
      "Model:Overview\n",
      "What is ALB?\n",
      "196In addition to the basic components of a K8S cluster and an ALB that you already\n",
      "know, in this model we use: \n",
      "•Ingress: is a resource in Kubernetes that is configured to make Services\n",
      "accessible from outside the k8s cluster via URL, and can also load balance\n",
      "traffic, support SSL/TLS connections and provide virtual hosting based on\n",
      "\n",
      "Document 3: Document ID: 292 source: dense\n",
      "Content:\n",
      "nginx-app-7c79c4bf97-5hcds   1/1     Running   0          49s   172.16.19\n",
      "nginx-app-7c79c4bf97-5hgwp   1/1     Running   0          49s   172.16.19\n",
      "nginx-app-7c79c4bf97-5l79h   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-5q2f4   1/1     Running   0          49s   172.16.22\n",
      "nginx-app-7c79c4bf97-5szc6   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-9272q   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-cgwrj   1/1     Running   0          49s   172.16.67\n",
      "nginx-app-7c79c4bf97-fhlg4   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-fj865   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-gh6hj   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-hx2rn   1/1     Running   0          49s   172.16.83\n",
      "nginx-app-7c79c4bf97-jv26j   1/1     Running   0          49s   172.16.16\n",
      "nginx-app-7c79c4bf97-km7p4   1/1     Running   0          49s   172.16.22\n",
      "\n",
      "Document 4: Document ID: 260 source: sparse\n",
      "Content:\n",
      "managing VNG Cloud resources for Kubernetes clusters, including:\n",
      "◦Create and manage Network Load Balancer \u0000NLB\u0000 for Kubernetes Services\n",
      "with service type = Load Balancer.\n",
      "What is NLB?\n",
      "225Integrate with Network Load Balancer\n",
      "To integrate a Network Load Balancer with a Kubernetes cluster, you can use a\n",
      "Service with type LoadBalancer . When you create such a Service, VNGCloud\n",
      "Controller Manager will automatically create an NLB to forward traffic to pods on\n",
      "your node . You can also use annotations to customize Network Load Balancer\n",
      "properties, such as port, protocol,...\n",
      "•Create a Kubernetes cluster on VNGCloud, or use an existing cluster. Note:\n",
      "make sure you have downloaded the cluster configuration file once the cluster\n",
      "has been successfully initialized and accessed your cluster.\n",
      "•Create or use a service account created on IAM and attach policy: \n",
      "vLBFullAccess , vServerFullAccess . To create a service account, go here and\n",
      "follow these steps:\n",
      "\n",
      "Document 5: Document ID: 390 source: dense\n",
      "Content:\n",
      "auto_scale_config {\n",
      "    min_size = 0\n",
      "    max_size = 5\n",
      "  }\n",
      "  upgrade_config {\n",
      "    strategy = \"SURGE\"\n",
      "    max_surge = 1\n",
      "max_unavailable = 0\n",
      "  }\n",
      "  image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\"\n",
      "  flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6dc4b\"\n",
      "  disk_size = 20\n",
      "  disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\"\n",
      "enableprivatenodes=true\n",
      "334Then, if you need to add Whitelist IP for Control Plane, add this field to the main.tf\n",
      "file and reapply this file:  enable_private_nodes  true\n",
      "  ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\"\n",
      "  labels = {\n",
      "    \"mylabel\" = \"vngcloud\"\n",
      "  }\n",
      "  taint {\n",
      "    key    = \"mykey\"\n",
      "    value  = \"myvalue\"\n",
      "    effect = \"PreferNoSchedule\"\n",
      "  }\n",
      "}\n",
      "335terraform {\n",
      "  required_providers {\n",
      "    vngcloud = {\n",
      "      source  = \"vngcloud/vngcloud\"\n",
      "      version = \"1.2.2\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "provider \"vngcloud\" {\n",
      "  token_url        = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/to\n",
      "  client_id        = var.client_id\n",
      "  client_secret    = var.client_secret\n",
      "\n",
      "Document 6: Document ID: 0 source: sparse\n",
      "Content:\n",
      "1ThuyVT2\n",
      "2VKS\n",
      "VKS \u0000VNGCloud Kubernetes Service) is a managed service on VNGCloud that helps\n",
      "you simplify the deployment and management of container-based applications.\n",
      "Kubernetes is an open-source platform developed by Google, widely used for\n",
      "managing and deploying containerized applications in distributed environments.\n",
      "VKS Demo Video - Gi ả i Pháp Qu ả n Lý Kubernetes Toàn Di ệ n C ủ a VN VKS Demo Video - Gi ả i Pháp Qu ả n Lý Kubernetes Toàn Di ệ n C ủ a VN ……\n",
      "3What is VKS?\n",
      "VKS \u0000VNGCloud Kubernetes Service) is a managed service on VNGCloud that\n",
      "simplifies the deployment and management of container-based applications.\n",
      "Kubernetes, an open-source platform developed by Google, is widely used to\n",
      "manage and deploy containerized applications in distributed environments.\n",
      "•Fully Managed control plane: VKS will free you from the burden of managing\n",
      "the Kubernetes Control Plane, allowing you to focus on developing applications.\n",
      "Highlights of VKS\n",
      "\n",
      "Document 7: Document ID: 175 source: dense\n",
      "Content:\n",
      "•Apply scaling-app.yaml manifest to generate resources for testing the\n",
      "autoscaling feature. This manifest run 1 pod of CUDA VectorAdd Test and the\n",
      "GPU Nodegroup will be scaled to 3 when the GPU usage is greater than 50%.\n",
      "•Apply scale-gpu.yaml manifest to create the ScaleObject for the above\n",
      "application. This manifest will scale the GPU Nodegroup based on the GPU\n",
      "usage.\n",
      "•When the ScaledObjectReady value is True, the GPU Nodegroup will\n",
      "be scaled based on the GPU usage.helm install --wait kedacore \\\n",
      "  --namespace keda --create-namespace \\\n",
      "  oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/keda \\\n",
      "  --version 2.14.2\n",
      "kubectl -n keda get all\n",
      "kubectl apply -f \\\n",
      "  https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-g\n",
      "kubectl apply -f \\\n",
      "  https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-g\n",
      "kubectl get deploy\n",
      "# Check the ScaledObject\n",
      "kubectl get scaledobject\n",
      "158Clusters\n",
      "A cluster in Kubernetes is a collection of one or more virtual machines \u0000VMs\u0000\n",
      "\n",
      "Document 8: Document ID: 23 source: sparse\n",
      "Content:\n",
      "New feature:\n",
      "•Support users working with VKS through Terraform: Users can easily create\n",
      "Clusters and Node Groups in VKS using Terraform. For more details, refer here .\n",
      "Improve:\n",
      "•Upgrade VNGCloud Controller Manager Plugin: Add Annotation to configure\n",
      "Load Balancer to support Proxy Protocol. For more details, refer here .\n",
      "We are extremely pleased to announce that the official release ( General\n",
      "Availability ) of VNGCloud Kubernetes Service is available. With this official\n",
      "release, in addition to the features we have provided on previous releases, this\n",
      "version will bring many new features and improvements to users. Here are the\n",
      "details about the update:\n",
      "New feature:\n",
      "•Re-activate: VKS allows you to request the system to automatically re-initialize\n",
      "the default IAM Service Account when you mistakenly delete or change\n",
      "previously created default IAM Service Account information. The default IAM\n",
      "Service Account is the IAM Service Account that is automatically created by theJune 12, 2024\n",
      "\n",
      "Document 9: Document ID: 378 source: dense\n",
      "Content:\n",
      "backup.velero.io/backup-volumes=volume1,volume2\n",
      "•Or you can automatically find volumes by:./helper.sh check_hostPath\n",
      "./helper.sh mark_volumeMark the Volumes you want to backup and\n",
      "unnecessary resources\n",
      "1. Convert hostPath Volume to Persistent Volume to be\n",
      "able to perform backup\n",
      "2. Mark Persistent Volume to include in backup\n",
      "3. Mark resources in exclude in backup\n",
      "325Because VKS operates under the Fully Managed Control Plane mechanism, you do\n",
      "not need to backup resources such as: calico, kube-dns, kube-scheduler, \n",
      "kube-apiserver,... In addition, vContainer resources such as: \n",
      "magnum-auto-healer, cluster-autoscaler, csi-cinder,... will also be ignored.\n",
      "•Identify resources that do not need backup via the command:\n",
      "When performing a migration, it is possible that the resources in the source Cluster\n",
      "are using labels and taints. You need to ensure these important labels and taints\n",
      "exist in the target Cluster.\n",
      "•Check lable and taint via command:\n",
      "\n",
      "Document 10: Document ID: 152 source: sparse\n",
      "Content:\n",
      "•On the pod nvidia-device-plugin-daemonset in the gpu-operator\n",
      "namespace, you can execute nvidia-smi command to check the GPU\n",
      "information of the node:\n",
      "•In this section, we will show you how to deploy a GPU workload in a VKS\n",
      "cluster. We will use the cuda-vectoradd-test workload as an example. The \n",
      "cuda-vectoradd-test workload is a simple CUDA program that adds two\n",
      "vectors together. The program is provided as a container image that you can\n",
      "deploy in your VKS cluster. See file cuda-vectoradd-test.yaml.kubectl get node -o json | jq '.items[].metadata.labels' | grep \"nvidi\n",
      "POD_NAME=$(kubectl -n gpu-operator get pods -l app=nvidia-device-plugi\n",
      "kubectl -n gpu-operator exec -it $POD_NAME -- nvidia-smi\n",
      "Deploy your GPU workload\n",
      "Cuda VectorAdd Test\n",
      "139•In this section, we apply a Deployment manifest for a TensorFlow GPU\n",
      "application. The purpose of this Deployment is to create and manage a single\n",
      "pod running a TensorFlow container that utilizes GPU resource for executing the\n",
      "\n",
      "Document 11: Document ID: 286 source: dense\n",
      "Content:\n",
      "helping to optimize resource allocation and network management.\n",
      "Step 5\u0000 Select Create Kubernetes cluster. Please wait a few minutes for us to\n",
      "initialize your Cluster, the status of the Cluster is now Creating .\n",
      "Step 6\u0000 When the Cluster status is Active , you can view Cluster information and\n",
      "Node Group information by selecting Cluster Name in the Name column .\n",
      "Below are instructions for deploying an nginx deployment and testing IP assignment\n",
      "for the pods deployed in your cluster.\n",
      "Deploy a Workload\n",
      "248Step 1\u0000 Access https://vks.console.vngcloud.vn/k8s-cluster\n",
      "Step 2\u0000 The Cluster list is displayed, select the Download icon and select \n",
      "Download Config File to download the kubeconfig file. This file will give you full\n",
      "access to your Cluster.\n",
      "Step 3 : Rename this file to config and save it to the ~/.kube/config folder\n",
      "Step 4\u0000 Perform Cluster check via command:\n",
      "•Run the following command to check the node\n",
      "•If the result is as below, it means your Cluster is successfully initialized with 5\n",
      "\n",
      "Document 12: Document ID: 24 source: sparse\n",
      "Content:\n",
      "previously created default IAM Service Account information. The default IAM\n",
      "Service Account is the IAM Service Account that is automatically created by theJune 12, 2024\n",
      "May 30, 2024\n",
      "19VKS system when you start working with VKS. We will use this IAM Service\n",
      "Account to initialize resources for your Cluster.\n",
      "•Event History : VKS will display the history of events that occur when users\n",
      "work with the Cluster or each Node Group. This will be a way to help you\n",
      "monitor activities occurring with your Cluster, thereby limiting unusual activities\n",
      "from occurring.\n",
      "•Volume : VKS has integrated the display of the Volume list at the Resource Tab,\n",
      "helping you easily manage the Volumes that are attached to your Cluster.\n",
      "•Load Balancer : VKS has integrated the display of the Load Balancer list in the\n",
      "Resource Tab, helping you easily manage the Load Balancers being used for\n",
      "your Cluster.\n",
      "Improve:\n",
      "•Performance : VKS has optimized performance when initializing the Cluster.\n",
      "\n",
      "Document 13: Document ID: 296 source: dense\n",
      "Content:\n",
      "To initialize a Cluster, follow the steps below:\n",
      "Step 1\u0000 Access https://vks.console.vngcloud.vn/overview\n",
      "Step 2\u0000 On the Overview screen , select Activate.\n",
      "Step 3\u0000 Wait until we successfully initialize your VKS account. After successfully\n",
      "Activating, select Create a Cluster.Necessary conditions\n",
      "Initialize a Cluster using Cilium Overlay\n",
      "254Step 4\u0000 At the Cluster initialization screen, we have set up the information for the\n",
      "Cluster and a Default Node Group for you. To use Cilium Overlay for your Cluster ,\n",
      "please select:\n",
      "•Network type : Cilium Overlay\n",
      "Field Meaning Illustrative example\n",
      "VPC The IP address range that\n",
      "the Cluster nodes will use\n",
      "to communicate.In the picture, we choose\n",
      "VPC with IP range \n",
      "10.111.0.0/16 ,\n",
      "corresponding to 65536\n",
      "IPs\n",
      "Subnet A smaller IP address range\n",
      "belonging to the VPC.\n",
      "Each node in the Cluster\n",
      "will be assigned an IP from\n",
      "this Subnet. The Subnet\n",
      "must be within the IP\n",
      "range of the selected VPC.In the picture, we choose\n",
      "Subnet with Primary IP\n",
      "\n",
      "Document 14: Document ID: 14 source: sparse\n",
      "Content:\n",
      "as: vStorage, vCR, vMonitor, VNGCloud APIs,... Private Cluster is the ideal\n",
      "choice for services that require strict access control, ensuring compliance\n",
      "with regulations on security and data privacy. For details on the two operating\n",
      "models of Cluster, you can refer to here and refer to the steps to create a\n",
      "private Cluster here .\n",
      "VKS \u0000VNGCloud Kubernetes Service) introduces the latest update for VKS, bringing\n",
      "numerous new improvements for users. Here are the details of the update:Aug 28, 2024\n",
      "Aug 26, 2024\n",
      "13Improve:\n",
      "•Kubernetes Version: VKS has added new images to optimize the size, features,\n",
      "and network compared to the old images. The creation of these images also\n",
      "aims to serve both Public and Private clusters that VKS is about to launch.\n",
      "Specifically, in this release, we have added the following images:\n",
      "◦Ubuntu-22.kube_v1.27.12-vks.1724605200\n",
      "◦Ubuntu-22.kube_v1.28.8-vks.1724605200\n",
      "◦Ubuntu-22.kube_v1.29.1-vks.1724605200\n",
      "Chú ý:\n",
      "\n",
      "Document 15: Document ID: 395 source: dense\n",
      "Content:\n",
      "•Find and select Policy: vMonitorMetricPush, then click \" Create a Service\n",
      "Account \" to create a Service Account, Policy: vMonitorMetricPush created by\n",
      "VNG Cloud only contains the correct permission to push metrics to the system.\n",
      "•After successful creation, you need to save the Client_ID and Secret_Key to\n",
      "perform the next step.\n",
      "Install helm on Debian/Ubuntu server\n",
      "1. You need to install Helm on a server with kubeconfig containing enough\n",
      "permissions to interact with the Kubernetes Cluster.\n",
      "•Check permissions with kubectl command :\n",
      "Kube checks permissionInstall Metric Agent using Helm\n",
      "340•If the result of the above commands is YES , then you have enough rights.\n",
      "2. Proceed to install Helm\n",
      "•Execute the following commands:\n",
      "•Check that Helm has been installed successfully\n",
      "Install Helm in other operating systems\n",
      "Refer to the installation instructions here: Install Helm Through Package Managers\n",
      "3. Some additional notes about Helm: (see details at: helm docs )\n",
      "\n",
      "Document 16: Document ID: 382 source: sparse\n",
      "Content:\n",
      "authority . Specifically, at the IAM site, you can:\n",
      "•Select \" Create a Service Account \", enter a name for the Service Account\n",
      "and click Next Step to assign permissions to the Service Account.\n",
      "•Find and select Policy: VKSFullAccess then click \" Create a Service\n",
      "Account \" to create a Service Account, Policy: VKSFullAccess is created\n",
      "by VNG Cloud, you cannot delete these policies.What is Terraform?\n",
      "Implementation steps\n",
      "328•After successful creation, you need to save the Client_ID and Secret_Key of\n",
      "the Service Account to perform the next step.\n",
      "2.Access the VKS Portal here , Activate the VKS service on the Overview tab.\n",
      "Please wait until we successfully create your VKS account.\n",
      "3.Install Terraform:\n",
      "•Download and install Terraform for your operating system from \n",
      "https://developer.hashicorp.com/terraform/install .\n",
      "4.Initialize Terraform configuration:\n",
      "•Create a file variable.tfand declare Service Account information in this\n",
      "file.\n",
      "\n",
      "Document 17: Document ID: 174 source: dense\n",
      "Content:\n",
      "- type: prometheus\n",
      "      metadata: # prometheus-stack-kube-prom-prometheus\n",
      "        serverAddress: http://prometheus-stack-kube-prom-prometheus.pr\n",
      "        metricName: engine_active\n",
      "        query: sum(DCGM_FI_DEV_MEM_COPY_UTIL) / count(DCGM_FI_DEV_MEM_\n",
      "        threshold: '0.5'  # Scale the GPU Nodegroup when the GPU memorAutoscaling GPU Resources\n",
      "157•The above manifest scales the GPU Nodegroup based on the GPU usage and\n",
      "memory usage. The query field specifies the query to fetch the metrics from\n",
      "Prometheus. The threshold field specifies the threshold value to scale the\n",
      "GPU Nodegroup. The minReplicaCount and maxReplicaCount fields specify\n",
      "the minimum and maximum number of replicas that the GPU Nodegroup can\n",
      "scale to.\n",
      "•Now let's install Keda in your cluster by executing the below command:\n",
      "•Apply scaling-app.yaml manifest to generate resources for testing the\n",
      "autoscaling feature. This manifest run 1 pod of CUDA VectorAdd Test and the\n",
      "\n",
      "Document 18: Document ID: 217 source: sparse\n",
      "Content:\n",
      "example: 1.24 to 1.25\u0000\n",
      "Step 5\u0000 The VKS system will upgrade all Node Groups to the Control Plane version.\n",
      "After the upgrade is complete, the Node Group status returns to ACTIVE .\n",
      "Attention:\n",
      "•Upgrading Node Group Version is optional and independent of\n",
      "upgrading Control Plane Version. However, all Node Groups in a\n",
      "Cluster will be upgraded at the same time, as well as Control Plane\n",
      "Version and Node Group Version in the same Cluster cannot differ by\n",
      "more than 1 minor version. Besides, the VKS system automatically\n",
      "188upgrades the Node Group Version when the current K8S Version being\n",
      "used for your Cluster exceeds the supplier's support period.\n",
      "•During the Node Group Version upgrade, you cannot perform other\n",
      "actions on your Node Group.\n",
      "•Below are a few notes before, during and after the upgrade process,\n",
      "please refer to:\n",
      "Before getting into work:\n",
      "•Check the current version: Visit Releases for a list of supported\n",
      "versions. Select a new version that is valid and compatible with the\n",
      "\n",
      "Document 19: Document ID: 407 source: dense\n",
      "Content:\n",
      "g1-standard-8\u000032\u00002rtx2080ti 8 32\n",
      "g1-standard-16\u000064\u00002rtx2080ti 16 64\n",
      "g1-standard-16\u000064\u00004rtx2080ti 16 64\n",
      "g1-standard-32\u0000128\u00008rtx2080ti 32 128\n",
      "358•GPU Code RTX4090\n",
      "g1-standard-8\u000064\u00001rtx2080ti 8 64\n",
      "g1-standard-8\u000064\u00002rtx2080ti 8 64\n",
      "g2-standard-32\u0000128\u00004rtx4090 32 128 4flav-\n",
      "79d5efb\n",
      "91bd-\n",
      "442b-\n",
      "9919\u0000\n",
      "f7d7dd5\n",
      "761a\n",
      "g2-standard-16\u0000128\u00002rtx4090 16 128 2flav-\n",
      "a9bfd4\n",
      "-0b9f-\n",
      "481a-\n",
      "a6ce-\n",
      "8c924d\n",
      "08ec3\n",
      "g2-standard-16\u000064\u00002rtx4090 16 64 2flav-\n",
      "fc2c0e0\n",
      "\u00008ecb-\n",
      "4039\u0000\n",
      "aa98\u0000\n",
      "11bccc9\n",
      "2d35\n",
      "g2-standard-32\u000064\u00002rtx4090 32 64 2flav-\n",
      "6b11c1c\n",
      "2631\u0000\n",
      "4ce8\u0000\n",
      "8184\u0000\n",
      "1f94859\n",
      "18cb\n",
      "g2-standard-16\u000064\u00001rtx4090 16 64 1flav-\n",
      "2a2286\n",
      "\u0000503d-\n",
      "4e0e-\n",
      "8cdf-\n",
      "359\n",
      "b99447\n",
      "865d3\n",
      "360System Image\n",
      "Below is a list of System Images that the VKS system currently supports:\n",
      "Kubernetes\n",
      "versionImage name Image ID\n",
      "v1.27.12 1_Ubuntu-22.kube_v1\u0000\n",
      "27\u000012img-36ee0a61\u0000863d-4d40-a768\u0000\n",
      "9b41269b8a62\n",
      "v1.28.8 1_Ubuntu-22.kube_v1\u0000\n",
      "28\u00008img-983d55cf-9b5b-44cf-aa72\u0000\n",
      "23f3b25d43ce\n",
      "v1.29.1 1_Ubuntu-22.kube_v1\u0000\n",
      "29\u00001img-108b3a77-ab58\u00004000\u00009b3e-\n",
      "190d0b4b07fc\n",
      "\n",
      "Document 20: Document ID: 380 source: sparse\n",
      "Content:\n",
      "sc-iops-200-retain (default)    csi.vngcloud.vn   Retain          I\n",
      "sc-ssd-10000-delete (default)   csi.vngcloud.vn   Delete          I4. Check label and taint of node\n",
      "5. Mapping Storage Class\n",
      "326apiVersion: v1\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  name: change-storage-class-config\n",
      "  namespace: velero\n",
      "  labels:\n",
      "    velero.io/plugin-config: \"\"\n",
      "    velero.io/change-storage-class: RestoreItemAction\n",
      "data:\n",
      "  sc-iops-200-retain: ssd-200\n",
      "  sc-ssd-10000-delete: ssd-10000\n",
      "327Working VKS with Terraform\n",
      "Terraform is an open source infrastructure as code tool that allows users to manage\n",
      "their infrastructure easily and efficiently across different cloud platforms, such as\n",
      "VNG Cloud, AWS, Google Cloud and Azure. Terraform Server refers to the instance\n",
      "of the Terraform engine running on a specific server or machine. This is where\n",
      "infrastructure code is written and executed, allowing users to create, modify, and\n",
      "destroy resources on the cloud platform.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User Query\n",
    "user_query = \"What is VKS\"\n",
    "\n",
    "result = rag_chain_with_source.invoke(user_query)\n",
    "retrieved_docs = result['context']\n",
    "\n",
    "print(f\"Original Question: {user_query}\\n\")\n",
    "print(f\"Relevance Score: {result['answer']['relevance_score']}\\n\")\n",
    "print(f\"Final Answer:\\n{result['answer']['final_answer']}\\n\\n\")\n",
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Document {i}: Document ID: {doc.metadata['id']} source: {doc.metadata['source']}\")\n",
    "    print(f\"Content:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
